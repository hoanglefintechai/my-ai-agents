{"type":"summary","summary":"Terminal Command Execution and Screen Clearing","leafUuid":"6f7b50b3-4f2c-407a-a2e5-7daf789ff1f0"}
{"type":"summary","summary":"Luồng đặt lệnh gRPC: Từ request đến xử lý và validation","leafUuid":"6824ee15-2c28-4c4a-a784-a08a79b1526e"}
{"type":"summary","summary":"Order Flow Refactor: Proto, Validation, Broker Integration","leafUuid":"4776710e-34ad-4c89-a3e0-fd7d93a857fb"}
{"parentUuid":null,"isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":"Chương trinh van dang con loi nhung toi khong the kiem tra vì thieu thong tin can thiet. Hay giup toi ghi log tường minh request đến và response trả về. Viêc che thong tin nhay cam toi se thực hiện sau"},"uuid":"8d3922e6-0bfe-447a-b6a0-8c58611bb9e9","timestamp":"2025-08-20T01:55:26.013Z"}
{"parentUuid":"8d3922e6-0bfe-447a-b6a0-8c58611bb9e9","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01VNNPcJa5nbgmotv84R7MT1","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Tôi sẽ giúp bạn thêm logging chi tiết cho request và response trong gRPC services. Để làm điều này hiệu quả, tôi cần tạo một todo list để theo dõi các bước cần thực hiện."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":5145,"cache_read_input_tokens":13653,"cache_creation":{"ephemeral_5m_input_tokens":5145,"ephemeral_1h_input_tokens":0},"output_tokens":9,"service_tier":"standard"}},"requestId":"req_011CSJ8U31grjZ6xfiokKWGy","type":"assistant","uuid":"0ed52d21-9679-4be9-a19f-ece402d02cfc","timestamp":"2025-08-20T01:55:29.819Z"}
{"parentUuid":"0ed52d21-9679-4be9-a19f-ece402d02cfc","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01VNNPcJa5nbgmotv84R7MT1","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_015iT5JSVogA1w873z3XZihB","name":"TodoWrite","input":{"todos":[{"id":"1","content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"pending","priority":"high"},{"id":"2","content":"Tạo interceptor gRPC để log request/response","status":"pending","priority":"high"},{"id":"3","content":"Thêm logging vào các servicer chính","status":"pending","priority":"high"},{"id":"4","content":"Cấu hình log format và level phù hợp","status":"pending","priority":"medium"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":5145,"cache_read_input_tokens":13653,"cache_creation":{"ephemeral_5m_input_tokens":5145,"ephemeral_1h_input_tokens":0},"output_tokens":336,"service_tier":"standard"}},"requestId":"req_011CSJ8U31grjZ6xfiokKWGy","type":"assistant","uuid":"d2e5dfc4-8fb1-48a3-b738-8b7263b3a362","timestamp":"2025-08-20T01:55:32.268Z"}
{"parentUuid":"d2e5dfc4-8fb1-48a3-b738-8b7263b3a362","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_015iT5JSVogA1w873z3XZihB","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"17c0d570-3b3f-42a0-8be9-a86bb978e072","timestamp":"2025-08-20T01:55:32.309Z","toolUseResult":{"oldTodos":[],"newTodos":[{"content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"pending","priority":"high","id":"1"},{"content":"Tạo interceptor gRPC để log request/response","status":"pending","priority":"high","id":"2"},{"content":"Thêm logging vào các servicer chính","status":"pending","priority":"high","id":"3"},{"content":"Cấu hình log format và level phù hợp","status":"pending","priority":"medium","id":"4"}]}}
{"parentUuid":"17c0d570-3b3f-42a0-8be9-a86bb978e072","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_019PMUdi4tbXDG3P25rzL2A8","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01AQbMKzeX8jqFuNyZFqs77S","name":"TodoWrite","input":{"todos":[{"id":"1","content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"in_progress","priority":"high"},{"id":"2","content":"Tạo interceptor gRPC để log request/response","status":"pending","priority":"high"},{"id":"3","content":"Thêm logging vào các servicer chính","status":"pending","priority":"high"},{"id":"4","content":"Cấu hình log format và level phù hợp","status":"pending","priority":"medium"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":567,"cache_read_input_tokens":18798,"cache_creation":{"ephemeral_5m_input_tokens":567,"ephemeral_1h_input_tokens":0},"output_tokens":283,"service_tier":"standard"}},"requestId":"req_011CSJ8UY9PmrCANNeeftxKt","type":"assistant","uuid":"cabb56fd-6345-4017-96bf-4d0168c0e463","timestamp":"2025-08-20T01:55:36.892Z"}
{"parentUuid":"cabb56fd-6345-4017-96bf-4d0168c0e463","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01AQbMKzeX8jqFuNyZFqs77S","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"92cc4584-18e4-4aa1-8a83-ed447afd10ec","timestamp":"2025-08-20T01:55:36.921Z","toolUseResult":{"oldTodos":[{"content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"pending","priority":"high","id":"1"},{"content":"Tạo interceptor gRPC để log request/response","status":"pending","priority":"high","id":"2"},{"content":"Thêm logging vào các servicer chính","status":"pending","priority":"high","id":"3"},{"content":"Cấu hình log format và level phù hợp","status":"pending","priority":"medium","id":"4"}],"newTodos":[{"content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"in_progress","priority":"high","id":"1"},{"content":"Tạo interceptor gRPC để log request/response","status":"pending","priority":"high","id":"2"},{"content":"Thêm logging vào các servicer chính","status":"pending","priority":"high","id":"3"},{"content":"Cấu hình log format và level phù hợp","status":"pending","priority":"medium","id":"4"}]}}
{"parentUuid":"92cc4584-18e4-4aa1-8a83-ed447afd10ec","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01WVKNS37Gxt5SbhK6uQtnQp","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Tôi sẽ bắt đầu bằng việc kiểm tra cấu trúc logging hiện tại trong codebase:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":503,"cache_read_input_tokens":19365,"cache_creation":{"ephemeral_5m_input_tokens":503,"ephemeral_1h_input_tokens":0},"output_tokens":9,"service_tier":"standard"}},"requestId":"req_011CSJ8UruznZ7NFVU4UMq4K","type":"assistant","uuid":"e26101ee-1129-4ec0-951b-8dd3336947e4","timestamp":"2025-08-20T01:55:40.290Z"}
{"parentUuid":"e26101ee-1129-4ec0-951b-8dd3336947e4","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01WVKNS37Gxt5SbhK6uQtnQp","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01PyNiNAAYiwpaspFZbk9zCg","name":"Grep","input":{"pattern":"import logging|logger","output_mode":"files_with_matches"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":503,"cache_read_input_tokens":19365,"cache_creation":{"ephemeral_5m_input_tokens":503,"ephemeral_1h_input_tokens":0},"output_tokens":118,"service_tier":"standard"}},"requestId":"req_011CSJ8UruznZ7NFVU4UMq4K","type":"assistant","uuid":"2cc3d83a-8815-471f-a023-82e54554223a","timestamp":"2025-08-20T01:55:41.012Z"}
{"parentUuid":"2cc3d83a-8815-471f-a023-82e54554223a","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01PyNiNAAYiwpaspFZbk9zCg","type":"tool_result","content":"Found 57 files\n/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/servicers/order_servicer.py\n/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/adapters/ssi_adapter.py\n/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/adapters/ssi/ssi_client.py\n/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/get_portfolio.py\n/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/adapters/ssi/ssi_portfolio_adapter.py\n/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/repositories/otp_repository.py\n/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/servicers/portfolio_servicer.py\n/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/update_broker_account.py\n/Users/lee/Documents/src/bep-oms-broker/src/application/services/order_application_service.py\n/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/repositories/sqlalchemy_broker_account_repository.py\n/Users/lee/Documents/src/bep-oms-broker/src/application/services/broker_account_application_service.py\n/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/servicers/broker_account_servicer.py\n/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/repositories/user_repository_impl.py\n/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/async_database.py\n/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/servicers/user_servicer.py\n/Users/lee/Documents/src/bep-oms-broker/main.py\n/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/server_config.py\n/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/otp/verify_otp_use_case.py\n/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/register_user.py\n/Users/lee/Documents/src/bep-oms-broker/src/application/services/transaction_manager.py\n/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/authentication_interceptor.py\n/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/otp/verify_registration_use_case.py\n/Users/lee/Documents/src/bep-oms-broker/src/application/services/user_application_service.py\n/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/repositories/sqlalchemy_trade_repository.py\n/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/repositories/sqlalchemy_order_repository.py\n/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/repositories/sqlalchemy_order_event_repository.py\n/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/otp/change_password_use_cases.py\n/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/adapters/email/aws_ses_service.py\n/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/sync_database.py\n/Users/lee/Documents/src/bep-oms-broker/alembic.ini\n/Users/lee/Documents/src/bep-oms-broker/tests/unit/shared/test_error_handlers.py\n/Users/lee/Documents/src/bep-oms-broker/src/shared/error_handlers.py\n/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/servicers/market_data_servicer.py\n/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/monitoring/metrics_collector.py\n/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/rate_limiting_interceptor.py\n/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py\n/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/health/health_service.py\n/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/repositories/sqlalchemy_security_repository.py\n/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/repositories/sqlalchemy_realtime_quote_repository.py\n/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/repositories/sqlalchemy_historical_price_repository.py\n/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/logging/structured_logger.py\n/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/grpc/interceptors/rate_limiting_interceptor.py\n/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/grpc/interceptors/metrics_interceptor.py\n/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/grpc/interceptors/logging_interceptor.py\n/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/grpc/interceptors/auth_interceptor.py\n/Users/lee/Documents/src/bep-oms-broker/src/domain/events/base_events.py\n/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/update_user_profile.py\n/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/update_security_info.py\n/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/search_securities.py\n/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/login_user.py\n/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/get_security_info.py\n/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/get_realtime_quote.py\n/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/get_historical_prices.py\n/Users/lee/Documents/src/bep-oms-broker/src/application/services/sync_transaction_manager.py\n/Users/lee/Documents/src/bep-oms-broker/src/application/services/market_data_application_service.py\n/Users/lee/Documents/src/bep-oms-broker/src/application/event_handlers/market_data_event_handlers.py\n/Users/lee/Documents/src/bep-oms-broker/README.md"}]},"uuid":"2d28df53-988c-4ee8-b030-5be4dbe362b3","timestamp":"2025-08-20T01:55:41.229Z","toolUseResult":{"mode":"files_with_matches","filenames":["/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/servicers/order_servicer.py","/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/adapters/ssi_adapter.py","/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/adapters/ssi/ssi_client.py","/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/get_portfolio.py","/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/adapters/ssi/ssi_portfolio_adapter.py","/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/repositories/otp_repository.py","/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/servicers/portfolio_servicer.py","/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/update_broker_account.py","/Users/lee/Documents/src/bep-oms-broker/src/application/services/order_application_service.py","/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/repositories/sqlalchemy_broker_account_repository.py","/Users/lee/Documents/src/bep-oms-broker/src/application/services/broker_account_application_service.py","/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/servicers/broker_account_servicer.py","/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/repositories/user_repository_impl.py","/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/async_database.py","/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/servicers/user_servicer.py","/Users/lee/Documents/src/bep-oms-broker/main.py","/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/server_config.py","/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/otp/verify_otp_use_case.py","/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/register_user.py","/Users/lee/Documents/src/bep-oms-broker/src/application/services/transaction_manager.py","/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/authentication_interceptor.py","/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/otp/verify_registration_use_case.py","/Users/lee/Documents/src/bep-oms-broker/src/application/services/user_application_service.py","/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/repositories/sqlalchemy_trade_repository.py","/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/repositories/sqlalchemy_order_repository.py","/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/repositories/sqlalchemy_order_event_repository.py","/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/otp/change_password_use_cases.py","/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/adapters/email/aws_ses_service.py","/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/sync_database.py","/Users/lee/Documents/src/bep-oms-broker/alembic.ini","/Users/lee/Documents/src/bep-oms-broker/tests/unit/shared/test_error_handlers.py","/Users/lee/Documents/src/bep-oms-broker/src/shared/error_handlers.py","/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/servicers/market_data_servicer.py","/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/monitoring/metrics_collector.py","/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/rate_limiting_interceptor.py","/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py","/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/health/health_service.py","/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/repositories/sqlalchemy_security_repository.py","/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/repositories/sqlalchemy_realtime_quote_repository.py","/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/persistence/repositories/sqlalchemy_historical_price_repository.py","/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/logging/structured_logger.py","/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/grpc/interceptors/rate_limiting_interceptor.py","/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/grpc/interceptors/metrics_interceptor.py","/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/grpc/interceptors/logging_interceptor.py","/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/grpc/interceptors/auth_interceptor.py","/Users/lee/Documents/src/bep-oms-broker/src/domain/events/base_events.py","/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/update_user_profile.py","/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/update_security_info.py","/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/search_securities.py","/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/login_user.py","/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/get_security_info.py","/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/get_realtime_quote.py","/Users/lee/Documents/src/bep-oms-broker/src/application/use_cases/get_historical_prices.py","/Users/lee/Documents/src/bep-oms-broker/src/application/services/sync_transaction_manager.py","/Users/lee/Documents/src/bep-oms-broker/src/application/services/market_data_application_service.py","/Users/lee/Documents/src/bep-oms-broker/src/application/event_handlers/market_data_event_handlers.py","/Users/lee/Documents/src/bep-oms-broker/README.md"],"numFiles":57}}
{"parentUuid":"2d28df53-988c-4ee8-b030-5be4dbe362b3","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01E1DGebriZo7RgP7qKwARds","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Tôi thấy có `logging_interceptor.py` trong cả `src/presentation/grpc/interceptors/` và `src/infrastructure/grpc/interceptors/`. Hãy kiểm tra cấu trúc logging hiện tại:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":1992,"cache_read_input_tokens":19868,"cache_creation":{"ephemeral_5m_input_tokens":1992,"ephemeral_1h_input_tokens":0},"output_tokens":2,"service_tier":"standard"}},"requestId":"req_011CSJ8VB88CJg6b3hArofGH","type":"assistant","uuid":"b1634912-0882-41b5-b277-eec77c34456c","timestamp":"2025-08-20T01:55:45.548Z"}
{"parentUuid":"b1634912-0882-41b5-b277-eec77c34456c","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01E1DGebriZo7RgP7qKwARds","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_0125kTZ72oMYmhLfzPUrZYzg","name":"Read","input":{"file_path":"/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/logging/structured_logger.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":1992,"cache_read_input_tokens":19868,"cache_creation":{"ephemeral_5m_input_tokens":1992,"ephemeral_1h_input_tokens":0},"output_tokens":2,"service_tier":"standard"}},"requestId":"req_011CSJ8VB88CJg6b3hArofGH","type":"assistant","uuid":"804d5f60-f81f-4e29-84c5-4bcec964e2c7","timestamp":"2025-08-20T01:55:46.037Z"}
{"parentUuid":"804d5f60-f81f-4e29-84c5-4bcec964e2c7","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01E1DGebriZo7RgP7qKwARds","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_014eXfxCwgGVzQgjAYqqNXPf","name":"Read","input":{"file_path":"/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/grpc/interceptors/logging_interceptor.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":1992,"cache_read_input_tokens":19868,"cache_creation":{"ephemeral_5m_input_tokens":1992,"ephemeral_1h_input_tokens":0},"output_tokens":2,"service_tier":"standard"}},"requestId":"req_011CSJ8VB88CJg6b3hArofGH","type":"assistant","uuid":"e2dd8028-2b47-4143-bee3-7fb8c9ff8d82","timestamp":"2025-08-20T01:55:46.564Z"}
{"parentUuid":"e2dd8028-2b47-4143-bee3-7fb8c9ff8d82","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01E1DGebriZo7RgP7qKwARds","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01QbWw4CmiUhnEtFs11yfnqN","name":"Read","input":{"file_path":"/Users/lee/Documents/src/bep-oms-broker/main.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":1992,"cache_read_input_tokens":19868,"cache_creation":{"ephemeral_5m_input_tokens":1992,"ephemeral_1h_input_tokens":0},"output_tokens":268,"service_tier":"standard"}},"requestId":"req_011CSJ8VB88CJg6b3hArofGH","type":"assistant","uuid":"b831de62-152f-4e37-a6dd-25295b26470c","timestamp":"2025-08-20T01:55:46.805Z"}
{"parentUuid":"b831de62-152f-4e37-a6dd-25295b26470c","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_0125kTZ72oMYmhLfzPUrZYzg","type":"tool_result","content":"     1→\"\"\"\n     2→Structured Logger for enhanced logging with consistent format and metadata.\n     3→\"\"\"\n     4→\n     5→import logging\n     6→import json\n     7→import sys\n     8→from typing import Any, Dict, Optional\n     9→from datetime import datetime, timezone\n    10→from dataclasses import dataclass, field, asdict\n    11→\n    12→\n    13→@dataclass\n    14→class LogContext:\n    15→    \"\"\"Structured logging context.\"\"\"\n    16→    service_name: str\n    17→    timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n    18→    level: str = \"\"\n    19→    message: str = \"\"\n    20→    extra: Dict[str, Any] = field(default_factory=dict)\n    21→\n    22→\n    23→class StructuredFormatter(logging.Formatter):\n    24→    \"\"\"JSON formatter for structured logging.\"\"\"\n    25→    \n    26→    def __init__(self, service_name: str):\n    27→        super().__init__()\n    28→        self.service_name = service_name\n    29→    \n    30→    def format(self, record: logging.LogRecord) -> str:\n    31→        \"\"\"Format log record as structured JSON.\"\"\"\n    32→        \n    33→        # Base log data\n    34→        log_data = {\n    35→            \"timestamp\": datetime.fromtimestamp(record.created, tz=timezone.utc).isoformat(),\n    36→            \"service\": self.service_name,\n    37→            \"level\": record.levelname,\n    38→            \"logger\": record.name,\n    39→            \"message\": record.getMessage(),\n    40→            \"module\": record.module,\n    41→            \"function\": record.funcName,\n    42→            \"line\": record.lineno\n    43→        }\n    44→        \n    45→        # Add thread info if available\n    46→        if hasattr(record, 'thread') and record.thread:\n    47→            log_data[\"thread_id\"] = record.thread\n    48→            log_data[\"thread_name\"] = getattr(record, 'threadName', '')\n    49→        \n    50→        # Add process info\n    51→        if hasattr(record, 'process') and record.process:\n    52→            log_data[\"process_id\"] = record.process\n    53→            log_data[\"process_name\"] = getattr(record, 'processName', '')\n    54→        \n    55→        # Add exception info if present\n    56→        if record.exc_info:\n    57→            log_data[\"exception\"] = {\n    58→                \"type\": record.exc_info[0].__name__,\n    59→                \"message\": str(record.exc_info[1]),\n    60→                \"traceback\": self.formatException(record.exc_info)\n    61→            }\n    62→        \n    63→        # Add any extra fields from the record\n    64→        extra_fields = {}\n    65→        for key, value in record.__dict__.items():\n    66→            if key not in ['name', 'msg', 'args', 'levelname', 'levelno', 'pathname', 'filename',\n    67→                          'module', 'exc_info', 'exc_text', 'stack_info', 'lineno', 'funcName',\n    68→                          'created', 'msecs', 'relativeCreated', 'thread', 'threadName',\n    69→                          'processName', 'process', 'message']:\n    70→                extra_fields[key] = value\n    71→        \n    72→        if extra_fields:\n    73→            log_data.update(extra_fields)\n    74→        \n    75→        return json.dumps(log_data, ensure_ascii=False, default=str)\n    76→\n    77→\n    78→class StructuredLogger:\n    79→    \"\"\"Enhanced structured logger with consistent formatting.\"\"\"\n    80→    \n    81→    def __init__(self, service_name: str, logger_name: Optional[str] = None):\n    82→        self.service_name = service_name\n    83→        self.logger = logging.getLogger(logger_name or __name__)\n    84→        \n    85→        # Set up structured formatting if not already configured\n    86→        if not self.logger.handlers:\n    87→            self._setup_structured_logging()\n    88→    \n    89→    def _setup_structured_logging(self):\n    90→        \"\"\"Set up structured JSON logging.\"\"\"\n    91→        \n    92→        # Create console handler with structured formatter\n    93→        handler = logging.StreamHandler(sys.stdout)\n    94→        handler.setFormatter(StructuredFormatter(self.service_name))\n    95→        \n    96→        self.logger.addHandler(handler)\n    97→        self.logger.setLevel(logging.INFO)\n    98→    \n    99→    def debug(self, message: str, extra: Optional[Dict[str, Any]] = None):\n   100→        \"\"\"Log debug message with structured format.\"\"\"\n   101→        self.logger.debug(message, extra=extra or {})\n   102→    \n   103→    def info(self, message: str, extra: Optional[Dict[str, Any]] = None):\n   104→        \"\"\"Log info message with structured format.\"\"\"\n   105→        self.logger.info(message, extra=extra or {})\n   106→    \n   107→    def warning(self, message: str, extra: Optional[Dict[str, Any]] = None):\n   108→        \"\"\"Log warning message with structured format.\"\"\"\n   109→        self.logger.warning(message, extra=extra or {})\n   110→    \n   111→    def error(self, message: str, extra: Optional[Dict[str, Any]] = None, exc_info: bool = False):\n   112→        \"\"\"Log error message with structured format.\"\"\"\n   113→        self.logger.error(message, extra=extra or {}, exc_info=exc_info)\n   114→    \n   115→    def critical(self, message: str, extra: Optional[Dict[str, Any]] = None, exc_info: bool = False):\n   116→        \"\"\"Log critical message with structured format.\"\"\"\n   117→        self.logger.critical(message, extra=extra or {}, exc_info=exc_info)\n   118→    \n   119→    def with_context(self, **context) -> 'ContextualLogger':\n   120→        \"\"\"Create a contextual logger with additional fields.\"\"\"\n   121→        return ContextualLogger(self, context)\n   122→\n   123→\n   124→class ContextualLogger:\n   125→    \"\"\"Logger with additional context that's added to all log messages.\"\"\"\n   126→    \n   127→    def __init__(self, base_logger: StructuredLogger, context: Dict[str, Any]):\n   128→        self.base_logger = base_logger\n   129→        self.context = context\n   130→    \n   131→    def _merge_extra(self, extra: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n   132→        \"\"\"Merge context with additional extra fields.\"\"\"\n   133→        merged = dict(self.context)\n   134→        if extra:\n   135→            merged.update(extra)\n   136→        return merged\n   137→    \n   138→    def debug(self, message: str, extra: Optional[Dict[str, Any]] = None):\n   139→        \"\"\"Log debug message with context.\"\"\"\n   140→        self.base_logger.debug(message, self._merge_extra(extra))\n   141→    \n   142→    def info(self, message: str, extra: Optional[Dict[str, Any]] = None):\n   143→        \"\"\"Log info message with context.\"\"\"\n   144→        self.base_logger.info(message, self._merge_extra(extra))\n   145→    \n   146→    def warning(self, message: str, extra: Optional[Dict[str, Any]] = None):\n   147→        \"\"\"Log warning message with context.\"\"\"\n   148→        self.base_logger.warning(message, self._merge_extra(extra))\n   149→    \n   150→    def error(self, message: str, extra: Optional[Dict[str, Any]] = None, exc_info: bool = False):\n   151→        \"\"\"Log error message with context.\"\"\"\n   152→        self.base_logger.error(message, self._merge_extra(extra), exc_info=exc_info)\n   153→    \n   154→    def critical(self, message: str, extra: Optional[Dict[str, Any]] = None, exc_info: bool = False):\n   155→        \"\"\"Log critical message with context.\"\"\"\n   156→        self.base_logger.critical(message, self._merge_extra(extra), exc_info=exc_info)\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"5158a5df-c774-406e-bd4f-5ce6f84ffdb8","timestamp":"2025-08-20T01:55:46.880Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/logging/structured_logger.py","content":"\"\"\"\nStructured Logger for enhanced logging with consistent format and metadata.\n\"\"\"\n\nimport logging\nimport json\nimport sys\nfrom typing import Any, Dict, Optional\nfrom datetime import datetime, timezone\nfrom dataclasses import dataclass, field, asdict\n\n\n@dataclass\nclass LogContext:\n    \"\"\"Structured logging context.\"\"\"\n    service_name: str\n    timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n    level: str = \"\"\n    message: str = \"\"\n    extra: Dict[str, Any] = field(default_factory=dict)\n\n\nclass StructuredFormatter(logging.Formatter):\n    \"\"\"JSON formatter for structured logging.\"\"\"\n    \n    def __init__(self, service_name: str):\n        super().__init__()\n        self.service_name = service_name\n    \n    def format(self, record: logging.LogRecord) -> str:\n        \"\"\"Format log record as structured JSON.\"\"\"\n        \n        # Base log data\n        log_data = {\n            \"timestamp\": datetime.fromtimestamp(record.created, tz=timezone.utc).isoformat(),\n            \"service\": self.service_name,\n            \"level\": record.levelname,\n            \"logger\": record.name,\n            \"message\": record.getMessage(),\n            \"module\": record.module,\n            \"function\": record.funcName,\n            \"line\": record.lineno\n        }\n        \n        # Add thread info if available\n        if hasattr(record, 'thread') and record.thread:\n            log_data[\"thread_id\"] = record.thread\n            log_data[\"thread_name\"] = getattr(record, 'threadName', '')\n        \n        # Add process info\n        if hasattr(record, 'process') and record.process:\n            log_data[\"process_id\"] = record.process\n            log_data[\"process_name\"] = getattr(record, 'processName', '')\n        \n        # Add exception info if present\n        if record.exc_info:\n            log_data[\"exception\"] = {\n                \"type\": record.exc_info[0].__name__,\n                \"message\": str(record.exc_info[1]),\n                \"traceback\": self.formatException(record.exc_info)\n            }\n        \n        # Add any extra fields from the record\n        extra_fields = {}\n        for key, value in record.__dict__.items():\n            if key not in ['name', 'msg', 'args', 'levelname', 'levelno', 'pathname', 'filename',\n                          'module', 'exc_info', 'exc_text', 'stack_info', 'lineno', 'funcName',\n                          'created', 'msecs', 'relativeCreated', 'thread', 'threadName',\n                          'processName', 'process', 'message']:\n                extra_fields[key] = value\n        \n        if extra_fields:\n            log_data.update(extra_fields)\n        \n        return json.dumps(log_data, ensure_ascii=False, default=str)\n\n\nclass StructuredLogger:\n    \"\"\"Enhanced structured logger with consistent formatting.\"\"\"\n    \n    def __init__(self, service_name: str, logger_name: Optional[str] = None):\n        self.service_name = service_name\n        self.logger = logging.getLogger(logger_name or __name__)\n        \n        # Set up structured formatting if not already configured\n        if not self.logger.handlers:\n            self._setup_structured_logging()\n    \n    def _setup_structured_logging(self):\n        \"\"\"Set up structured JSON logging.\"\"\"\n        \n        # Create console handler with structured formatter\n        handler = logging.StreamHandler(sys.stdout)\n        handler.setFormatter(StructuredFormatter(self.service_name))\n        \n        self.logger.addHandler(handler)\n        self.logger.setLevel(logging.INFO)\n    \n    def debug(self, message: str, extra: Optional[Dict[str, Any]] = None):\n        \"\"\"Log debug message with structured format.\"\"\"\n        self.logger.debug(message, extra=extra or {})\n    \n    def info(self, message: str, extra: Optional[Dict[str, Any]] = None):\n        \"\"\"Log info message with structured format.\"\"\"\n        self.logger.info(message, extra=extra or {})\n    \n    def warning(self, message: str, extra: Optional[Dict[str, Any]] = None):\n        \"\"\"Log warning message with structured format.\"\"\"\n        self.logger.warning(message, extra=extra or {})\n    \n    def error(self, message: str, extra: Optional[Dict[str, Any]] = None, exc_info: bool = False):\n        \"\"\"Log error message with structured format.\"\"\"\n        self.logger.error(message, extra=extra or {}, exc_info=exc_info)\n    \n    def critical(self, message: str, extra: Optional[Dict[str, Any]] = None, exc_info: bool = False):\n        \"\"\"Log critical message with structured format.\"\"\"\n        self.logger.critical(message, extra=extra or {}, exc_info=exc_info)\n    \n    def with_context(self, **context) -> 'ContextualLogger':\n        \"\"\"Create a contextual logger with additional fields.\"\"\"\n        return ContextualLogger(self, context)\n\n\nclass ContextualLogger:\n    \"\"\"Logger with additional context that's added to all log messages.\"\"\"\n    \n    def __init__(self, base_logger: StructuredLogger, context: Dict[str, Any]):\n        self.base_logger = base_logger\n        self.context = context\n    \n    def _merge_extra(self, extra: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"Merge context with additional extra fields.\"\"\"\n        merged = dict(self.context)\n        if extra:\n            merged.update(extra)\n        return merged\n    \n    def debug(self, message: str, extra: Optional[Dict[str, Any]] = None):\n        \"\"\"Log debug message with context.\"\"\"\n        self.base_logger.debug(message, self._merge_extra(extra))\n    \n    def info(self, message: str, extra: Optional[Dict[str, Any]] = None):\n        \"\"\"Log info message with context.\"\"\"\n        self.base_logger.info(message, self._merge_extra(extra))\n    \n    def warning(self, message: str, extra: Optional[Dict[str, Any]] = None):\n        \"\"\"Log warning message with context.\"\"\"\n        self.base_logger.warning(message, self._merge_extra(extra))\n    \n    def error(self, message: str, extra: Optional[Dict[str, Any]] = None, exc_info: bool = False):\n        \"\"\"Log error message with context.\"\"\"\n        self.base_logger.error(message, self._merge_extra(extra), exc_info=exc_info)\n    \n    def critical(self, message: str, extra: Optional[Dict[str, Any]] = None, exc_info: bool = False):\n        \"\"\"Log critical message with context.\"\"\"\n        self.base_logger.critical(message, self._merge_extra(extra), exc_info=exc_info)","numLines":156,"startLine":1,"totalLines":156}}}
{"parentUuid":"5158a5df-c774-406e-bd4f-5ce6f84ffdb8","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_014eXfxCwgGVzQgjAYqqNXPf","type":"tool_result","content":"     1→\"\"\"\n     2→Advanced Logging gRPC Interceptor\n     3→\n     4→This interceptor provides comprehensive logging, metrics collection,\n     5→and request tracing for gRPC services.\n     6→\"\"\"\n     7→\n     8→import grpc\n     9→import logging\n    10→import time\n    11→import json\n    12→import uuid\n    13→from typing import Callable, Any, Dict, Optional\n    14→from dataclasses import dataclass, asdict\n    15→from datetime import datetime, timezone\n    16→from functools import wraps\n    17→\n    18→from src.infrastructure.logging.structured_logger import StructuredLogger\n    19→\n    20→\n    21→@dataclass\n    22→class RequestMetrics:\n    23→    \"\"\"Request metrics for monitoring and analysis.\"\"\"\n    24→    request_id: str\n    25→    method_name: str\n    26→    start_time: float\n    27→    end_time: Optional[float] = None\n    28→    duration_ms: Optional[float] = None\n    29→    status_code: Optional[grpc.StatusCode] = None\n    30→    request_size_bytes: Optional[int] = None\n    31→    response_size_bytes: Optional[int] = None\n    32→    user_id: Optional[str] = None\n    33→    error_message: Optional[str] = None\n    34→\n    35→\n    36→class LoggingInterceptor(grpc.ServerInterceptor):\n    37→    \"\"\"\n    38→    Advanced logging interceptor with structured logging and metrics.\n    39→    \n    40→    Features:\n    41→    - Structured logging with correlation IDs\n    42→    - Request/response metrics collection\n    43→    - Performance monitoring\n    44→    - Error tracking and alerting\n    45→    - Audit logging for sensitive operations\n    46→    \"\"\"\n    47→    \n    48→    # Sensitive methods that require audit logging\n    49→    AUDIT_METHODS = {\n    50→        '/user.UserService/Login',\n    51→        '/user.UserService/RegisterUser',\n    52→        '/user.UserService/DeactivateUser',\n    53→        '/broker_account.BrokerAccountService/UpdateBrokerAccount',\n    54→    }\n    55→    \n    56→    # Methods to exclude from detailed logging (health checks, etc.)\n    57→    EXCLUDED_METHODS = {\n    58→        '/grpc.health.v1.Health/Check',\n    59→    }\n    60→    \n    61→    def __init__(self, \n    62→                 service_name: str = \"BEP-OMS-BROKER\",\n    63→                 log_requests: bool = True,\n    64→                 log_responses: bool = True,\n    65→                 log_performance: bool = True):\n    66→        self.service_name = service_name\n    67→        self.log_requests = log_requests\n    68→        self.log_responses = log_responses\n    69→        self.log_performance = log_performance\n    70→        \n    71→        # Setup loggers\n    72→        self.logger = logging.getLogger(__name__)\n    73→        self.audit_logger = logging.getLogger(f\"{__name__}.audit\")\n    74→        self.metrics_logger = logging.getLogger(f\"{__name__}.metrics\")\n    75→        self.structured_logger = StructuredLogger(service_name)\n    76→        \n    77→        # Metrics storage (in production, use proper metrics backend)\n    78→        self.request_metrics: Dict[str, RequestMetrics] = {}\n    79→    \n    80→    def intercept_service(self, continuation: Callable, handler_call_details: grpc.HandlerCallDetails) -> grpc.RpcMethodHandler:\n    81→        \"\"\"Intercept gRPC service calls for logging.\"\"\"\n    82→        \n    83→        def logging_wrapper(behavior: Callable) -> Callable:\n    84→            def new_behavior(request: Any, context: grpc.ServicerContext) -> Any:\n    85→                method_name = handler_call_details.method\n    86→                \n    87→                # Skip logging for excluded methods\n    88→                if method_name in self.EXCLUDED_METHODS:\n    89→                    return behavior(request, context)\n    90→                \n    91→                # Generate request ID if not set by previous interceptor\n    92→                request_id = getattr(context, 'request_id', str(uuid.uuid4()))\n    93→                context.request_id = request_id\n    94→                \n    95→                # Initialize metrics\n    96→                metrics = RequestMetrics(\n    97→                    request_id=request_id,\n    98→                    method_name=method_name,\n    99→                    start_time=time.time(),\n   100→                    request_size_bytes=self._calculate_message_size(request)\n   101→                )\n   102→                \n   103→                # Get user context if available (set by auth interceptor)\n   104→                auth_context = getattr(context, 'auth_context', None)\n   105→                if auth_context:\n   106→                    metrics.user_id = auth_context.user_id\n   107→                \n   108→                try:\n   109→                    # Log incoming request\n   110→                    self._log_request(request, context, metrics, auth_context)\n   111→                    \n   112→                    # Call the actual service method\n   113→                    start_time = time.time()\n   114→                    response = behavior(request, context)\n   115→                    end_time = time.time()\n   116→                    \n   117→                    # Update metrics\n   118→                    metrics.end_time = end_time\n   119→                    metrics.duration_ms = (end_time - start_time) * 1000\n   120→                    metrics.status_code = grpc.StatusCode.OK\n   121→                    metrics.response_size_bytes = self._calculate_message_size(response)\n   122→                    \n   123→                    # Log successful response\n   124→                    self._log_response(response, context, metrics, auth_context)\n   125→                    \n   126→                    # Log performance metrics\n   127→                    if self.log_performance:\n   128→                        self._log_performance_metrics(metrics)\n   129→                    \n   130→                    # Audit logging for sensitive operations\n   131→                    if method_name in self.AUDIT_METHODS:\n   132→                        self._log_audit_event(request, response, context, metrics, auth_context, success=True)\n   133→                    \n   134→                    return response\n   135→                    \n   136→                except Exception as e:\n   137→                    # Update metrics with error information\n   138→                    metrics.end_time = time.time()\n   139→                    metrics.duration_ms = (metrics.end_time - metrics.start_time) * 1000\n   140→                    metrics.error_message = str(e)\n   141→                    \n   142→                    # Determine gRPC status code\n   143→                    if hasattr(context, '_state') and hasattr(context._state, 'code'):\n   144→                        metrics.status_code = context._state.code\n   145→                    else:\n   146→                        metrics.status_code = grpc.StatusCode.INTERNAL\n   147→                    \n   148→                    # Log error\n   149→                    self._log_error(e, context, metrics, auth_context)\n   150→                    \n   151→                    # Audit logging for failed sensitive operations\n   152→                    if method_name in self.AUDIT_METHODS:\n   153→                        self._log_audit_event(request, None, context, metrics, auth_context, success=False, error=e)\n   154→                    \n   155→                    # Re-raise the exception\n   156→                    raise\n   157→                \n   158→                finally:\n   159→                    # Store metrics for monitoring\n   160→                    self.request_metrics[request_id] = metrics\n   161→                    \n   162→                    # Clean up old metrics (keep last 1000 requests)\n   163→                    if len(self.request_metrics) > 1000:\n   164→                        oldest_keys = list(self.request_metrics.keys())[:-1000]\n   165→                        for key in oldest_keys:\n   166→                            del self.request_metrics[key]\n   167→            \n   168→            return new_behavior\n   169→        \n   170→        # Get the original handler\n   171→        handler = continuation(handler_call_details)\n   172→        \n   173→        if handler is None:\n   174→            return None\n   175→        \n   176→        # Wrap the handler with logging\n   177→        return grpc.unary_unary_rpc_method_handler(\n   178→            logging_wrapper(handler.unary_unary),\n   179→            request_deserializer=handler.request_deserializer,\n   180→            response_serializer=handler.response_serializer,\n   181→        )\n   182→    \n   183→    def _log_request(self, request: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n   184→        \"\"\"Log incoming gRPC request.\"\"\"\n   185→        if not self.log_requests:\n   186→            return\n   187→        \n   188→        request_data = {\n   189→            \"request_id\": metrics.request_id,\n   190→            \"method\": metrics.method_name,\n   191→            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n   192→            \"user_id\": metrics.user_id,\n   193→            \"request_size_bytes\": metrics.request_size_bytes,\n   194→            \"client_metadata\": dict(context.invocation_metadata()) if context.invocation_metadata() else {}\n   195→        }\n   196→        \n   197→        # Add sanitized request data (remove sensitive fields)\n   198→        if hasattr(request, 'data'):\n   199→            request_data[\"request_data\"] = self._sanitize_request_data(request.data)\n   200→        \n   201→        self.structured_logger.info(\n   202→            \"gRPC request received\",\n   203→            extra=request_data\n   204→        )\n   205→    \n   206→    def _log_response(self, response: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n   207→        \"\"\"Log gRPC response.\"\"\"\n   208→        if not self.log_responses:\n   209→            return\n   210→        \n   211→        response_data = {\n   212→            \"request_id\": metrics.request_id,\n   213→            \"method\": metrics.method_name,\n   214→            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n   215→            \"duration_ms\": metrics.duration_ms,\n   216→            \"status\": \"SUCCESS\",\n   217→            \"status_code\": metrics.status_code.name if metrics.status_code else \"OK\",\n   218→            \"response_size_bytes\": metrics.response_size_bytes,\n   219→            \"user_id\": metrics.user_id\n   220→        }\n   221→        \n   222→        # Add sanitized response data\n   223→        if hasattr(response, 'code') and hasattr(response, 'message'):\n   224→            response_data[\"response_code\"] = response.code\n   225→            response_data[\"response_message\"] = response.message\n   226→        \n   227→        self.structured_logger.info(\n   228→            \"gRPC response sent\",\n   229→            extra=response_data\n   230→        )\n   231→    \n   232→    def _log_error(self, error: Exception, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n   233→        \"\"\"Log gRPC error.\"\"\"\n   234→        error_data = {\n   235→            \"request_id\": metrics.request_id,\n   236→            \"method\": metrics.method_name,\n   237→            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n   238→            \"duration_ms\": metrics.duration_ms,\n   239→            \"status\": \"ERROR\",\n   240→            \"status_code\": metrics.status_code.name if metrics.status_code else \"INTERNAL\",\n   241→            \"error_type\": type(error).__name__,\n   242→            \"error_message\": str(error),\n   243→            \"user_id\": metrics.user_id\n   244→        }\n   245→        \n   246→        self.structured_logger.error(\n   247→            \"gRPC request failed\",\n   248→            extra=error_data,\n   249→            exc_info=True\n   250→        )\n   251→    \n   252→    def _log_performance_metrics(self, metrics: RequestMetrics) -> None:\n   253→        \"\"\"Log performance metrics.\"\"\"\n   254→        perf_data = {\n   255→            \"request_id\": metrics.request_id,\n   256→            \"method\": metrics.method_name,\n   257→            \"duration_ms\": metrics.duration_ms,\n   258→            \"request_size_bytes\": metrics.request_size_bytes,\n   259→            \"response_size_bytes\": metrics.response_size_bytes,\n   260→            \"timestamp\": datetime.now(timezone.utc).isoformat()\n   261→        }\n   262→        \n   263→        self.metrics_logger.info(\"performance_metric\", extra=perf_data)\n   264→        \n   265→        # Alert on slow requests (> 5 seconds)\n   266→        if metrics.duration_ms and metrics.duration_ms > 5000:\n   267→            self.logger.warning(\n   268→                f\"Slow gRPC request detected: {metrics.method_name} \"\n   269→                f\"took {metrics.duration_ms:.2f}ms\",\n   270→                extra=perf_data\n   271→            )\n   272→    \n   273→    def _log_audit_event(self, request: Any, response: Any, context: grpc.ServicerContext, \n   274→                        metrics: RequestMetrics, auth_context: Any, success: bool, error: Exception = None) -> None:\n   275→        \"\"\"Log audit events for sensitive operations.\"\"\"\n   276→        audit_data = {\n   277→            \"event_type\": \"grpc_operation\",\n   278→            \"request_id\": metrics.request_id,\n   279→            \"method\": metrics.method_name,\n   280→            \"user_id\": metrics.user_id,\n   281→            \"user_email\": auth_context.email if auth_context else None,\n   282→            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n   283→            \"success\": success,\n   284→            \"duration_ms\": metrics.duration_ms,\n   285→            \"client_ip\": self._extract_client_ip(context),\n   286→            \"user_agent\": self._extract_user_agent(context)\n   287→        }\n   288→        \n   289→        if not success and error:\n   290→            audit_data[\"error_type\"] = type(error).__name__\n   291→            audit_data[\"error_message\"] = str(error)\n   292→        \n   293→        # Add operation-specific audit data\n   294→        if hasattr(request, 'data'):\n   295→            audit_data[\"operation_data\"] = self._sanitize_audit_data(request.data, metrics.method_name)\n   296→        \n   297→        self.audit_logger.info(\"audit_event\", extra=audit_data)\n   298→    \n   299→    def _sanitize_request_data(self, request_data: Any) -> Dict[str, Any]:\n   300→        \"\"\"Sanitize request data for logging (remove sensitive fields).\"\"\"\n   301→        if not request_data:\n   302→            return {}\n   303→        \n   304→        sensitive_fields = {'password', 'password_hash', 'token', 'secret', 'credentials', 'access_token'}\n   305→        \n   306→        sanitized = {}\n   307→        for field in request_data.DESCRIPTOR.fields:\n   308→            field_name = field.name\n   309→            if field_name.lower() in sensitive_fields:\n   310→                sanitized[field_name] = \"***REDACTED***\"\n   311→            else:\n   312→                value = getattr(request_data, field_name, None)\n   313→                if value is not None:\n   314→                    sanitized[field_name] = str(value)\n   315→        \n   316→        return sanitized\n   317→    \n   318→    def _sanitize_audit_data(self, request_data: Any, method_name: str) -> Dict[str, Any]:\n   319→        \"\"\"Sanitize data for audit logging.\"\"\"\n   320→        if not request_data:\n   321→            return {}\n   322→        \n   323→        # Method-specific sanitization rules\n   324→        if method_name == '/user.UserService/Login':\n   325→            return {\n   326→                \"email\": getattr(request_data, 'email', None),\n   327→                \"login_attempt\": True\n   328→            }\n   329→        elif method_name == '/user.UserService/RegisterUser':\n   330→            return {\n   331→                \"email\": getattr(request_data, 'email', None),\n   332→                \"username\": getattr(request_data, 'username', None),\n   333→                \"registration_attempt\": True\n   334→            }\n   335→        elif method_name == '/broker_account.BrokerAccountService/UpdateBrokerAccount':\n   336→            return {\n   337→                \"user_id\": getattr(request_data, 'user_id', None),\n   338→                \"broker_id\": getattr(request_data, 'broker_id', None),\n   339→                \"account_number\": getattr(request_data, 'account_number', None),\n   340→                \"update_attempt\": True\n   341→            }\n   342→        \n   343→        return self._sanitize_request_data(request_data)\n   344→    \n   345→    def _calculate_message_size(self, message: Any) -> Optional[int]:\n   346→        \"\"\"Calculate message size in bytes.\"\"\"\n   347→        try:\n   348→            if hasattr(message, 'SerializeToString'):\n   349→                return len(message.SerializeToString())\n   350→            return len(str(message).encode('utf-8'))\n   351→        except Exception:\n   352→            return None\n   353→    \n   354→    def _extract_client_ip(self, context: grpc.ServicerContext) -> Optional[str]:\n   355→        \"\"\"Extract client IP from gRPC context.\"\"\"\n   356→        try:\n   357→            peer = context.peer()\n   358→            if peer:\n   359→                # Format: \"ipv4:127.0.0.1:52050\" or \"ipv6:[::1]:52050\"\n   360→                if peer.startswith('ipv4:'):\n   361→                    return peer.split(':')[1]\n   362→                elif peer.startswith('ipv6:'):\n   363→                    return peer.split('[')[1].split(']')[0]\n   364→            return None\n   365→        except Exception:\n   366→            return None\n   367→    \n   368→    def _extract_user_agent(self, context: grpc.ServicerContext) -> Optional[str]:\n   369→        \"\"\"Extract user agent from gRPC metadata.\"\"\"\n   370→        try:\n   371→            metadata = dict(context.invocation_metadata())\n   372→            return metadata.get('user-agent') or metadata.get('User-Agent')\n   373→        except Exception:\n   374→            return None\n   375→    \n   376→    def get_metrics_summary(self) -> Dict[str, Any]:\n   377→        \"\"\"Get summary of collected metrics.\"\"\"\n   378→        if not self.request_metrics:\n   379→            return {}\n   380→        \n   381→        total_requests = len(self.request_metrics)\n   382→        successful_requests = sum(1 for m in self.request_metrics.values() \n   383→                                if m.status_code == grpc.StatusCode.OK)\n   384→        failed_requests = total_requests - successful_requests\n   385→        \n   386→        durations = [m.duration_ms for m in self.request_metrics.values() if m.duration_ms]\n   387→        avg_duration = sum(durations) / len(durations) if durations else 0\n   388→        \n   389→        return {\n   390→            \"total_requests\": total_requests,\n   391→            \"successful_requests\": successful_requests,\n   392→            \"failed_requests\": failed_requests,\n   393→            \"success_rate\": successful_requests / total_requests if total_requests > 0 else 0,\n   394→            \"average_duration_ms\": avg_duration,\n   395→            \"methods\": list(set(m.method_name for m in self.request_metrics.values()))\n   396→        }\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"bda01209-b6ae-4d6a-b0fb-e06b29b8b5e9","timestamp":"2025-08-20T01:55:46.880Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/grpc/interceptors/logging_interceptor.py","content":"\"\"\"\nAdvanced Logging gRPC Interceptor\n\nThis interceptor provides comprehensive logging, metrics collection,\nand request tracing for gRPC services.\n\"\"\"\n\nimport grpc\nimport logging\nimport time\nimport json\nimport uuid\nfrom typing import Callable, Any, Dict, Optional\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime, timezone\nfrom functools import wraps\n\nfrom src.infrastructure.logging.structured_logger import StructuredLogger\n\n\n@dataclass\nclass RequestMetrics:\n    \"\"\"Request metrics for monitoring and analysis.\"\"\"\n    request_id: str\n    method_name: str\n    start_time: float\n    end_time: Optional[float] = None\n    duration_ms: Optional[float] = None\n    status_code: Optional[grpc.StatusCode] = None\n    request_size_bytes: Optional[int] = None\n    response_size_bytes: Optional[int] = None\n    user_id: Optional[str] = None\n    error_message: Optional[str] = None\n\n\nclass LoggingInterceptor(grpc.ServerInterceptor):\n    \"\"\"\n    Advanced logging interceptor with structured logging and metrics.\n    \n    Features:\n    - Structured logging with correlation IDs\n    - Request/response metrics collection\n    - Performance monitoring\n    - Error tracking and alerting\n    - Audit logging for sensitive operations\n    \"\"\"\n    \n    # Sensitive methods that require audit logging\n    AUDIT_METHODS = {\n        '/user.UserService/Login',\n        '/user.UserService/RegisterUser',\n        '/user.UserService/DeactivateUser',\n        '/broker_account.BrokerAccountService/UpdateBrokerAccount',\n    }\n    \n    # Methods to exclude from detailed logging (health checks, etc.)\n    EXCLUDED_METHODS = {\n        '/grpc.health.v1.Health/Check',\n    }\n    \n    def __init__(self, \n                 service_name: str = \"BEP-OMS-BROKER\",\n                 log_requests: bool = True,\n                 log_responses: bool = True,\n                 log_performance: bool = True):\n        self.service_name = service_name\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_performance = log_performance\n        \n        # Setup loggers\n        self.logger = logging.getLogger(__name__)\n        self.audit_logger = logging.getLogger(f\"{__name__}.audit\")\n        self.metrics_logger = logging.getLogger(f\"{__name__}.metrics\")\n        self.structured_logger = StructuredLogger(service_name)\n        \n        # Metrics storage (in production, use proper metrics backend)\n        self.request_metrics: Dict[str, RequestMetrics] = {}\n    \n    def intercept_service(self, continuation: Callable, handler_call_details: grpc.HandlerCallDetails) -> grpc.RpcMethodHandler:\n        \"\"\"Intercept gRPC service calls for logging.\"\"\"\n        \n        def logging_wrapper(behavior: Callable) -> Callable:\n            def new_behavior(request: Any, context: grpc.ServicerContext) -> Any:\n                method_name = handler_call_details.method\n                \n                # Skip logging for excluded methods\n                if method_name in self.EXCLUDED_METHODS:\n                    return behavior(request, context)\n                \n                # Generate request ID if not set by previous interceptor\n                request_id = getattr(context, 'request_id', str(uuid.uuid4()))\n                context.request_id = request_id\n                \n                # Initialize metrics\n                metrics = RequestMetrics(\n                    request_id=request_id,\n                    method_name=method_name,\n                    start_time=time.time(),\n                    request_size_bytes=self._calculate_message_size(request)\n                )\n                \n                # Get user context if available (set by auth interceptor)\n                auth_context = getattr(context, 'auth_context', None)\n                if auth_context:\n                    metrics.user_id = auth_context.user_id\n                \n                try:\n                    # Log incoming request\n                    self._log_request(request, context, metrics, auth_context)\n                    \n                    # Call the actual service method\n                    start_time = time.time()\n                    response = behavior(request, context)\n                    end_time = time.time()\n                    \n                    # Update metrics\n                    metrics.end_time = end_time\n                    metrics.duration_ms = (end_time - start_time) * 1000\n                    metrics.status_code = grpc.StatusCode.OK\n                    metrics.response_size_bytes = self._calculate_message_size(response)\n                    \n                    # Log successful response\n                    self._log_response(response, context, metrics, auth_context)\n                    \n                    # Log performance metrics\n                    if self.log_performance:\n                        self._log_performance_metrics(metrics)\n                    \n                    # Audit logging for sensitive operations\n                    if method_name in self.AUDIT_METHODS:\n                        self._log_audit_event(request, response, context, metrics, auth_context, success=True)\n                    \n                    return response\n                    \n                except Exception as e:\n                    # Update metrics with error information\n                    metrics.end_time = time.time()\n                    metrics.duration_ms = (metrics.end_time - metrics.start_time) * 1000\n                    metrics.error_message = str(e)\n                    \n                    # Determine gRPC status code\n                    if hasattr(context, '_state') and hasattr(context._state, 'code'):\n                        metrics.status_code = context._state.code\n                    else:\n                        metrics.status_code = grpc.StatusCode.INTERNAL\n                    \n                    # Log error\n                    self._log_error(e, context, metrics, auth_context)\n                    \n                    # Audit logging for failed sensitive operations\n                    if method_name in self.AUDIT_METHODS:\n                        self._log_audit_event(request, None, context, metrics, auth_context, success=False, error=e)\n                    \n                    # Re-raise the exception\n                    raise\n                \n                finally:\n                    # Store metrics for monitoring\n                    self.request_metrics[request_id] = metrics\n                    \n                    # Clean up old metrics (keep last 1000 requests)\n                    if len(self.request_metrics) > 1000:\n                        oldest_keys = list(self.request_metrics.keys())[:-1000]\n                        for key in oldest_keys:\n                            del self.request_metrics[key]\n            \n            return new_behavior\n        \n        # Get the original handler\n        handler = continuation(handler_call_details)\n        \n        if handler is None:\n            return None\n        \n        # Wrap the handler with logging\n        return grpc.unary_unary_rpc_method_handler(\n            logging_wrapper(handler.unary_unary),\n            request_deserializer=handler.request_deserializer,\n            response_serializer=handler.response_serializer,\n        )\n    \n    def _log_request(self, request: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log incoming gRPC request.\"\"\"\n        if not self.log_requests:\n            return\n        \n        request_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"user_id\": metrics.user_id,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"client_metadata\": dict(context.invocation_metadata()) if context.invocation_metadata() else {}\n        }\n        \n        # Add sanitized request data (remove sensitive fields)\n        if hasattr(request, 'data'):\n            request_data[\"request_data\"] = self._sanitize_request_data(request.data)\n        \n        self.structured_logger.info(\n            \"gRPC request received\",\n            extra=request_data\n        )\n    \n    def _log_response(self, response: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC response.\"\"\"\n        if not self.log_responses:\n            return\n        \n        response_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"SUCCESS\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"OK\",\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"user_id\": metrics.user_id\n        }\n        \n        # Add sanitized response data\n        if hasattr(response, 'code') and hasattr(response, 'message'):\n            response_data[\"response_code\"] = response.code\n            response_data[\"response_message\"] = response.message\n        \n        self.structured_logger.info(\n            \"gRPC response sent\",\n            extra=response_data\n        )\n    \n    def _log_error(self, error: Exception, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC error.\"\"\"\n        error_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"ERROR\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"INTERNAL\",\n            \"error_type\": type(error).__name__,\n            \"error_message\": str(error),\n            \"user_id\": metrics.user_id\n        }\n        \n        self.structured_logger.error(\n            \"gRPC request failed\",\n            extra=error_data,\n            exc_info=True\n        )\n    \n    def _log_performance_metrics(self, metrics: RequestMetrics) -> None:\n        \"\"\"Log performance metrics.\"\"\"\n        perf_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"duration_ms\": metrics.duration_ms,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"timestamp\": datetime.now(timezone.utc).isoformat()\n        }\n        \n        self.metrics_logger.info(\"performance_metric\", extra=perf_data)\n        \n        # Alert on slow requests (> 5 seconds)\n        if metrics.duration_ms and metrics.duration_ms > 5000:\n            self.logger.warning(\n                f\"Slow gRPC request detected: {metrics.method_name} \"\n                f\"took {metrics.duration_ms:.2f}ms\",\n                extra=perf_data\n            )\n    \n    def _log_audit_event(self, request: Any, response: Any, context: grpc.ServicerContext, \n                        metrics: RequestMetrics, auth_context: Any, success: bool, error: Exception = None) -> None:\n        \"\"\"Log audit events for sensitive operations.\"\"\"\n        audit_data = {\n            \"event_type\": \"grpc_operation\",\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"user_id\": metrics.user_id,\n            \"user_email\": auth_context.email if auth_context else None,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"success\": success,\n            \"duration_ms\": metrics.duration_ms,\n            \"client_ip\": self._extract_client_ip(context),\n            \"user_agent\": self._extract_user_agent(context)\n        }\n        \n        if not success and error:\n            audit_data[\"error_type\"] = type(error).__name__\n            audit_data[\"error_message\"] = str(error)\n        \n        # Add operation-specific audit data\n        if hasattr(request, 'data'):\n            audit_data[\"operation_data\"] = self._sanitize_audit_data(request.data, metrics.method_name)\n        \n        self.audit_logger.info(\"audit_event\", extra=audit_data)\n    \n    def _sanitize_request_data(self, request_data: Any) -> Dict[str, Any]:\n        \"\"\"Sanitize request data for logging (remove sensitive fields).\"\"\"\n        if not request_data:\n            return {}\n        \n        sensitive_fields = {'password', 'password_hash', 'token', 'secret', 'credentials', 'access_token'}\n        \n        sanitized = {}\n        for field in request_data.DESCRIPTOR.fields:\n            field_name = field.name\n            if field_name.lower() in sensitive_fields:\n                sanitized[field_name] = \"***REDACTED***\"\n            else:\n                value = getattr(request_data, field_name, None)\n                if value is not None:\n                    sanitized[field_name] = str(value)\n        \n        return sanitized\n    \n    def _sanitize_audit_data(self, request_data: Any, method_name: str) -> Dict[str, Any]:\n        \"\"\"Sanitize data for audit logging.\"\"\"\n        if not request_data:\n            return {}\n        \n        # Method-specific sanitization rules\n        if method_name == '/user.UserService/Login':\n            return {\n                \"email\": getattr(request_data, 'email', None),\n                \"login_attempt\": True\n            }\n        elif method_name == '/user.UserService/RegisterUser':\n            return {\n                \"email\": getattr(request_data, 'email', None),\n                \"username\": getattr(request_data, 'username', None),\n                \"registration_attempt\": True\n            }\n        elif method_name == '/broker_account.BrokerAccountService/UpdateBrokerAccount':\n            return {\n                \"user_id\": getattr(request_data, 'user_id', None),\n                \"broker_id\": getattr(request_data, 'broker_id', None),\n                \"account_number\": getattr(request_data, 'account_number', None),\n                \"update_attempt\": True\n            }\n        \n        return self._sanitize_request_data(request_data)\n    \n    def _calculate_message_size(self, message: Any) -> Optional[int]:\n        \"\"\"Calculate message size in bytes.\"\"\"\n        try:\n            if hasattr(message, 'SerializeToString'):\n                return len(message.SerializeToString())\n            return len(str(message).encode('utf-8'))\n        except Exception:\n            return None\n    \n    def _extract_client_ip(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract client IP from gRPC context.\"\"\"\n        try:\n            peer = context.peer()\n            if peer:\n                # Format: \"ipv4:127.0.0.1:52050\" or \"ipv6:[::1]:52050\"\n                if peer.startswith('ipv4:'):\n                    return peer.split(':')[1]\n                elif peer.startswith('ipv6:'):\n                    return peer.split('[')[1].split(']')[0]\n            return None\n        except Exception:\n            return None\n    \n    def _extract_user_agent(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract user agent from gRPC metadata.\"\"\"\n        try:\n            metadata = dict(context.invocation_metadata())\n            return metadata.get('user-agent') or metadata.get('User-Agent')\n        except Exception:\n            return None\n    \n    def get_metrics_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary of collected metrics.\"\"\"\n        if not self.request_metrics:\n            return {}\n        \n        total_requests = len(self.request_metrics)\n        successful_requests = sum(1 for m in self.request_metrics.values() \n                                if m.status_code == grpc.StatusCode.OK)\n        failed_requests = total_requests - successful_requests\n        \n        durations = [m.duration_ms for m in self.request_metrics.values() if m.duration_ms]\n        avg_duration = sum(durations) / len(durations) if durations else 0\n        \n        return {\n            \"total_requests\": total_requests,\n            \"successful_requests\": successful_requests,\n            \"failed_requests\": failed_requests,\n            \"success_rate\": successful_requests / total_requests if total_requests > 0 else 0,\n            \"average_duration_ms\": avg_duration,\n            \"methods\": list(set(m.method_name for m in self.request_metrics.values()))\n        }","numLines":396,"startLine":1,"totalLines":396}}}
{"parentUuid":"bda01209-b6ae-4d6a-b0fb-e06b29b8b5e9","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01QbWw4CmiUhnEtFs11yfnqN","type":"tool_result","content":"     1→import grpc\n     2→import logging\n     3→import sys\n     4→import os\n     5→import asyncio\n     6→from dotenv import load_dotenv\n     7→\n     8→load_dotenv()\n     9→\n    10→sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), 'src')))\n    11→\n    12→from src.infrastructure.di.container import Container\n    13→from src.presentation.grpc.server_config import EnhancedGrpcServerConfig\n    14→from src.application.services.order_application_service import OrderApplicationService\n    15→from src.presentation.grpc.servicers.order_servicer import OrderServicer\n    16→from src.infrastructure.config.aws_config import AWSConfig\n    17→from src.infrastructure.adapters.email.aws_ses_service import AWSSESService\n    18→from src.infrastructure.persistence.repositories.otp_repository import OTPRepository as OTPRepositoryImpl\n    19→from src.application.use_cases.otp.send_otp_email_use_case import SendOTPEmailUseCase\n    20→from src.application.use_cases.otp.verify_otp_use_case import VerifyOTPUseCase\n    21→from src.application.use_cases.otp.verify_registration_use_case import VerifyRegistrationUseCase\n    22→from src.application.use_cases.otp.change_password_use_cases import (\n    23→    RequestPasswordResetUseCase,\n    24→    ConfirmPasswordResetUseCase,\n    25→)\n    26→from src.infrastructure.persistence.async_database import get_async_session\n    27→\n    28→async def main():\n    29→    # Configure logging with detailed format\n    30→    logging.basicConfig(\n    31→        level=logging.DEBUG,\n    32→        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    33→        handlers=[\n    34→            logging.StreamHandler(),\n    35→            logging.FileHandler('grpc_server.log'),\n    36→            logging.FileHandler('server_output.log')\n    37→        ]\n    38→    )\n    39→    \n    40→    # Set specific loggers to appropriate levels\n    41→    logging.getLogger('sqlalchemy.engine').setLevel(logging.WARNING)  # Reduce SQL noise\n    42→    \n    43→    # Initialize DI container\n    44→    container = Container()\n    45→    await container.init_resources()\n    46→    container.wire(modules=[\"src.presentation.grpc.servicers\", \"src.application.services\"])\n    47→\n    48→    # Create enhanced server with authentication enabled\n    49→    server_config = EnhancedGrpcServerConfig(\n    50→        port=55051,\n    51→        max_workers=10,\n    52→        enable_reflection=True,\n    53→        enable_health_checks=True,\n    54→        enable_metrics=True,\n    55→        enable_authentication=True,\n    56→        enable_rate_limiting=False,\n    57→        enable_detailed_logging=True,\n    58→        jwt_secret_key=os.getenv(\"TOKEN_ENCRYPTION_KEY\", \"super-secret-key\")\n    59→    )\n    60→    \n    61→    # Helper to resolve possibly awaitable providers\n    62→    import inspect\n    63→    async def _resolve(maybe):\n    64→        return await maybe if inspect.isawaitable(maybe) else maybe\n    65→\n    66→    # Configure server dependencies first\n    67→    # Prepare OTP dependencies (async session)\n    68→    send_otp_uc = None\n    69→    verify_otp_uc = None\n    70→    verify_registration_uc = None\n    71→    request_change_password_uc = None\n    72→    confirm_change_password_uc = None\n    73→    try:\n    74→        aws_config = AWSConfig()\n    75→        ses_service = AWSSESService(aws_config)\n    76→        # Pass session factory callable to repository to avoid closing issues\n    77→        otp_repo = OTPRepositoryImpl(get_async_session)\n    78→        send_otp_uc = SendOTPEmailUseCase(ses_service, otp_repo)\n    79→        verify_otp_uc = VerifyOTPUseCase(otp_repo)\n    80→        # Resolve providers to concrete instances to avoid passing awaitables\n    81→        user_repo = await _resolve(container.user_repository())\n    82→        password_hasher = await _resolve(container.password_hasher())\n    83→\n    84→        verify_registration_uc = VerifyRegistrationUseCase(\n    85→            user_repository=user_repo,\n    86→            verify_otp_use_case=verify_otp_uc,\n    87→            otp_repository=otp_repo,\n    88→        )\n    89→        request_change_password_uc = RequestPasswordResetUseCase(\n    90→            user_repository=user_repo,\n    91→            send_otp_email_use_case=send_otp_uc,\n    92→        )\n    93→        confirm_change_password_uc = ConfirmPasswordResetUseCase(\n    94→            otp_repository=otp_repo,\n    95→            verify_otp_use_case=verify_otp_uc,\n    96→            user_repository=user_repo,\n    97→            password_hasher=password_hasher,\n    98→        )\n    99→        logging.info(\"OTP email feature enabled (AWS SES configured)\")\n   100→    except Exception as e:\n   101→        logging.warning(f\"OTP email feature disabled: {e}\")\n   102→\n   103→    user_app = await _resolve(container.user_application_service())\n   104→    broker_app = await _resolve(container.broker_account_application_service())\n   105→    market_data_app = await _resolve(container.market_data_application_service())\n   106→    order_app = await _resolve(container.order_application_service())\n   107→    \n   108→    # Portfolio dependencies\n   109→    get_portfolio_uc = await _resolve(container.get_portfolio_use_case())\n   110→    portfolio_domain_mapper = await _resolve(container.portfolio_domain_mapper())\n   111→\n   112→    server_config.configure_dependencies(\n   113→        user_application_service=user_app,\n   114→        broker_account_application_service=broker_app,\n   115→        market_data_application_service=market_data_app,\n   116→        order_application_service=order_app,\n   117→        use_cases={\n   118→            # User use cases for consistent fallback paths across flows\n   119→            'register_user': await _resolve(container.register_user_use_case()),\n   120→            'update_user_profile': await _resolve(container.update_user_profile_use_case()),\n   121→            'deactivate_user': await _resolve(container.deactivate_user_use_case()),\n   122→            'login_user': await _resolve(container.login_user_use_case()),\n   123→            'logout_user': await _resolve(container.logout_user_use_case()),\n   124→            # OTP flows\n   125→            'send_otp_email': send_otp_uc,\n   126→            'verify_otp': verify_otp_uc,\n   127→            'verify_registration': verify_registration_uc,\n   128→            'request_password_reset': request_change_password_uc,\n   129→            'confirm_password_reset': confirm_change_password_uc,\n   130→        },\n   131→        repositories={}\n   132→    )\n   133→    \n   134→    # Set portfolio dependencies manually\n   135→    server_config.get_portfolio_use_case = get_portfolio_uc\n   136→    server_config.portfolio_domain_mapper = portfolio_domain_mapper\n   137→    \n   138→    # Configure the server\n   139→    server_config.server = server_config.configure_server()\n   140→    \n   141→    # Start enhanced server\n   142→    print(\"Starting enhanced gRPC server on port 55051...\")\n   143→    print(\"Features enabled: Enhanced Servicers, Interceptors, Health Checks, Metrics\")\n   144→    logging.info(\"Enhanced gRPC server starting with Phase 3 improvements\")\n   145→    \n   146→    await server_config.start_server()\n   147→    await server_config.server.wait_for_termination()\n   148→\n   149→if __name__ == \"__main__\":\n   150→    asyncio.run(main())\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"9841a45f-0d36-4fee-91af-c3600186c774","timestamp":"2025-08-20T01:55:46.880Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/lee/Documents/src/bep-oms-broker/main.py","content":"import grpc\nimport logging\nimport sys\nimport os\nimport asyncio\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), 'src')))\n\nfrom src.infrastructure.di.container import Container\nfrom src.presentation.grpc.server_config import EnhancedGrpcServerConfig\nfrom src.application.services.order_application_service import OrderApplicationService\nfrom src.presentation.grpc.servicers.order_servicer import OrderServicer\nfrom src.infrastructure.config.aws_config import AWSConfig\nfrom src.infrastructure.adapters.email.aws_ses_service import AWSSESService\nfrom src.infrastructure.persistence.repositories.otp_repository import OTPRepository as OTPRepositoryImpl\nfrom src.application.use_cases.otp.send_otp_email_use_case import SendOTPEmailUseCase\nfrom src.application.use_cases.otp.verify_otp_use_case import VerifyOTPUseCase\nfrom src.application.use_cases.otp.verify_registration_use_case import VerifyRegistrationUseCase\nfrom src.application.use_cases.otp.change_password_use_cases import (\n    RequestPasswordResetUseCase,\n    ConfirmPasswordResetUseCase,\n)\nfrom src.infrastructure.persistence.async_database import get_async_session\n\nasync def main():\n    # Configure logging with detailed format\n    logging.basicConfig(\n        level=logging.DEBUG,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.StreamHandler(),\n            logging.FileHandler('grpc_server.log'),\n            logging.FileHandler('server_output.log')\n        ]\n    )\n    \n    # Set specific loggers to appropriate levels\n    logging.getLogger('sqlalchemy.engine').setLevel(logging.WARNING)  # Reduce SQL noise\n    \n    # Initialize DI container\n    container = Container()\n    await container.init_resources()\n    container.wire(modules=[\"src.presentation.grpc.servicers\", \"src.application.services\"])\n\n    # Create enhanced server with authentication enabled\n    server_config = EnhancedGrpcServerConfig(\n        port=55051,\n        max_workers=10,\n        enable_reflection=True,\n        enable_health_checks=True,\n        enable_metrics=True,\n        enable_authentication=True,\n        enable_rate_limiting=False,\n        enable_detailed_logging=True,\n        jwt_secret_key=os.getenv(\"TOKEN_ENCRYPTION_KEY\", \"super-secret-key\")\n    )\n    \n    # Helper to resolve possibly awaitable providers\n    import inspect\n    async def _resolve(maybe):\n        return await maybe if inspect.isawaitable(maybe) else maybe\n\n    # Configure server dependencies first\n    # Prepare OTP dependencies (async session)\n    send_otp_uc = None\n    verify_otp_uc = None\n    verify_registration_uc = None\n    request_change_password_uc = None\n    confirm_change_password_uc = None\n    try:\n        aws_config = AWSConfig()\n        ses_service = AWSSESService(aws_config)\n        # Pass session factory callable to repository to avoid closing issues\n        otp_repo = OTPRepositoryImpl(get_async_session)\n        send_otp_uc = SendOTPEmailUseCase(ses_service, otp_repo)\n        verify_otp_uc = VerifyOTPUseCase(otp_repo)\n        # Resolve providers to concrete instances to avoid passing awaitables\n        user_repo = await _resolve(container.user_repository())\n        password_hasher = await _resolve(container.password_hasher())\n\n        verify_registration_uc = VerifyRegistrationUseCase(\n            user_repository=user_repo,\n            verify_otp_use_case=verify_otp_uc,\n            otp_repository=otp_repo,\n        )\n        request_change_password_uc = RequestPasswordResetUseCase(\n            user_repository=user_repo,\n            send_otp_email_use_case=send_otp_uc,\n        )\n        confirm_change_password_uc = ConfirmPasswordResetUseCase(\n            otp_repository=otp_repo,\n            verify_otp_use_case=verify_otp_uc,\n            user_repository=user_repo,\n            password_hasher=password_hasher,\n        )\n        logging.info(\"OTP email feature enabled (AWS SES configured)\")\n    except Exception as e:\n        logging.warning(f\"OTP email feature disabled: {e}\")\n\n    user_app = await _resolve(container.user_application_service())\n    broker_app = await _resolve(container.broker_account_application_service())\n    market_data_app = await _resolve(container.market_data_application_service())\n    order_app = await _resolve(container.order_application_service())\n    \n    # Portfolio dependencies\n    get_portfolio_uc = await _resolve(container.get_portfolio_use_case())\n    portfolio_domain_mapper = await _resolve(container.portfolio_domain_mapper())\n\n    server_config.configure_dependencies(\n        user_application_service=user_app,\n        broker_account_application_service=broker_app,\n        market_data_application_service=market_data_app,\n        order_application_service=order_app,\n        use_cases={\n            # User use cases for consistent fallback paths across flows\n            'register_user': await _resolve(container.register_user_use_case()),\n            'update_user_profile': await _resolve(container.update_user_profile_use_case()),\n            'deactivate_user': await _resolve(container.deactivate_user_use_case()),\n            'login_user': await _resolve(container.login_user_use_case()),\n            'logout_user': await _resolve(container.logout_user_use_case()),\n            # OTP flows\n            'send_otp_email': send_otp_uc,\n            'verify_otp': verify_otp_uc,\n            'verify_registration': verify_registration_uc,\n            'request_password_reset': request_change_password_uc,\n            'confirm_password_reset': confirm_change_password_uc,\n        },\n        repositories={}\n    )\n    \n    # Set portfolio dependencies manually\n    server_config.get_portfolio_use_case = get_portfolio_uc\n    server_config.portfolio_domain_mapper = portfolio_domain_mapper\n    \n    # Configure the server\n    server_config.server = server_config.configure_server()\n    \n    # Start enhanced server\n    print(\"Starting enhanced gRPC server on port 55051...\")\n    print(\"Features enabled: Enhanced Servicers, Interceptors, Health Checks, Metrics\")\n    logging.info(\"Enhanced gRPC server starting with Phase 3 improvements\")\n    \n    await server_config.start_server()\n    await server_config.server.wait_for_termination()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())","numLines":150,"startLine":1,"totalLines":150}}}
{"parentUuid":"9841a45f-0d36-4fee-91af-c3600186c774","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01ErmqRHtig7mcAaWfiaCAhS","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01NqaJuC5Xc6U9DTGeF56FiK","name":"TodoWrite","input":{"todos":[{"id":"1","content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"completed","priority":"high"},{"id":"2","content":"Tạo interceptor gRPC để log request/response","status":"pending","priority":"high"},{"id":"3","content":"Thêm logging vào các servicer chính","status":"pending","priority":"high"},{"id":"4","content":"Cấu hình log format và level phù hợp","status":"pending","priority":"medium"}]}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":10374,"cache_read_input_tokens":21860,"cache_creation":{"ephemeral_5m_input_tokens":10374,"ephemeral_1h_input_tokens":0},"output_tokens":209,"service_tier":"standard"}},"requestId":"req_011CSJ8Vbanb5Z3vaHxqSino","type":"assistant","uuid":"833925be-3028-4cb2-80e3-01083d6d047d","timestamp":"2025-08-20T01:55:52.902Z"}
{"parentUuid":"833925be-3028-4cb2-80e3-01083d6d047d","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01NqaJuC5Xc6U9DTGeF56FiK","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"b99cb843-039d-4cf6-b546-74c2eed1163e","timestamp":"2025-08-20T01:55:52.935Z","toolUseResult":{"oldTodos":[{"content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"in_progress","priority":"high","id":"1"},{"content":"Tạo interceptor gRPC để log request/response","status":"pending","priority":"high","id":"2"},{"content":"Thêm logging vào các servicer chính","status":"pending","priority":"high","id":"3"},{"content":"Cấu hình log format và level phù hợp","status":"pending","priority":"medium","id":"4"}],"newTodos":[{"content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"completed","priority":"high","id":"1"},{"content":"Tạo interceptor gRPC để log request/response","status":"pending","priority":"high","id":"2"},{"content":"Thêm logging vào các servicer chính","status":"pending","priority":"high","id":"3"},{"content":"Cấu hình log format và level phù hợp","status":"pending","priority":"medium","id":"4"}]}}
{"parentUuid":"b99cb843-039d-4cf6-b546-74c2eed1163e","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_017YopNH49w2aY4f2bQR8zcD","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Tôi thấy hệ thống đã có logging interceptor khá chi tiết. Tuy nhiên, để có logging request/response đầy đủ hơn, tôi sẽ cải thiện interceptor hiện tại để log toàn bộ nội dung request và response (không chỉ metadata)."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":500,"cache_read_input_tokens":32234,"cache_creation":{"ephemeral_5m_input_tokens":500,"ephemeral_1h_input_tokens":0},"output_tokens":2,"service_tier":"standard"}},"requestId":"req_011CSJ8W3Umu8Mm3BdkdV1ue","type":"assistant","uuid":"e33a761d-b04f-4879-afe2-54c71efb06b5","timestamp":"2025-08-20T01:55:57.373Z"}
{"parentUuid":"e33a761d-b04f-4879-afe2-54c71efb06b5","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_017YopNH49w2aY4f2bQR8zcD","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_018bGDqBYxQCXD1vHBEqMr15","name":"TodoWrite","input":{"todos":[{"id":"1","content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"completed","priority":"high"},{"id":"2","content":"Tạo interceptor gRPC để log request/response","status":"in_progress","priority":"high"},{"id":"3","content":"Thêm logging vào các servicer chính","status":"pending","priority":"high"},{"id":"4","content":"Cấu hình log format và level phù hợp","status":"pending","priority":"medium"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":500,"cache_read_input_tokens":32234,"cache_creation":{"ephemeral_5m_input_tokens":500,"ephemeral_1h_input_tokens":0},"output_tokens":370,"service_tier":"standard"}},"requestId":"req_011CSJ8W3Umu8Mm3BdkdV1ue","type":"assistant","uuid":"e453b4d8-6d1b-47b8-9ea0-45c72a166695","timestamp":"2025-08-20T01:55:58.940Z"}
{"parentUuid":"e453b4d8-6d1b-47b8-9ea0-45c72a166695","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_018bGDqBYxQCXD1vHBEqMr15","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"e06f6af1-5c97-4799-9746-c8df898859d1","timestamp":"2025-08-20T01:55:58.977Z","toolUseResult":{"oldTodos":[{"content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"completed","priority":"high","id":"1"},{"content":"Tạo interceptor gRPC để log request/response","status":"pending","priority":"high","id":"2"},{"content":"Thêm logging vào các servicer chính","status":"pending","priority":"high","id":"3"},{"content":"Cấu hình log format và level phù hợp","status":"pending","priority":"medium","id":"4"}],"newTodos":[{"content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"completed","priority":"high","id":"1"},{"content":"Tạo interceptor gRPC để log request/response","status":"in_progress","priority":"high","id":"2"},{"content":"Thêm logging vào các servicer chính","status":"pending","priority":"high","id":"3"},{"content":"Cấu hình log format và level phù hợp","status":"pending","priority":"medium","id":"4"}]}}
{"parentUuid":"e06f6af1-5c97-4799-9746-c8df898859d1","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01CidcW9P6RXtJLzusKEd2em","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01FsCfxSssK9kHwTZCHAM6Cg","name":"Edit","input":{"file_path":"/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/grpc/interceptors/logging_interceptor.py","old_string":"    def _log_request(self, request: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log incoming gRPC request.\"\"\"\n        if not self.log_requests:\n            return\n        \n        request_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"user_id\": metrics.user_id,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"client_metadata\": dict(context.invocation_metadata()) if context.invocation_metadata() else {}\n        }\n        \n        # Add sanitized request data (remove sensitive fields)\n        if hasattr(request, 'data'):\n            request_data[\"request_data\"] = self._sanitize_request_data(request.data)\n        \n        self.structured_logger.info(\n            \"gRPC request received\",\n            extra=request_data\n        )","new_string":"    def _log_request(self, request: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log incoming gRPC request.\"\"\"\n        if not self.log_requests:\n            return\n        \n        request_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"user_id\": metrics.user_id,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"client_metadata\": dict(context.invocation_metadata()) if context.invocation_metadata() else {}\n        }\n        \n        # Add detailed request data (sanitized)\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # This is a protobuf message\n                request_data[\"request_payload\"] = self._proto_to_dict(request)\n            elif hasattr(request, 'data'):\n                request_data[\"request_data\"] = self._sanitize_request_data(request.data)\n            else:\n                # Fallback to string representation\n                request_data[\"request_str\"] = str(request)\n        except Exception as e:\n            self.logger.warning(f\"Could not serialize request data: {e}\")\n            request_data[\"request_error\"] = \"Could not serialize request\"\n        \n        self.structured_logger.info(\n            \"gRPC request received\",\n            extra=request_data\n        )"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":590,"cache_read_input_tokens":32734,"cache_creation":{"ephemeral_5m_input_tokens":590,"ephemeral_1h_input_tokens":0},"output_tokens":742,"service_tier":"standard"}},"requestId":"req_011CSJ8WV1gP62w2D6uMyE1T","type":"assistant","uuid":"566b7fe5-231d-47b0-838e-9e8bc645ed14","timestamp":"2025-08-20T01:56:09.632Z"}
{"parentUuid":"566b7fe5-231d-47b0-838e-9e8bc645ed14","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01FsCfxSssK9kHwTZCHAM6Cg","type":"tool_result","content":"The file /Users/lee/Documents/src/bep-oms-broker/src/infrastructure/grpc/interceptors/logging_interceptor.py has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n     1→\"\"\"\n     2→Advanced Logging gRPC Interceptor\n     3→\n     4→This interceptor provides comprehensive logging, metrics collection,\n     5→and request tracing for gRPC services.\n     6→\"\"\"\n     7→\n     8→import grpc\n     9→import logging\n    10→import time\n    11→import json\n    12→import uuid\n    13→from typing import Callable, Any, Dict, Optional\n    14→from dataclasses import dataclass, asdict\n    15→from datetime import datetime, timezone\n    16→from functools import wraps\n    17→\n    18→from src.infrastructure.logging.structured_logger import StructuredLogger\n    19→\n    20→\n    21→@dataclass\n    22→class RequestMetrics:\n    23→    \"\"\"Request metrics for monitoring and analysis.\"\"\"\n    24→    request_id: str\n    25→    method_name: str\n    26→    start_time: float\n    27→    end_time: Optional[float] = None\n    28→    duration_ms: Optional[float] = None\n    29→    status_code: Optional[grpc.StatusCode] = None\n    30→    request_size_bytes: Optional[int] = None\n    31→    response_size_bytes: Optional[int] = None\n    32→    user_id: Optional[str] = None\n    33→    error_message: Optional[str] = None\n    34→\n    35→\n    36→class LoggingInterceptor(grpc.ServerInterceptor):\n    37→    \"\"\"\n    38→    Advanced logging interceptor with structured logging and metrics.\n    39→    \n    40→    Features:\n    41→    - Structured logging with correlation IDs\n    42→    - Request/response metrics collection\n    43→    - Performance monitoring\n    44→    - Error tracking and alerting\n    45→    - Audit logging for sensitive operations\n    46→    \"\"\"\n    47→    \n    48→    # Sensitive methods that require audit logging\n    49→    AUDIT_METHODS = {\n    50→        '/user.UserService/Login',\n    51→        '/user.UserService/RegisterUser',\n    52→        '/user.UserService/DeactivateUser',\n    53→        '/broker_account.BrokerAccountService/UpdateBrokerAccount',\n    54→    }\n    55→    \n    56→    # Methods to exclude from detailed logging (health checks, etc.)\n    57→    EXCLUDED_METHODS = {\n    58→        '/grpc.health.v1.Health/Check',\n    59→    }\n    60→    \n    61→    def __init__(self, \n    62→                 service_name: str = \"BEP-OMS-BROKER\",\n    63→                 log_requests: bool = True,\n    64→                 log_responses: bool = True,\n    65→                 log_performance: bool = True):\n    66→        self.service_name = service_name\n    67→        self.log_requests = log_requests\n    68→        self.log_responses = log_responses\n    69→        self.log_performance = log_performance\n    70→        \n    71→        # Setup loggers\n    72→        self.logger = logging.getLogger(__name__)\n    73→        self.audit_logger = logging.getLogger(f\"{__name__}.audit\")\n    74→        self.metrics_logger = logging.getLogger(f\"{__name__}.metrics\")\n    75→        self.structured_logger = StructuredLogger(service_name)\n    76→        \n    77→        # Metrics storage (in production, use proper metrics backend)\n    78→        self.request_metrics: Dict[str, RequestMetrics] = {}\n    79→    \n    80→    def intercept_service(self, continuation: Callable, handler_call_details: grpc.HandlerCallDetails) -> grpc.RpcMethodHandler:\n    81→        \"\"\"Intercept gRPC service calls for logging.\"\"\"\n    82→        \n    83→        def logging_wrapper(behavior: Callable) -> Callable:\n    84→            def new_behavior(request: Any, context: grpc.ServicerContext) -> Any:\n    85→                method_name = handler_call_details.method\n    86→                \n    87→                # Skip logging for excluded methods\n    88→                if method_name in self.EXCLUDED_METHODS:\n    89→                    return behavior(request, context)\n    90→                \n    91→                # Generate request ID if not set by previous interceptor\n    92→                request_id = getattr(context, 'request_id', str(uuid.uuid4()))\n    93→                context.request_id = request_id\n    94→                \n    95→                # Initialize metrics\n    96→                metrics = RequestMetrics(\n    97→                    request_id=request_id,\n    98→                    method_name=method_name,\n    99→                    start_time=time.time(),\n   100→                    request_size_bytes=self._calculate_message_size(request)\n   101→                )\n   102→                \n   103→                # Get user context if available (set by auth interceptor)\n   104→                auth_context = getattr(context, 'auth_context', None)\n   105→                if auth_context:\n   106→                    metrics.user_id = auth_context.user_id\n   107→                \n   108→                try:\n   109→                    # Log incoming request\n   110→                    self._log_request(request, context, metrics, auth_context)\n   111→                    \n   112→                    # Call the actual service method\n   113→                    start_time = time.time()\n   114→                    response = behavior(request, context)\n   115→                    end_time = time.time()\n   116→                    \n   117→                    # Update metrics\n   118→                    metrics.end_time = end_time\n   119→                    metrics.duration_ms = (end_time - start_time) * 1000\n   120→                    metrics.status_code = grpc.StatusCode.OK\n   121→                    metrics.response_size_bytes = self._calculate_message_size(response)\n   122→                    \n   123→                    # Log successful response\n   124→                    self._log_response(response, context, metrics, auth_context)\n   125→                    \n   126→                    # Log performance metrics\n   127→                    if self.log_performance:\n   128→                        self._log_performance_metrics(metrics)\n   129→                    \n   130→                    # Audit logging for sensitive operations\n   131→                    if method_name in self.AUDIT_METHODS:\n   132→                        self._log_audit_event(request, response, context, metrics, auth_context, success=True)\n   133→                    \n   134→                    return response\n   135→                    \n   136→                except Exception as e:\n   137→                    # Update metrics with error information\n   138→                    metrics.end_time = time.time()\n   139→                    metrics.duration_ms = (metrics.end_time - metrics.start_time) * 1000\n   140→                    metrics.error_message = str(e)\n   141→                    \n   142→                    # Determine gRPC status code\n   143→                    if hasattr(context, '_state') and hasattr(context._state, 'code'):\n   144→                        metrics.status_code = context._state.code\n   145→                    else:\n   146→                        metrics.status_code = grpc.StatusCode.INTERNAL\n   147→                    \n   148→                    # Log error\n   149→                    self._log_error(e, context, metrics, auth_context)\n   150→                    \n   151→                    # Audit logging for failed sensitive operations\n   152→                    if method_name in self.AUDIT_METHODS:\n   153→                        self._log_audit_event(request, None, context, metrics, auth_context, success=False, error=e)\n   154→                    \n   155→                    # Re-raise the exception\n   156→                    raise\n   157→                \n   158→                finally:\n   159→                    # Store metrics for monitoring\n   160→                    self.request_metrics[request_id] = metrics\n   161→                    \n   162→                    # Clean up old metrics (keep last 1000 requests)\n   163→                    if len(self.request_metrics) > 1000:\n   164→                        oldest_keys = list(self.request_metrics.keys())[:-1000]\n   165→                        for key in oldest_keys:\n   166→                            del self.request_metrics[key]\n   167→            \n   168→            return new_behavior\n   169→        \n   170→        # Get the original handler\n   171→        handler = continuation(handler_call_details)\n   172→        \n   173→        if handler is None:\n   174→            return None\n   175→        \n   176→        # Wrap the handler with logging\n   177→        return grpc.unary_unary_rpc_method_handler(\n   178→            logging_wrapper(handler.unary_unary),\n   179→            request_deserializer=handler.request_deserializer,\n   180→            response_serializer=handler.response_serializer,\n   181→        )\n   182→    \n   183→    def _log_request(self, request: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n   184→        \"\"\"Log incoming gRPC request.\"\"\"\n   185→        if not self.log_requests:\n   186→            return\n   187→        \n   188→        request_data = {\n   189→            \"request_id\": metrics.request_id,\n   190→            \"method\": metrics.method_name,\n   191→            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n   192→            \"user_id\": metrics.user_id,\n   193→            \"request_size_bytes\": metrics.request_size_bytes,\n   194→            \"client_metadata\": dict(context.invocation_metadata()) if context.invocation_metadata() else {}\n   195→        }\n   196→        \n   197→        # Add detailed request data (sanitized)\n   198→        try:\n   199→            if hasattr(request, 'DESCRIPTOR'):\n   200→                # This is a protobuf message\n   201→                request_data[\"request_payload\"] = self._proto_to_dict(request)\n   202→            elif hasattr(request, 'data'):\n   203→                request_data[\"request_data\"] = self._sanitize_request_data(request.data)\n   204→            else:\n   205→                # Fallback to string representation\n   206→                request_data[\"request_str\"] = str(request)\n   207→        except Exception as e:\n   208→            self.logger.warning(f\"Could not serialize request data: {e}\")\n   209→            request_data[\"request_error\"] = \"Could not serialize request\"\n   210→        \n   211→        self.structured_logger.info(\n   212→            \"gRPC request received\",\n   213→            extra=request_data\n   214→        )\n   215→    \n   216→    def _log_response(self, response: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n   217→        \"\"\"Log gRPC response.\"\"\"\n   218→        if not self.log_responses:\n   219→            return\n   220→        \n   221→        response_data = {\n   222→            \"request_id\": metrics.request_id,\n   223→            \"method\": metrics.method_name,\n   224→            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n   225→            \"duration_ms\": metrics.duration_ms,\n   226→            \"status\": \"SUCCESS\",\n   227→            \"status_code\": metrics.status_code.name if metrics.status_code else \"OK\",\n   228→            \"response_size_bytes\": metrics.response_size_bytes,\n   229→            \"user_id\": metrics.user_id\n   230→        }\n   231→        \n   232→        # Add sanitized response data\n   233→        if hasattr(response, 'code') and hasattr(response, 'message'):\n   234→            response_data[\"response_code\"] = response.code\n   235→            response_data[\"response_message\"] = response.message\n   236→        \n   237→        self.structured_logger.info(\n   238→            \"gRPC response sent\",\n   239→            extra=response_data\n   240→        )\n   241→    \n   242→    def _log_error(self, error: Exception, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n   243→        \"\"\"Log gRPC error.\"\"\"\n   244→        error_data = {\n   245→            \"request_id\": metrics.request_id,\n   246→            \"method\": metrics.method_name,\n   247→            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n   248→            \"duration_ms\": metrics.duration_ms,\n   249→            \"status\": \"ERROR\",\n   250→            \"status_code\": metrics.status_code.name if metrics.status_code else \"INTERNAL\",\n   251→            \"error_type\": type(error).__name__,\n   252→            \"error_message\": str(error),\n   253→            \"user_id\": metrics.user_id\n   254→        }\n   255→        \n   256→        self.structured_logger.error(\n   257→            \"gRPC request failed\",\n   258→            extra=error_data,\n   259→            exc_info=True\n   260→        )\n   261→    \n   262→    def _log_performance_metrics(self, metrics: RequestMetrics) -> None:\n   263→        \"\"\"Log performance metrics.\"\"\"\n   264→        perf_data = {\n   265→            \"request_id\": metrics.request_id,\n   266→            \"method\": metrics.method_name,\n   267→            \"duration_ms\": metrics.duration_ms,\n   268→            \"request_size_bytes\": metrics.request_size_bytes,\n   269→            \"response_size_bytes\": metrics.response_size_bytes,\n   270→            \"timestamp\": datetime.now(timezone.utc).isoformat()\n   271→        }\n   272→        \n   273→        self.metrics_logger.info(\"performance_metric\", extra=perf_data)\n   274→        \n   275→        # Alert on slow requests (> 5 seconds)\n   276→        if metrics.duration_ms and metrics.duration_ms > 5000:\n   277→            self.logger.warning(\n   278→                f\"Slow gRPC request detected: {metrics.method_name} \"\n   279→                f\"took {metrics.duration_ms:.2f}ms\",\n   280→                extra=perf_data\n   281→            )\n   282→    \n   283→    def _log_audit_event(self, request: Any, response: Any, context: grpc.ServicerContext, \n   284→                        metrics: RequestMetrics, auth_context: Any, success: bool, error: Exception = None) -> None:\n   285→        \"\"\"Log audit events for sensitive operations.\"\"\"\n   286→        audit_data = {\n   287→            \"event_type\": \"grpc_operation\",\n   288→            \"request_id\": metrics.request_id,\n   289→            \"method\": metrics.method_name,\n   290→            \"user_id\": metrics.user_id,\n   291→            \"user_email\": auth_context.email if auth_context else None,\n   292→            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n   293→            \"success\": success,\n   294→            \"duration_ms\": metrics.duration_ms,\n   295→            \"client_ip\": self._extract_client_ip(context),\n   296→            \"user_agent\": self._extract_user_agent(context)\n   297→        }\n   298→        \n   299→        if not success and error:\n   300→            audit_data[\"error_type\"] = type(error).__name__\n   301→            audit_data[\"error_message\"] = str(error)\n   302→        \n   303→        # Add operation-specific audit data\n   304→        if hasattr(request, 'data'):\n   305→            audit_data[\"operation_data\"] = self._sanitize_audit_data(request.data, metrics.method_name)\n   306→        \n   307→        self.audit_logger.info(\"audit_event\", extra=audit_data)\n   308→    \n   309→    def _sanitize_request_data(self, request_data: Any) -> Dict[str, Any]:\n   310→        \"\"\"Sanitize request data for logging (remove sensitive fields).\"\"\"\n   311→        if not request_data:\n   312→            return {}\n   313→        \n   314→        sensitive_fields = {'password', 'password_hash', 'token', 'secret', 'credentials', 'access_token'}\n   315→        \n   316→        sanitized = {}\n   317→        for field in request_data.DESCRIPTOR.fields:\n   318→            field_name = field.name\n   319→            if field_name.lower() in sensitive_fields:\n   320→                sanitized[field_name] = \"***REDACTED***\"\n   321→            else:\n   322→                value = getattr(request_data, field_name, None)\n   323→                if value is not None:\n   324→                    sanitized[field_name] = str(value)\n   325→        \n   326→        return sanitized\n   327→    \n   328→    def _sanitize_audit_data(self, request_data: Any, method_name: str) -> Dict[str, Any]:\n   329→        \"\"\"Sanitize data for audit logging.\"\"\"\n   330→        if not request_data:\n   331→            return {}\n   332→        \n   333→        # Method-specific sanitization rules\n   334→        if method_name == '/user.UserService/Login':\n   335→            return {\n   336→                \"email\": getattr(request_data, 'email', None),\n   337→                \"login_attempt\": True\n   338→            }\n   339→        elif method_name == '/user.UserService/RegisterUser':\n   340→            return {\n   341→                \"email\": getattr(request_data, 'email', None),\n   342→                \"username\": getattr(request_data, 'username', None),\n   343→                \"registration_attempt\": True\n   344→            }\n   345→        elif method_name == '/broker_account.BrokerAccountService/UpdateBrokerAccount':\n   346→            return {\n   347→                \"user_id\": getattr(request_data, 'user_id', None),\n   348→                \"broker_id\": getattr(request_data, 'broker_id', None),\n   349→                \"account_number\": getattr(request_data, 'account_number', None),\n   350→                \"update_attempt\": True\n   351→            }\n   352→        \n   353→        return self._sanitize_request_data(request_data)\n   354→    \n   355→    def _calculate_message_size(self, message: Any) -> Optional[int]:\n   356→        \"\"\"Calculate message size in bytes.\"\"\"\n   357→        try:\n   358→            if hasattr(message, 'SerializeToString'):\n   359→                return len(message.SerializeToString())\n   360→            return len(str(message).encode('utf-8'))\n   361→        except Exception:\n   362→            return None\n   363→    \n   364→    def _extract_client_ip(self, context: grpc.ServicerContext) -> Optional[str]:\n   365→        \"\"\"Extract client IP from gRPC context.\"\"\"\n   366→        try:\n   367→            peer = context.peer()\n   368→            if peer:\n   369→                # Format: \"ipv4:127.0.0.1:52050\" or \"ipv6:[::1]:52050\"\n   370→                if peer.startswith('ipv4:'):\n   371→                    return peer.split(':')[1]\n   372→                elif peer.startswith('ipv6:'):\n   373→                    return peer.split('[')[1].split(']')[0]\n   374→            return None\n   375→        except Exception:\n   376→            return None\n   377→    \n   378→    def _extract_user_agent(self, context: grpc.ServicerContext) -> Optional[str]:\n   379→        \"\"\"Extract user agent from gRPC metadata.\"\"\"\n   380→        try:\n   381→            metadata = dict(context.invocation_metadata())\n   382→            return metadata.get('user-agent') or metadata.get('User-Agent')\n   383→        except Exception:\n   384→            return None\n   385→    \n   386→    def get_metrics_summary(self) -> Dict[str, Any]:\n   387→        \"\"\"Get summary of collected metrics.\"\"\"\n   388→        if not self.request_metrics:\n   389→            return {}\n   390→        \n   391→        total_requests = len(self.request_metrics)\n   392→        successful_requests = sum(1 for m in self.request_metrics.values() \n   393→                                if m.status_code == grpc.StatusCode.OK)\n   394→        failed_requests = total_requests - successful_requests\n   395→        \n   396→        durations = [m.duration_ms for m in self.request_metrics.values() if m.duration_ms]\n   397→        avg_duration = sum(durations) / len(durations) if durations else 0\n   398→        \n   399→        return {\n   400→            \"total_requests\": total_requests,\n   401→            \"successful_requests\": successful_requests,\n   402→            \"failed_requests\": failed_requests,\n   403→            \"success_rate\": successful_requests / total_requests if total_requests > 0 else 0,\n   404→            \"average_duration_ms\": avg_duration,\n   405→            \"methods\": list(set(m.method_name for m in self.request_metrics.values()))\n   406→        }"}]},"uuid":"c247ceb8-3809-49ab-ae96-0a0d986f3f11","timestamp":"2025-08-20T01:56:41.551Z","toolUseResult":{"filePath":"/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/grpc/interceptors/logging_interceptor.py","oldString":"\"\"\"\nAdvanced Logging gRPC Interceptor\n\nThis interceptor provides comprehensive logging, metrics collection,\nand request tracing for gRPC services.\n\"\"\"\n\nimport grpc\nimport logging\nimport time\nimport json\nimport uuid\nfrom typing import Callable, Any, Dict, Optional\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime, timezone\nfrom functools import wraps\n\nfrom src.infrastructure.logging.structured_logger import StructuredLogger\n\n\n@dataclass\nclass RequestMetrics:\n    \"\"\"Request metrics for monitoring and analysis.\"\"\"\n    request_id: str\n    method_name: str\n    start_time: float\n    end_time: Optional[float] = None\n    duration_ms: Optional[float] = None\n    status_code: Optional[grpc.StatusCode] = None\n    request_size_bytes: Optional[int] = None\n    response_size_bytes: Optional[int] = None\n    user_id: Optional[str] = None\n    error_message: Optional[str] = None\n\n\nclass LoggingInterceptor(grpc.ServerInterceptor):\n    \"\"\"\n    Advanced logging interceptor with structured logging and metrics.\n    \n    Features:\n    - Structured logging with correlation IDs\n    - Request/response metrics collection\n    - Performance monitoring\n    - Error tracking and alerting\n    - Audit logging for sensitive operations\n    \"\"\"\n    \n    # Sensitive methods that require audit logging\n    AUDIT_METHODS = {\n        '/user.UserService/Login',\n        '/user.UserService/RegisterUser',\n        '/user.UserService/DeactivateUser',\n        '/broker_account.BrokerAccountService/UpdateBrokerAccount',\n    }\n    \n    # Methods to exclude from detailed logging (health checks, etc.)\n    EXCLUDED_METHODS = {\n        '/grpc.health.v1.Health/Check',\n    }\n    \n    def __init__(self, \n                 service_name: str = \"BEP-OMS-BROKER\",\n                 log_requests: bool = True,\n                 log_responses: bool = True,\n                 log_performance: bool = True):\n        self.service_name = service_name\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_performance = log_performance\n        \n        # Setup loggers\n        self.logger = logging.getLogger(__name__)\n        self.audit_logger = logging.getLogger(f\"{__name__}.audit\")\n        self.metrics_logger = logging.getLogger(f\"{__name__}.metrics\")\n        self.structured_logger = StructuredLogger(service_name)\n        \n        # Metrics storage (in production, use proper metrics backend)\n        self.request_metrics: Dict[str, RequestMetrics] = {}\n    \n    def intercept_service(self, continuation: Callable, handler_call_details: grpc.HandlerCallDetails) -> grpc.RpcMethodHandler:\n        \"\"\"Intercept gRPC service calls for logging.\"\"\"\n        \n        def logging_wrapper(behavior: Callable) -> Callable:\n            def new_behavior(request: Any, context: grpc.ServicerContext) -> Any:\n                method_name = handler_call_details.method\n                \n                # Skip logging for excluded methods\n                if method_name in self.EXCLUDED_METHODS:\n                    return behavior(request, context)\n                \n                # Generate request ID if not set by previous interceptor\n                request_id = getattr(context, 'request_id', str(uuid.uuid4()))\n                context.request_id = request_id\n                \n                # Initialize metrics\n                metrics = RequestMetrics(\n                    request_id=request_id,\n                    method_name=method_name,\n                    start_time=time.time(),\n                    request_size_bytes=self._calculate_message_size(request)\n                )\n                \n                # Get user context if available (set by auth interceptor)\n                auth_context = getattr(context, 'auth_context', None)\n                if auth_context:\n                    metrics.user_id = auth_context.user_id\n                \n                try:\n                    # Log incoming request\n                    self._log_request(request, context, metrics, auth_context)\n                    \n                    # Call the actual service method\n                    start_time = time.time()\n                    response = behavior(request, context)\n                    end_time = time.time()\n                    \n                    # Update metrics\n                    metrics.end_time = end_time\n                    metrics.duration_ms = (end_time - start_time) * 1000\n                    metrics.status_code = grpc.StatusCode.OK\n                    metrics.response_size_bytes = self._calculate_message_size(response)\n                    \n                    # Log successful response\n                    self._log_response(response, context, metrics, auth_context)\n                    \n                    # Log performance metrics\n                    if self.log_performance:\n                        self._log_performance_metrics(metrics)\n                    \n                    # Audit logging for sensitive operations\n                    if method_name in self.AUDIT_METHODS:\n                        self._log_audit_event(request, response, context, metrics, auth_context, success=True)\n                    \n                    return response\n                    \n                except Exception as e:\n                    # Update metrics with error information\n                    metrics.end_time = time.time()\n                    metrics.duration_ms = (metrics.end_time - metrics.start_time) * 1000\n                    metrics.error_message = str(e)\n                    \n                    # Determine gRPC status code\n                    if hasattr(context, '_state') and hasattr(context._state, 'code'):\n                        metrics.status_code = context._state.code\n                    else:\n                        metrics.status_code = grpc.StatusCode.INTERNAL\n                    \n                    # Log error\n                    self._log_error(e, context, metrics, auth_context)\n                    \n                    # Audit logging for failed sensitive operations\n                    if method_name in self.AUDIT_METHODS:\n                        self._log_audit_event(request, None, context, metrics, auth_context, success=False, error=e)\n                    \n                    # Re-raise the exception\n                    raise\n                \n                finally:\n                    # Store metrics for monitoring\n                    self.request_metrics[request_id] = metrics\n                    \n                    # Clean up old metrics (keep last 1000 requests)\n                    if len(self.request_metrics) > 1000:\n                        oldest_keys = list(self.request_metrics.keys())[:-1000]\n                        for key in oldest_keys:\n                            del self.request_metrics[key]\n            \n            return new_behavior\n        \n        # Get the original handler\n        handler = continuation(handler_call_details)\n        \n        if handler is None:\n            return None\n        \n        # Wrap the handler with logging\n        return grpc.unary_unary_rpc_method_handler(\n            logging_wrapper(handler.unary_unary),\n            request_deserializer=handler.request_deserializer,\n            response_serializer=handler.response_serializer,\n        )\n    \n    def _log_request(self, request: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log incoming gRPC request.\"\"\"\n        if not self.log_requests:\n            return\n        \n        request_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"user_id\": metrics.user_id,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"client_metadata\": dict(context.invocation_metadata()) if context.invocation_metadata() else {}\n        }\n        \n        # Add sanitized request data (remove sensitive fields)\n        if hasattr(request, 'data'):\n            request_data[\"request_data\"] = self._sanitize_request_data(request.data)\n        \n        self.structured_logger.info(\n            \"gRPC request received\",\n            extra=request_data\n        )\n    \n    def _log_response(self, response: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC response.\"\"\"\n        if not self.log_responses:\n            return\n        \n        response_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"SUCCESS\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"OK\",\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"user_id\": metrics.user_id\n        }\n        \n        # Add sanitized response data\n        if hasattr(response, 'code') and hasattr(response, 'message'):\n            response_data[\"response_code\"] = response.code\n            response_data[\"response_message\"] = response.message\n        \n        self.structured_logger.info(\n            \"gRPC response sent\",\n            extra=response_data\n        )\n    \n    def _log_error(self, error: Exception, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC error.\"\"\"\n        error_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"ERROR\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"INTERNAL\",\n            \"error_type\": type(error).__name__,\n            \"error_message\": str(error),\n            \"user_id\": metrics.user_id\n        }\n        \n        self.structured_logger.error(\n            \"gRPC request failed\",\n            extra=error_data,\n            exc_info=True\n        )\n    \n    def _log_performance_metrics(self, metrics: RequestMetrics) -> None:\n        \"\"\"Log performance metrics.\"\"\"\n        perf_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"duration_ms\": metrics.duration_ms,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"timestamp\": datetime.now(timezone.utc).isoformat()\n        }\n        \n        self.metrics_logger.info(\"performance_metric\", extra=perf_data)\n        \n        # Alert on slow requests (> 5 seconds)\n        if metrics.duration_ms and metrics.duration_ms > 5000:\n            self.logger.warning(\n                f\"Slow gRPC request detected: {metrics.method_name} \"\n                f\"took {metrics.duration_ms:.2f}ms\",\n                extra=perf_data\n            )\n    \n    def _log_audit_event(self, request: Any, response: Any, context: grpc.ServicerContext, \n                        metrics: RequestMetrics, auth_context: Any, success: bool, error: Exception = None) -> None:\n        \"\"\"Log audit events for sensitive operations.\"\"\"\n        audit_data = {\n            \"event_type\": \"grpc_operation\",\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"user_id\": metrics.user_id,\n            \"user_email\": auth_context.email if auth_context else None,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"success\": success,\n            \"duration_ms\": metrics.duration_ms,\n            \"client_ip\": self._extract_client_ip(context),\n            \"user_agent\": self._extract_user_agent(context)\n        }\n        \n        if not success and error:\n            audit_data[\"error_type\"] = type(error).__name__\n            audit_data[\"error_message\"] = str(error)\n        \n        # Add operation-specific audit data\n        if hasattr(request, 'data'):\n            audit_data[\"operation_data\"] = self._sanitize_audit_data(request.data, metrics.method_name)\n        \n        self.audit_logger.info(\"audit_event\", extra=audit_data)\n    \n    def _sanitize_request_data(self, request_data: Any) -> Dict[str, Any]:\n        \"\"\"Sanitize request data for logging (remove sensitive fields).\"\"\"\n        if not request_data:\n            return {}\n        \n        sensitive_fields = {'password', 'password_hash', 'token', 'secret', 'credentials', 'access_token'}\n        \n        sanitized = {}\n        for field in request_data.DESCRIPTOR.fields:\n            field_name = field.name\n            if field_name.lower() in sensitive_fields:\n                sanitized[field_name] = \"***REDACTED***\"\n            else:\n                value = getattr(request_data, field_name, None)\n                if value is not None:\n                    sanitized[field_name] = str(value)\n        \n        return sanitized\n    \n    def _sanitize_audit_data(self, request_data: Any, method_name: str) -> Dict[str, Any]:\n        \"\"\"Sanitize data for audit logging.\"\"\"\n        if not request_data:\n            return {}\n        \n        # Method-specific sanitization rules\n        if method_name == '/user.UserService/Login':\n            return {\n                \"email\": getattr(request_data, 'email', None),\n                \"login_attempt\": True\n            }\n        elif method_name == '/user.UserService/RegisterUser':\n            return {\n                \"email\": getattr(request_data, 'email', None),\n                \"username\": getattr(request_data, 'username', None),\n                \"registration_attempt\": True\n            }\n        elif method_name == '/broker_account.BrokerAccountService/UpdateBrokerAccount':\n            return {\n                \"user_id\": getattr(request_data, 'user_id', None),\n                \"broker_id\": getattr(request_data, 'broker_id', None),\n                \"account_number\": getattr(request_data, 'account_number', None),\n                \"update_attempt\": True\n            }\n        \n        return self._sanitize_request_data(request_data)\n    \n    def _calculate_message_size(self, message: Any) -> Optional[int]:\n        \"\"\"Calculate message size in bytes.\"\"\"\n        try:\n            if hasattr(message, 'SerializeToString'):\n                return len(message.SerializeToString())\n            return len(str(message).encode('utf-8'))\n        except Exception:\n            return None\n    \n    def _extract_client_ip(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract client IP from gRPC context.\"\"\"\n        try:\n            peer = context.peer()\n            if peer:\n                # Format: \"ipv4:127.0.0.1:52050\" or \"ipv6:[::1]:52050\"\n                if peer.startswith('ipv4:'):\n                    return peer.split(':')[1]\n                elif peer.startswith('ipv6:'):\n                    return peer.split('[')[1].split(']')[0]\n            return None\n        except Exception:\n            return None\n    \n    def _extract_user_agent(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract user agent from gRPC metadata.\"\"\"\n        try:\n            metadata = dict(context.invocation_metadata())\n            return metadata.get('user-agent') or metadata.get('User-Agent')\n        except Exception:\n            return None\n    \n    def get_metrics_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary of collected metrics.\"\"\"\n        if not self.request_metrics:\n            return {}\n        \n        total_requests = len(self.request_metrics)\n        successful_requests = sum(1 for m in self.request_metrics.values() \n                                if m.status_code == grpc.StatusCode.OK)\n        failed_requests = total_requests - successful_requests\n        \n        durations = [m.duration_ms for m in self.request_metrics.values() if m.duration_ms]\n        avg_duration = sum(durations) / len(durations) if durations else 0\n        \n        return {\n            \"total_requests\": total_requests,\n            \"successful_requests\": successful_requests,\n            \"failed_requests\": failed_requests,\n            \"success_rate\": successful_requests / total_requests if total_requests > 0 else 0,\n            \"average_duration_ms\": avg_duration,\n            \"methods\": list(set(m.method_name for m in self.request_metrics.values()))\n        }","newString":"\"\"\"\nAdvanced Logging gRPC Interceptor\n\nThis interceptor provides comprehensive logging, metrics collection,\nand request tracing for gRPC services.\n\"\"\"\n\nimport grpc\nimport logging\nimport time\nimport json\nimport uuid\nfrom typing import Callable, Any, Dict, Optional\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime, timezone\nfrom functools import wraps\n\nfrom src.infrastructure.logging.structured_logger import StructuredLogger\n\n\n@dataclass\nclass RequestMetrics:\n    \"\"\"Request metrics for monitoring and analysis.\"\"\"\n    request_id: str\n    method_name: str\n    start_time: float\n    end_time: Optional[float] = None\n    duration_ms: Optional[float] = None\n    status_code: Optional[grpc.StatusCode] = None\n    request_size_bytes: Optional[int] = None\n    response_size_bytes: Optional[int] = None\n    user_id: Optional[str] = None\n    error_message: Optional[str] = None\n\n\nclass LoggingInterceptor(grpc.ServerInterceptor):\n    \"\"\"\n    Advanced logging interceptor with structured logging and metrics.\n    \n    Features:\n    - Structured logging with correlation IDs\n    - Request/response metrics collection\n    - Performance monitoring\n    - Error tracking and alerting\n    - Audit logging for sensitive operations\n    \"\"\"\n    \n    # Sensitive methods that require audit logging\n    AUDIT_METHODS = {\n        '/user.UserService/Login',\n        '/user.UserService/RegisterUser',\n        '/user.UserService/DeactivateUser',\n        '/broker_account.BrokerAccountService/UpdateBrokerAccount',\n    }\n    \n    # Methods to exclude from detailed logging (health checks, etc.)\n    EXCLUDED_METHODS = {\n        '/grpc.health.v1.Health/Check',\n    }\n    \n    def __init__(self, \n                 service_name: str = \"BEP-OMS-BROKER\",\n                 log_requests: bool = True,\n                 log_responses: bool = True,\n                 log_performance: bool = True):\n        self.service_name = service_name\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_performance = log_performance\n        \n        # Setup loggers\n        self.logger = logging.getLogger(__name__)\n        self.audit_logger = logging.getLogger(f\"{__name__}.audit\")\n        self.metrics_logger = logging.getLogger(f\"{__name__}.metrics\")\n        self.structured_logger = StructuredLogger(service_name)\n        \n        # Metrics storage (in production, use proper metrics backend)\n        self.request_metrics: Dict[str, RequestMetrics] = {}\n    \n    def intercept_service(self, continuation: Callable, handler_call_details: grpc.HandlerCallDetails) -> grpc.RpcMethodHandler:\n        \"\"\"Intercept gRPC service calls for logging.\"\"\"\n        \n        def logging_wrapper(behavior: Callable) -> Callable:\n            def new_behavior(request: Any, context: grpc.ServicerContext) -> Any:\n                method_name = handler_call_details.method\n                \n                # Skip logging for excluded methods\n                if method_name in self.EXCLUDED_METHODS:\n                    return behavior(request, context)\n                \n                # Generate request ID if not set by previous interceptor\n                request_id = getattr(context, 'request_id', str(uuid.uuid4()))\n                context.request_id = request_id\n                \n                # Initialize metrics\n                metrics = RequestMetrics(\n                    request_id=request_id,\n                    method_name=method_name,\n                    start_time=time.time(),\n                    request_size_bytes=self._calculate_message_size(request)\n                )\n                \n                # Get user context if available (set by auth interceptor)\n                auth_context = getattr(context, 'auth_context', None)\n                if auth_context:\n                    metrics.user_id = auth_context.user_id\n                \n                try:\n                    # Log incoming request\n                    self._log_request(request, context, metrics, auth_context)\n                    \n                    # Call the actual service method\n                    start_time = time.time()\n                    response = behavior(request, context)\n                    end_time = time.time()\n                    \n                    # Update metrics\n                    metrics.end_time = end_time\n                    metrics.duration_ms = (end_time - start_time) * 1000\n                    metrics.status_code = grpc.StatusCode.OK\n                    metrics.response_size_bytes = self._calculate_message_size(response)\n                    \n                    # Log successful response\n                    self._log_response(response, context, metrics, auth_context)\n                    \n                    # Log performance metrics\n                    if self.log_performance:\n                        self._log_performance_metrics(metrics)\n                    \n                    # Audit logging for sensitive operations\n                    if method_name in self.AUDIT_METHODS:\n                        self._log_audit_event(request, response, context, metrics, auth_context, success=True)\n                    \n                    return response\n                    \n                except Exception as e:\n                    # Update metrics with error information\n                    metrics.end_time = time.time()\n                    metrics.duration_ms = (metrics.end_time - metrics.start_time) * 1000\n                    metrics.error_message = str(e)\n                    \n                    # Determine gRPC status code\n                    if hasattr(context, '_state') and hasattr(context._state, 'code'):\n                        metrics.status_code = context._state.code\n                    else:\n                        metrics.status_code = grpc.StatusCode.INTERNAL\n                    \n                    # Log error\n                    self._log_error(e, context, metrics, auth_context)\n                    \n                    # Audit logging for failed sensitive operations\n                    if method_name in self.AUDIT_METHODS:\n                        self._log_audit_event(request, None, context, metrics, auth_context, success=False, error=e)\n                    \n                    # Re-raise the exception\n                    raise\n                \n                finally:\n                    # Store metrics for monitoring\n                    self.request_metrics[request_id] = metrics\n                    \n                    # Clean up old metrics (keep last 1000 requests)\n                    if len(self.request_metrics) > 1000:\n                        oldest_keys = list(self.request_metrics.keys())[:-1000]\n                        for key in oldest_keys:\n                            del self.request_metrics[key]\n            \n            return new_behavior\n        \n        # Get the original handler\n        handler = continuation(handler_call_details)\n        \n        if handler is None:\n            return None\n        \n        # Wrap the handler with logging\n        return grpc.unary_unary_rpc_method_handler(\n            logging_wrapper(handler.unary_unary),\n            request_deserializer=handler.request_deserializer,\n            response_serializer=handler.response_serializer,\n        )\n    \n    def _log_request(self, request: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log incoming gRPC request.\"\"\"\n        if not self.log_requests:\n            return\n        \n        request_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"user_id\": metrics.user_id,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"client_metadata\": dict(context.invocation_metadata()) if context.invocation_metadata() else {}\n        }\n        \n        # Add detailed request data (sanitized)\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # This is a protobuf message\n                request_data[\"request_payload\"] = self._proto_to_dict(request)\n            elif hasattr(request, 'data'):\n                request_data[\"request_data\"] = self._sanitize_request_data(request.data)\n            else:\n                # Fallback to string representation\n                request_data[\"request_str\"] = str(request)\n        except Exception as e:\n            self.logger.warning(f\"Could not serialize request data: {e}\")\n            request_data[\"request_error\"] = \"Could not serialize request\"\n        \n        self.structured_logger.info(\n            \"gRPC request received\",\n            extra=request_data\n        )\n    \n    def _log_response(self, response: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC response.\"\"\"\n        if not self.log_responses:\n            return\n        \n        response_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"SUCCESS\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"OK\",\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"user_id\": metrics.user_id\n        }\n        \n        # Add sanitized response data\n        if hasattr(response, 'code') and hasattr(response, 'message'):\n            response_data[\"response_code\"] = response.code\n            response_data[\"response_message\"] = response.message\n        \n        self.structured_logger.info(\n            \"gRPC response sent\",\n            extra=response_data\n        )\n    \n    def _log_error(self, error: Exception, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC error.\"\"\"\n        error_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"ERROR\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"INTERNAL\",\n            \"error_type\": type(error).__name__,\n            \"error_message\": str(error),\n            \"user_id\": metrics.user_id\n        }\n        \n        self.structured_logger.error(\n            \"gRPC request failed\",\n            extra=error_data,\n            exc_info=True\n        )\n    \n    def _log_performance_metrics(self, metrics: RequestMetrics) -> None:\n        \"\"\"Log performance metrics.\"\"\"\n        perf_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"duration_ms\": metrics.duration_ms,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"timestamp\": datetime.now(timezone.utc).isoformat()\n        }\n        \n        self.metrics_logger.info(\"performance_metric\", extra=perf_data)\n        \n        # Alert on slow requests (> 5 seconds)\n        if metrics.duration_ms and metrics.duration_ms > 5000:\n            self.logger.warning(\n                f\"Slow gRPC request detected: {metrics.method_name} \"\n                f\"took {metrics.duration_ms:.2f}ms\",\n                extra=perf_data\n            )\n    \n    def _log_audit_event(self, request: Any, response: Any, context: grpc.ServicerContext, \n                        metrics: RequestMetrics, auth_context: Any, success: bool, error: Exception = None) -> None:\n        \"\"\"Log audit events for sensitive operations.\"\"\"\n        audit_data = {\n            \"event_type\": \"grpc_operation\",\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"user_id\": metrics.user_id,\n            \"user_email\": auth_context.email if auth_context else None,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"success\": success,\n            \"duration_ms\": metrics.duration_ms,\n            \"client_ip\": self._extract_client_ip(context),\n            \"user_agent\": self._extract_user_agent(context)\n        }\n        \n        if not success and error:\n            audit_data[\"error_type\"] = type(error).__name__\n            audit_data[\"error_message\"] = str(error)\n        \n        # Add operation-specific audit data\n        if hasattr(request, 'data'):\n            audit_data[\"operation_data\"] = self._sanitize_audit_data(request.data, metrics.method_name)\n        \n        self.audit_logger.info(\"audit_event\", extra=audit_data)\n    \n    def _sanitize_request_data(self, request_data: Any) -> Dict[str, Any]:\n        \"\"\"Sanitize request data for logging (remove sensitive fields).\"\"\"\n        if not request_data:\n            return {}\n        \n        sensitive_fields = {'password', 'password_hash', 'token', 'secret', 'credentials', 'access_token'}\n        \n        sanitized = {}\n        for field in request_data.DESCRIPTOR.fields:\n            field_name = field.name\n            if field_name.lower() in sensitive_fields:\n                sanitized[field_name] = \"***REDACTED***\"\n            else:\n                value = getattr(request_data, field_name, None)\n                if value is not None:\n                    sanitized[field_name] = str(value)\n        \n        return sanitized\n    \n    def _sanitize_audit_data(self, request_data: Any, method_name: str) -> Dict[str, Any]:\n        \"\"\"Sanitize data for audit logging.\"\"\"\n        if not request_data:\n            return {}\n        \n        # Method-specific sanitization rules\n        if method_name == '/user.UserService/Login':\n            return {\n                \"email\": getattr(request_data, 'email', None),\n                \"login_attempt\": True\n            }\n        elif method_name == '/user.UserService/RegisterUser':\n            return {\n                \"email\": getattr(request_data, 'email', None),\n                \"username\": getattr(request_data, 'username', None),\n                \"registration_attempt\": True\n            }\n        elif method_name == '/broker_account.BrokerAccountService/UpdateBrokerAccount':\n            return {\n                \"user_id\": getattr(request_data, 'user_id', None),\n                \"broker_id\": getattr(request_data, 'broker_id', None),\n                \"account_number\": getattr(request_data, 'account_number', None),\n                \"update_attempt\": True\n            }\n        \n        return self._sanitize_request_data(request_data)\n    \n    def _calculate_message_size(self, message: Any) -> Optional[int]:\n        \"\"\"Calculate message size in bytes.\"\"\"\n        try:\n            if hasattr(message, 'SerializeToString'):\n                return len(message.SerializeToString())\n            return len(str(message).encode('utf-8'))\n        except Exception:\n            return None\n    \n    def _extract_client_ip(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract client IP from gRPC context.\"\"\"\n        try:\n            peer = context.peer()\n            if peer:\n                # Format: \"ipv4:127.0.0.1:52050\" or \"ipv6:[::1]:52050\"\n                if peer.startswith('ipv4:'):\n                    return peer.split(':')[1]\n                elif peer.startswith('ipv6:'):\n                    return peer.split('[')[1].split(']')[0]\n            return None\n        except Exception:\n            return None\n    \n    def _extract_user_agent(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract user agent from gRPC metadata.\"\"\"\n        try:\n            metadata = dict(context.invocation_metadata())\n            return metadata.get('user-agent') or metadata.get('User-Agent')\n        except Exception:\n            return None\n    \n    def get_metrics_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary of collected metrics.\"\"\"\n        if not self.request_metrics:\n            return {}\n        \n        total_requests = len(self.request_metrics)\n        successful_requests = sum(1 for m in self.request_metrics.values() \n                                if m.status_code == grpc.StatusCode.OK)\n        failed_requests = total_requests - successful_requests\n        \n        durations = [m.duration_ms for m in self.request_metrics.values() if m.duration_ms]\n        avg_duration = sum(durations) / len(durations) if durations else 0\n        \n        return {\n            \"total_requests\": total_requests,\n            \"successful_requests\": successful_requests,\n            \"failed_requests\": failed_requests,\n            \"success_rate\": successful_requests / total_requests if total_requests > 0 else 0,\n            \"average_duration_ms\": avg_duration,\n            \"methods\": list(set(m.method_name for m in self.request_metrics.values()))\n        }","originalFile":"\"\"\"\nAdvanced Logging gRPC Interceptor\n\nThis interceptor provides comprehensive logging, metrics collection,\nand request tracing for gRPC services.\n\"\"\"\n\nimport grpc\nimport logging\nimport time\nimport json\nimport uuid\nfrom typing import Callable, Any, Dict, Optional\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime, timezone\nfrom functools import wraps\n\nfrom src.infrastructure.logging.structured_logger import StructuredLogger\n\n\n@dataclass\nclass RequestMetrics:\n    \"\"\"Request metrics for monitoring and analysis.\"\"\"\n    request_id: str\n    method_name: str\n    start_time: float\n    end_time: Optional[float] = None\n    duration_ms: Optional[float] = None\n    status_code: Optional[grpc.StatusCode] = None\n    request_size_bytes: Optional[int] = None\n    response_size_bytes: Optional[int] = None\n    user_id: Optional[str] = None\n    error_message: Optional[str] = None\n\n\nclass LoggingInterceptor(grpc.ServerInterceptor):\n    \"\"\"\n    Advanced logging interceptor with structured logging and metrics.\n    \n    Features:\n    - Structured logging with correlation IDs\n    - Request/response metrics collection\n    - Performance monitoring\n    - Error tracking and alerting\n    - Audit logging for sensitive operations\n    \"\"\"\n    \n    # Sensitive methods that require audit logging\n    AUDIT_METHODS = {\n        '/user.UserService/Login',\n        '/user.UserService/RegisterUser',\n        '/user.UserService/DeactivateUser',\n        '/broker_account.BrokerAccountService/UpdateBrokerAccount',\n    }\n    \n    # Methods to exclude from detailed logging (health checks, etc.)\n    EXCLUDED_METHODS = {\n        '/grpc.health.v1.Health/Check',\n    }\n    \n    def __init__(self, \n                 service_name: str = \"BEP-OMS-BROKER\",\n                 log_requests: bool = True,\n                 log_responses: bool = True,\n                 log_performance: bool = True):\n        self.service_name = service_name\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_performance = log_performance\n        \n        # Setup loggers\n        self.logger = logging.getLogger(__name__)\n        self.audit_logger = logging.getLogger(f\"{__name__}.audit\")\n        self.metrics_logger = logging.getLogger(f\"{__name__}.metrics\")\n        self.structured_logger = StructuredLogger(service_name)\n        \n        # Metrics storage (in production, use proper metrics backend)\n        self.request_metrics: Dict[str, RequestMetrics] = {}\n    \n    def intercept_service(self, continuation: Callable, handler_call_details: grpc.HandlerCallDetails) -> grpc.RpcMethodHandler:\n        \"\"\"Intercept gRPC service calls for logging.\"\"\"\n        \n        def logging_wrapper(behavior: Callable) -> Callable:\n            def new_behavior(request: Any, context: grpc.ServicerContext) -> Any:\n                method_name = handler_call_details.method\n                \n                # Skip logging for excluded methods\n                if method_name in self.EXCLUDED_METHODS:\n                    return behavior(request, context)\n                \n                # Generate request ID if not set by previous interceptor\n                request_id = getattr(context, 'request_id', str(uuid.uuid4()))\n                context.request_id = request_id\n                \n                # Initialize metrics\n                metrics = RequestMetrics(\n                    request_id=request_id,\n                    method_name=method_name,\n                    start_time=time.time(),\n                    request_size_bytes=self._calculate_message_size(request)\n                )\n                \n                # Get user context if available (set by auth interceptor)\n                auth_context = getattr(context, 'auth_context', None)\n                if auth_context:\n                    metrics.user_id = auth_context.user_id\n                \n                try:\n                    # Log incoming request\n                    self._log_request(request, context, metrics, auth_context)\n                    \n                    # Call the actual service method\n                    start_time = time.time()\n                    response = behavior(request, context)\n                    end_time = time.time()\n                    \n                    # Update metrics\n                    metrics.end_time = end_time\n                    metrics.duration_ms = (end_time - start_time) * 1000\n                    metrics.status_code = grpc.StatusCode.OK\n                    metrics.response_size_bytes = self._calculate_message_size(response)\n                    \n                    # Log successful response\n                    self._log_response(response, context, metrics, auth_context)\n                    \n                    # Log performance metrics\n                    if self.log_performance:\n                        self._log_performance_metrics(metrics)\n                    \n                    # Audit logging for sensitive operations\n                    if method_name in self.AUDIT_METHODS:\n                        self._log_audit_event(request, response, context, metrics, auth_context, success=True)\n                    \n                    return response\n                    \n                except Exception as e:\n                    # Update metrics with error information\n                    metrics.end_time = time.time()\n                    metrics.duration_ms = (metrics.end_time - metrics.start_time) * 1000\n                    metrics.error_message = str(e)\n                    \n                    # Determine gRPC status code\n                    if hasattr(context, '_state') and hasattr(context._state, 'code'):\n                        metrics.status_code = context._state.code\n                    else:\n                        metrics.status_code = grpc.StatusCode.INTERNAL\n                    \n                    # Log error\n                    self._log_error(e, context, metrics, auth_context)\n                    \n                    # Audit logging for failed sensitive operations\n                    if method_name in self.AUDIT_METHODS:\n                        self._log_audit_event(request, None, context, metrics, auth_context, success=False, error=e)\n                    \n                    # Re-raise the exception\n                    raise\n                \n                finally:\n                    # Store metrics for monitoring\n                    self.request_metrics[request_id] = metrics\n                    \n                    # Clean up old metrics (keep last 1000 requests)\n                    if len(self.request_metrics) > 1000:\n                        oldest_keys = list(self.request_metrics.keys())[:-1000]\n                        for key in oldest_keys:\n                            del self.request_metrics[key]\n            \n            return new_behavior\n        \n        # Get the original handler\n        handler = continuation(handler_call_details)\n        \n        if handler is None:\n            return None\n        \n        # Wrap the handler with logging\n        return grpc.unary_unary_rpc_method_handler(\n            logging_wrapper(handler.unary_unary),\n            request_deserializer=handler.request_deserializer,\n            response_serializer=handler.response_serializer,\n        )\n    \n    def _log_request(self, request: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log incoming gRPC request.\"\"\"\n        if not self.log_requests:\n            return\n        \n        request_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"user_id\": metrics.user_id,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"client_metadata\": dict(context.invocation_metadata()) if context.invocation_metadata() else {}\n        }\n        \n        # Add sanitized request data (remove sensitive fields)\n        if hasattr(request, 'data'):\n            request_data[\"request_data\"] = self._sanitize_request_data(request.data)\n        \n        self.structured_logger.info(\n            \"gRPC request received\",\n            extra=request_data\n        )\n    \n    def _log_response(self, response: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC response.\"\"\"\n        if not self.log_responses:\n            return\n        \n        response_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"SUCCESS\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"OK\",\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"user_id\": metrics.user_id\n        }\n        \n        # Add sanitized response data\n        if hasattr(response, 'code') and hasattr(response, 'message'):\n            response_data[\"response_code\"] = response.code\n            response_data[\"response_message\"] = response.message\n        \n        self.structured_logger.info(\n            \"gRPC response sent\",\n            extra=response_data\n        )\n    \n    def _log_error(self, error: Exception, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC error.\"\"\"\n        error_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"ERROR\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"INTERNAL\",\n            \"error_type\": type(error).__name__,\n            \"error_message\": str(error),\n            \"user_id\": metrics.user_id\n        }\n        \n        self.structured_logger.error(\n            \"gRPC request failed\",\n            extra=error_data,\n            exc_info=True\n        )\n    \n    def _log_performance_metrics(self, metrics: RequestMetrics) -> None:\n        \"\"\"Log performance metrics.\"\"\"\n        perf_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"duration_ms\": metrics.duration_ms,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"timestamp\": datetime.now(timezone.utc).isoformat()\n        }\n        \n        self.metrics_logger.info(\"performance_metric\", extra=perf_data)\n        \n        # Alert on slow requests (> 5 seconds)\n        if metrics.duration_ms and metrics.duration_ms > 5000:\n            self.logger.warning(\n                f\"Slow gRPC request detected: {metrics.method_name} \"\n                f\"took {metrics.duration_ms:.2f}ms\",\n                extra=perf_data\n            )\n    \n    def _log_audit_event(self, request: Any, response: Any, context: grpc.ServicerContext, \n                        metrics: RequestMetrics, auth_context: Any, success: bool, error: Exception = None) -> None:\n        \"\"\"Log audit events for sensitive operations.\"\"\"\n        audit_data = {\n            \"event_type\": \"grpc_operation\",\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"user_id\": metrics.user_id,\n            \"user_email\": auth_context.email if auth_context else None,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"success\": success,\n            \"duration_ms\": metrics.duration_ms,\n            \"client_ip\": self._extract_client_ip(context),\n            \"user_agent\": self._extract_user_agent(context)\n        }\n        \n        if not success and error:\n            audit_data[\"error_type\"] = type(error).__name__\n            audit_data[\"error_message\"] = str(error)\n        \n        # Add operation-specific audit data\n        if hasattr(request, 'data'):\n            audit_data[\"operation_data\"] = self._sanitize_audit_data(request.data, metrics.method_name)\n        \n        self.audit_logger.info(\"audit_event\", extra=audit_data)\n    \n    def _sanitize_request_data(self, request_data: Any) -> Dict[str, Any]:\n        \"\"\"Sanitize request data for logging (remove sensitive fields).\"\"\"\n        if not request_data:\n            return {}\n        \n        sensitive_fields = {'password', 'password_hash', 'token', 'secret', 'credentials', 'access_token'}\n        \n        sanitized = {}\n        for field in request_data.DESCRIPTOR.fields:\n            field_name = field.name\n            if field_name.lower() in sensitive_fields:\n                sanitized[field_name] = \"***REDACTED***\"\n            else:\n                value = getattr(request_data, field_name, None)\n                if value is not None:\n                    sanitized[field_name] = str(value)\n        \n        return sanitized\n    \n    def _sanitize_audit_data(self, request_data: Any, method_name: str) -> Dict[str, Any]:\n        \"\"\"Sanitize data for audit logging.\"\"\"\n        if not request_data:\n            return {}\n        \n        # Method-specific sanitization rules\n        if method_name == '/user.UserService/Login':\n            return {\n                \"email\": getattr(request_data, 'email', None),\n                \"login_attempt\": True\n            }\n        elif method_name == '/user.UserService/RegisterUser':\n            return {\n                \"email\": getattr(request_data, 'email', None),\n                \"username\": getattr(request_data, 'username', None),\n                \"registration_attempt\": True\n            }\n        elif method_name == '/broker_account.BrokerAccountService/UpdateBrokerAccount':\n            return {\n                \"user_id\": getattr(request_data, 'user_id', None),\n                \"broker_id\": getattr(request_data, 'broker_id', None),\n                \"account_number\": getattr(request_data, 'account_number', None),\n                \"update_attempt\": True\n            }\n        \n        return self._sanitize_request_data(request_data)\n    \n    def _calculate_message_size(self, message: Any) -> Optional[int]:\n        \"\"\"Calculate message size in bytes.\"\"\"\n        try:\n            if hasattr(message, 'SerializeToString'):\n                return len(message.SerializeToString())\n            return len(str(message).encode('utf-8'))\n        except Exception:\n            return None\n    \n    def _extract_client_ip(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract client IP from gRPC context.\"\"\"\n        try:\n            peer = context.peer()\n            if peer:\n                # Format: \"ipv4:127.0.0.1:52050\" or \"ipv6:[::1]:52050\"\n                if peer.startswith('ipv4:'):\n                    return peer.split(':')[1]\n                elif peer.startswith('ipv6:'):\n                    return peer.split('[')[1].split(']')[0]\n            return None\n        except Exception:\n            return None\n    \n    def _extract_user_agent(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract user agent from gRPC metadata.\"\"\"\n        try:\n            metadata = dict(context.invocation_metadata())\n            return metadata.get('user-agent') or metadata.get('User-Agent')\n        except Exception:\n            return None\n    \n    def get_metrics_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary of collected metrics.\"\"\"\n        if not self.request_metrics:\n            return {}\n        \n        total_requests = len(self.request_metrics)\n        successful_requests = sum(1 for m in self.request_metrics.values() \n                                if m.status_code == grpc.StatusCode.OK)\n        failed_requests = total_requests - successful_requests\n        \n        durations = [m.duration_ms for m in self.request_metrics.values() if m.duration_ms]\n        avg_duration = sum(durations) / len(durations) if durations else 0\n        \n        return {\n            \"total_requests\": total_requests,\n            \"successful_requests\": successful_requests,\n            \"failed_requests\": failed_requests,\n            \"success_rate\": successful_requests / total_requests if total_requests > 0 else 0,\n            \"average_duration_ms\": avg_duration,\n            \"methods\": list(set(m.method_name for m in self.request_metrics.values()))\n        }","structuredPatch":[{"oldStart":194,"oldLines":9,"newStart":194,"newLines":19,"lines":["             \"client_metadata\": dict(context.invocation_metadata()) if context.invocation_metadata() else {}","         }","         ","-        # Add sanitized request data (remove sensitive fields)","-        if hasattr(request, 'data'):","-            request_data[\"request_data\"] = self._sanitize_request_data(request.data)","+        # Add detailed request data (sanitized)","+        try:","+            if hasattr(request, 'DESCRIPTOR'):","+                # This is a protobuf message","+                request_data[\"request_payload\"] = self._proto_to_dict(request)","+            elif hasattr(request, 'data'):","+                request_data[\"request_data\"] = self._sanitize_request_data(request.data)","+            else:","+                # Fallback to string representation","+                request_data[\"request_str\"] = str(request)","+        except Exception as e:","+            self.logger.warning(f\"Could not serialize request data: {e}\")","+            request_data[\"request_error\"] = \"Could not serialize request\"","         ","         self.structured_logger.info(","             \"gRPC request received\","]}],"userModified":false,"replaceAll":false}}
{"parentUuid":"c247ceb8-3809-49ab-ae96-0a0d986f3f11","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_014KK7yMaz6Xc9TkgYotVBt4","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Bây giờ tôi cần thêm method `_proto_to_dict` và cải thiện `_log_response` để log đầy đủ response payload:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":6354,"cache_read_input_tokens":33324,"cache_creation":{"ephemeral_5m_input_tokens":6354,"ephemeral_1h_input_tokens":0},"output_tokens":10,"service_tier":"standard"}},"requestId":"req_011CSJ8Ze6iCaTpA5GQ4Yryt","type":"assistant","uuid":"9d84ac88-58ca-480f-812b-429569aa5734","timestamp":"2025-08-20T01:56:47.146Z"}
{"parentUuid":"9d84ac88-58ca-480f-812b-429569aa5734","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_014KK7yMaz6Xc9TkgYotVBt4","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01QbGyorzYEU2Doahhmr24RP","name":"Edit","input":{"file_path":"/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/grpc/interceptors/logging_interceptor.py","old_string":"    def _log_response(self, response: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC response.\"\"\"\n        if not self.log_responses:\n            return\n        \n        response_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"SUCCESS\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"OK\",\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"user_id\": metrics.user_id\n        }\n        \n        # Add sanitized response data\n        if hasattr(response, 'code') and hasattr(response, 'message'):\n            response_data[\"response_code\"] = response.code\n            response_data[\"response_message\"] = response.message\n        \n        self.structured_logger.info(\n            \"gRPC response sent\",\n            extra=response_data\n        )","new_string":"    def _log_response(self, response: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC response.\"\"\"\n        if not self.log_responses:\n            return\n        \n        response_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"SUCCESS\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"OK\",\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"user_id\": metrics.user_id\n        }\n        \n        # Add detailed response data (sanitized)\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # This is a protobuf message\n                response_data[\"response_payload\"] = self._proto_to_dict(response, is_response=True)\n            elif hasattr(response, 'code') and hasattr(response, 'message'):\n                response_data[\"response_code\"] = response.code\n                response_data[\"response_message\"] = response.message\n            else:\n                # Fallback to string representation\n                response_data[\"response_str\"] = str(response)\n        except Exception as e:\n            self.logger.warning(f\"Could not serialize response data: {e}\")\n            response_data[\"response_error\"] = \"Could not serialize response\"\n        \n        self.structured_logger.info(\n            \"gRPC response sent\",\n            extra=response_data\n        )"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":6354,"cache_read_input_tokens":33324,"cache_creation":{"ephemeral_5m_input_tokens":6354,"ephemeral_1h_input_tokens":0},"output_tokens":851,"service_tier":"standard"}},"requestId":"req_011CSJ8Ze6iCaTpA5GQ4Yryt","type":"assistant","uuid":"f103decd-910c-4d7b-a336-ab12f13221d9","timestamp":"2025-08-20T01:56:54.835Z"}
{"parentUuid":"f103decd-910c-4d7b-a336-ab12f13221d9","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01QbGyorzYEU2Doahhmr24RP","type":"tool_result","content":"The file /Users/lee/Documents/src/bep-oms-broker/src/infrastructure/grpc/interceptors/logging_interceptor.py has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n     1→\"\"\"\n     2→Advanced Logging gRPC Interceptor\n     3→\n     4→This interceptor provides comprehensive logging, metrics collection,\n     5→and request tracing for gRPC services.\n     6→\"\"\"\n     7→\n     8→import grpc\n     9→import logging\n    10→import time\n    11→import json\n    12→import uuid\n    13→from typing import Callable, Any, Dict, Optional\n    14→from dataclasses import dataclass, asdict\n    15→from datetime import datetime, timezone\n    16→from functools import wraps\n    17→\n    18→from src.infrastructure.logging.structured_logger import StructuredLogger\n    19→\n    20→\n    21→@dataclass\n    22→class RequestMetrics:\n    23→    \"\"\"Request metrics for monitoring and analysis.\"\"\"\n    24→    request_id: str\n    25→    method_name: str\n    26→    start_time: float\n    27→    end_time: Optional[float] = None\n    28→    duration_ms: Optional[float] = None\n    29→    status_code: Optional[grpc.StatusCode] = None\n    30→    request_size_bytes: Optional[int] = None\n    31→    response_size_bytes: Optional[int] = None\n    32→    user_id: Optional[str] = None\n    33→    error_message: Optional[str] = None\n    34→\n    35→\n    36→class LoggingInterceptor(grpc.ServerInterceptor):\n    37→    \"\"\"\n    38→    Advanced logging interceptor with structured logging and metrics.\n    39→    \n    40→    Features:\n    41→    - Structured logging with correlation IDs\n    42→    - Request/response metrics collection\n    43→    - Performance monitoring\n    44→    - Error tracking and alerting\n    45→    - Audit logging for sensitive operations\n    46→    \"\"\"\n    47→    \n    48→    # Sensitive methods that require audit logging\n    49→    AUDIT_METHODS = {\n    50→        '/user.UserService/Login',\n    51→        '/user.UserService/RegisterUser',\n    52→        '/user.UserService/DeactivateUser',\n    53→        '/broker_account.BrokerAccountService/UpdateBrokerAccount',\n    54→    }\n    55→    \n    56→    # Methods to exclude from detailed logging (health checks, etc.)\n    57→    EXCLUDED_METHODS = {\n    58→        '/grpc.health.v1.Health/Check',\n    59→    }\n    60→    \n    61→    def __init__(self, \n    62→                 service_name: str = \"BEP-OMS-BROKER\",\n    63→                 log_requests: bool = True,\n    64→                 log_responses: bool = True,\n    65→                 log_performance: bool = True):\n    66→        self.service_name = service_name\n    67→        self.log_requests = log_requests\n    68→        self.log_responses = log_responses\n    69→        self.log_performance = log_performance\n    70→        \n    71→        # Setup loggers\n    72→        self.logger = logging.getLogger(__name__)\n    73→        self.audit_logger = logging.getLogger(f\"{__name__}.audit\")\n    74→        self.metrics_logger = logging.getLogger(f\"{__name__}.metrics\")\n    75→        self.structured_logger = StructuredLogger(service_name)\n    76→        \n    77→        # Metrics storage (in production, use proper metrics backend)\n    78→        self.request_metrics: Dict[str, RequestMetrics] = {}\n    79→    \n    80→    def intercept_service(self, continuation: Callable, handler_call_details: grpc.HandlerCallDetails) -> grpc.RpcMethodHandler:\n    81→        \"\"\"Intercept gRPC service calls for logging.\"\"\"\n    82→        \n    83→        def logging_wrapper(behavior: Callable) -> Callable:\n    84→            def new_behavior(request: Any, context: grpc.ServicerContext) -> Any:\n    85→                method_name = handler_call_details.method\n    86→                \n    87→                # Skip logging for excluded methods\n    88→                if method_name in self.EXCLUDED_METHODS:\n    89→                    return behavior(request, context)\n    90→                \n    91→                # Generate request ID if not set by previous interceptor\n    92→                request_id = getattr(context, 'request_id', str(uuid.uuid4()))\n    93→                context.request_id = request_id\n    94→                \n    95→                # Initialize metrics\n    96→                metrics = RequestMetrics(\n    97→                    request_id=request_id,\n    98→                    method_name=method_name,\n    99→                    start_time=time.time(),\n   100→                    request_size_bytes=self._calculate_message_size(request)\n   101→                )\n   102→                \n   103→                # Get user context if available (set by auth interceptor)\n   104→                auth_context = getattr(context, 'auth_context', None)\n   105→                if auth_context:\n   106→                    metrics.user_id = auth_context.user_id\n   107→                \n   108→                try:\n   109→                    # Log incoming request\n   110→                    self._log_request(request, context, metrics, auth_context)\n   111→                    \n   112→                    # Call the actual service method\n   113→                    start_time = time.time()\n   114→                    response = behavior(request, context)\n   115→                    end_time = time.time()\n   116→                    \n   117→                    # Update metrics\n   118→                    metrics.end_time = end_time\n   119→                    metrics.duration_ms = (end_time - start_time) * 1000\n   120→                    metrics.status_code = grpc.StatusCode.OK\n   121→                    metrics.response_size_bytes = self._calculate_message_size(response)\n   122→                    \n   123→                    # Log successful response\n   124→                    self._log_response(response, context, metrics, auth_context)\n   125→                    \n   126→                    # Log performance metrics\n   127→                    if self.log_performance:\n   128→                        self._log_performance_metrics(metrics)\n   129→                    \n   130→                    # Audit logging for sensitive operations\n   131→                    if method_name in self.AUDIT_METHODS:\n   132→                        self._log_audit_event(request, response, context, metrics, auth_context, success=True)\n   133→                    \n   134→                    return response\n   135→                    \n   136→                except Exception as e:\n   137→                    # Update metrics with error information\n   138→                    metrics.end_time = time.time()\n   139→                    metrics.duration_ms = (metrics.end_time - metrics.start_time) * 1000\n   140→                    metrics.error_message = str(e)\n   141→                    \n   142→                    # Determine gRPC status code\n   143→                    if hasattr(context, '_state') and hasattr(context._state, 'code'):\n   144→                        metrics.status_code = context._state.code\n   145→                    else:\n   146→                        metrics.status_code = grpc.StatusCode.INTERNAL\n   147→                    \n   148→                    # Log error\n   149→                    self._log_error(e, context, metrics, auth_context)\n   150→                    \n   151→                    # Audit logging for failed sensitive operations\n   152→                    if method_name in self.AUDIT_METHODS:\n   153→                        self._log_audit_event(request, None, context, metrics, auth_context, success=False, error=e)\n   154→                    \n   155→                    # Re-raise the exception\n   156→                    raise\n   157→                \n   158→                finally:\n   159→                    # Store metrics for monitoring\n   160→                    self.request_metrics[request_id] = metrics\n   161→                    \n   162→                    # Clean up old metrics (keep last 1000 requests)\n   163→                    if len(self.request_metrics) > 1000:\n   164→                        oldest_keys = list(self.request_metrics.keys())[:-1000]\n   165→                        for key in oldest_keys:\n   166→                            del self.request_metrics[key]\n   167→            \n   168→            return new_behavior\n   169→        \n   170→        # Get the original handler\n   171→        handler = continuation(handler_call_details)\n   172→        \n   173→        if handler is None:\n   174→            return None\n   175→        \n   176→        # Wrap the handler with logging\n   177→        return grpc.unary_unary_rpc_method_handler(\n   178→            logging_wrapper(handler.unary_unary),\n   179→            request_deserializer=handler.request_deserializer,\n   180→            response_serializer=handler.response_serializer,\n   181→        )\n   182→    \n   183→    def _log_request(self, request: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n   184→        \"\"\"Log incoming gRPC request.\"\"\"\n   185→        if not self.log_requests:\n   186→            return\n   187→        \n   188→        request_data = {\n   189→            \"request_id\": metrics.request_id,\n   190→            \"method\": metrics.method_name,\n   191→            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n   192→            \"user_id\": metrics.user_id,\n   193→            \"request_size_bytes\": metrics.request_size_bytes,\n   194→            \"client_metadata\": dict(context.invocation_metadata()) if context.invocation_metadata() else {}\n   195→        }\n   196→        \n   197→        # Add detailed request data (sanitized)\n   198→        try:\n   199→            if hasattr(request, 'DESCRIPTOR'):\n   200→                # This is a protobuf message\n   201→                request_data[\"request_payload\"] = self._proto_to_dict(request)\n   202→            elif hasattr(request, 'data'):\n   203→                request_data[\"request_data\"] = self._sanitize_request_data(request.data)\n   204→            else:\n   205→                # Fallback to string representation\n   206→                request_data[\"request_str\"] = str(request)\n   207→        except Exception as e:\n   208→            self.logger.warning(f\"Could not serialize request data: {e}\")\n   209→            request_data[\"request_error\"] = \"Could not serialize request\"\n   210→        \n   211→        self.structured_logger.info(\n   212→            \"gRPC request received\",\n   213→            extra=request_data\n   214→        )\n   215→    \n   216→    def _log_response(self, response: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n   217→        \"\"\"Log gRPC response.\"\"\"\n   218→        if not self.log_responses:\n   219→            return\n   220→        \n   221→        response_data = {\n   222→            \"request_id\": metrics.request_id,\n   223→            \"method\": metrics.method_name,\n   224→            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n   225→            \"duration_ms\": metrics.duration_ms,\n   226→            \"status\": \"SUCCESS\",\n   227→            \"status_code\": metrics.status_code.name if metrics.status_code else \"OK\",\n   228→            \"response_size_bytes\": metrics.response_size_bytes,\n   229→            \"user_id\": metrics.user_id\n   230→        }\n   231→        \n   232→        # Add detailed response data (sanitized)\n   233→        try:\n   234→            if hasattr(response, 'DESCRIPTOR'):\n   235→                # This is a protobuf message\n   236→                response_data[\"response_payload\"] = self._proto_to_dict(response, is_response=True)\n   237→            elif hasattr(response, 'code') and hasattr(response, 'message'):\n   238→                response_data[\"response_code\"] = response.code\n   239→                response_data[\"response_message\"] = response.message\n   240→            else:\n   241→                # Fallback to string representation\n   242→                response_data[\"response_str\"] = str(response)\n   243→        except Exception as e:\n   244→            self.logger.warning(f\"Could not serialize response data: {e}\")\n   245→            response_data[\"response_error\"] = \"Could not serialize response\"\n   246→        \n   247→        self.structured_logger.info(\n   248→            \"gRPC response sent\",\n   249→            extra=response_data\n   250→        )\n   251→    \n   252→    def _log_error(self, error: Exception, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n   253→        \"\"\"Log gRPC error.\"\"\"\n   254→        error_data = {\n   255→            \"request_id\": metrics.request_id,\n   256→            \"method\": metrics.method_name,\n   257→            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n   258→            \"duration_ms\": metrics.duration_ms,\n   259→            \"status\": \"ERROR\",\n   260→            \"status_code\": metrics.status_code.name if metrics.status_code else \"INTERNAL\",\n   261→            \"error_type\": type(error).__name__,\n   262→            \"error_message\": str(error),\n   263→            \"user_id\": metrics.user_id\n   264→        }\n   265→        \n   266→        self.structured_logger.error(\n   267→            \"gRPC request failed\",\n   268→            extra=error_data,\n   269→            exc_info=True\n   270→        )\n   271→    \n   272→    def _log_performance_metrics(self, metrics: RequestMetrics) -> None:\n   273→        \"\"\"Log performance metrics.\"\"\"\n   274→        perf_data = {\n   275→            \"request_id\": metrics.request_id,\n   276→            \"method\": metrics.method_name,\n   277→            \"duration_ms\": metrics.duration_ms,\n   278→            \"request_size_bytes\": metrics.request_size_bytes,\n   279→            \"response_size_bytes\": metrics.response_size_bytes,\n   280→            \"timestamp\": datetime.now(timezone.utc).isoformat()\n   281→        }\n   282→        \n   283→        self.metrics_logger.info(\"performance_metric\", extra=perf_data)\n   284→        \n   285→        # Alert on slow requests (> 5 seconds)\n   286→        if metrics.duration_ms and metrics.duration_ms > 5000:\n   287→            self.logger.warning(\n   288→                f\"Slow gRPC request detected: {metrics.method_name} \"\n   289→                f\"took {metrics.duration_ms:.2f}ms\",\n   290→                extra=perf_data\n   291→            )\n   292→    \n   293→    def _log_audit_event(self, request: Any, response: Any, context: grpc.ServicerContext, \n   294→                        metrics: RequestMetrics, auth_context: Any, success: bool, error: Exception = None) -> None:\n   295→        \"\"\"Log audit events for sensitive operations.\"\"\"\n   296→        audit_data = {\n   297→            \"event_type\": \"grpc_operation\",\n   298→            \"request_id\": metrics.request_id,\n   299→            \"method\": metrics.method_name,\n   300→            \"user_id\": metrics.user_id,\n   301→            \"user_email\": auth_context.email if auth_context else None,\n   302→            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n   303→            \"success\": success,\n   304→            \"duration_ms\": metrics.duration_ms,\n   305→            \"client_ip\": self._extract_client_ip(context),\n   306→            \"user_agent\": self._extract_user_agent(context)\n   307→        }\n   308→        \n   309→        if not success and error:\n   310→            audit_data[\"error_type\"] = type(error).__name__\n   311→            audit_data[\"error_message\"] = str(error)\n   312→        \n   313→        # Add operation-specific audit data\n   314→        if hasattr(request, 'data'):\n   315→            audit_data[\"operation_data\"] = self._sanitize_audit_data(request.data, metrics.method_name)\n   316→        \n   317→        self.audit_logger.info(\"audit_event\", extra=audit_data)\n   318→    \n   319→    def _sanitize_request_data(self, request_data: Any) -> Dict[str, Any]:\n   320→        \"\"\"Sanitize request data for logging (remove sensitive fields).\"\"\"\n   321→        if not request_data:\n   322→            return {}\n   323→        \n   324→        sensitive_fields = {'password', 'password_hash', 'token', 'secret', 'credentials', 'access_token'}\n   325→        \n   326→        sanitized = {}\n   327→        for field in request_data.DESCRIPTOR.fields:\n   328→            field_name = field.name\n   329→            if field_name.lower() in sensitive_fields:\n   330→                sanitized[field_name] = \"***REDACTED***\"\n   331→            else:\n   332→                value = getattr(request_data, field_name, None)\n   333→                if value is not None:\n   334→                    sanitized[field_name] = str(value)\n   335→        \n   336→        return sanitized\n   337→    \n   338→    def _sanitize_audit_data(self, request_data: Any, method_name: str) -> Dict[str, Any]:\n   339→        \"\"\"Sanitize data for audit logging.\"\"\"\n   340→        if not request_data:\n   341→            return {}\n   342→        \n   343→        # Method-specific sanitization rules\n   344→        if method_name == '/user.UserService/Login':\n   345→            return {\n   346→                \"email\": getattr(request_data, 'email', None),\n   347→                \"login_attempt\": True\n   348→            }\n   349→        elif method_name == '/user.UserService/RegisterUser':\n   350→            return {\n   351→                \"email\": getattr(request_data, 'email', None),\n   352→                \"username\": getattr(request_data, 'username', None),\n   353→                \"registration_attempt\": True\n   354→            }\n   355→        elif method_name == '/broker_account.BrokerAccountService/UpdateBrokerAccount':\n   356→            return {\n   357→                \"user_id\": getattr(request_data, 'user_id', None),\n   358→                \"broker_id\": getattr(request_data, 'broker_id', None),\n   359→                \"account_number\": getattr(request_data, 'account_number', None),\n   360→                \"update_attempt\": True\n   361→            }\n   362→        \n   363→        return self._sanitize_request_data(request_data)\n   364→    \n   365→    def _calculate_message_size(self, message: Any) -> Optional[int]:\n   366→        \"\"\"Calculate message size in bytes.\"\"\"\n   367→        try:\n   368→            if hasattr(message, 'SerializeToString'):\n   369→                return len(message.SerializeToString())\n   370→            return len(str(message).encode('utf-8'))\n   371→        except Exception:\n   372→            return None\n   373→    \n   374→    def _extract_client_ip(self, context: grpc.ServicerContext) -> Optional[str]:\n   375→        \"\"\"Extract client IP from gRPC context.\"\"\"\n   376→        try:\n   377→            peer = context.peer()\n   378→            if peer:\n   379→                # Format: \"ipv4:127.0.0.1:52050\" or \"ipv6:[::1]:52050\"\n   380→                if peer.startswith('ipv4:'):\n   381→                    return peer.split(':')[1]\n   382→                elif peer.startswith('ipv6:'):\n   383→                    return peer.split('[')[1].split(']')[0]\n   384→            return None\n   385→        except Exception:\n   386→            return None\n   387→    \n   388→    def _extract_user_agent(self, context: grpc.ServicerContext) -> Optional[str]:\n   389→        \"\"\"Extract user agent from gRPC metadata.\"\"\"\n   390→        try:\n   391→            metadata = dict(context.invocation_metadata())\n   392→            return metadata.get('user-agent') or metadata.get('User-Agent')\n   393→        except Exception:\n   394→            return None\n   395→    \n   396→    def get_metrics_summary(self) -> Dict[str, Any]:\n   397→        \"\"\"Get summary of collected metrics.\"\"\"\n   398→        if not self.request_metrics:\n   399→            return {}\n   400→        \n   401→        total_requests = len(self.request_metrics)\n   402→        successful_requests = sum(1 for m in self.request_metrics.values() \n   403→                                if m.status_code == grpc.StatusCode.OK)\n   404→        failed_requests = total_requests - successful_requests\n   405→        \n   406→        durations = [m.duration_ms for m in self.request_metrics.values() if m.duration_ms]\n   407→        avg_duration = sum(durations) / len(durations) if durations else 0\n   408→        \n   409→        return {\n   410→            \"total_requests\": total_requests,\n   411→            \"successful_requests\": successful_requests,\n   412→            \"failed_requests\": failed_requests,\n   413→            \"success_rate\": successful_requests / total_requests if total_requests > 0 else 0,\n   414→            \"average_duration_ms\": avg_duration,\n   415→            \"methods\": list(set(m.method_name for m in self.request_metrics.values()))\n   416→        }"}]},"uuid":"a13bcf57-1d0c-4ff5-94f9-b692e46911a3","timestamp":"2025-08-20T01:57:05.222Z","toolUseResult":{"filePath":"/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/grpc/interceptors/logging_interceptor.py","oldString":"\"\"\"\nAdvanced Logging gRPC Interceptor\n\nThis interceptor provides comprehensive logging, metrics collection,\nand request tracing for gRPC services.\n\"\"\"\n\nimport grpc\nimport logging\nimport time\nimport json\nimport uuid\nfrom typing import Callable, Any, Dict, Optional\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime, timezone\nfrom functools import wraps\n\nfrom src.infrastructure.logging.structured_logger import StructuredLogger\n\n\n@dataclass\nclass RequestMetrics:\n    \"\"\"Request metrics for monitoring and analysis.\"\"\"\n    request_id: str\n    method_name: str\n    start_time: float\n    end_time: Optional[float] = None\n    duration_ms: Optional[float] = None\n    status_code: Optional[grpc.StatusCode] = None\n    request_size_bytes: Optional[int] = None\n    response_size_bytes: Optional[int] = None\n    user_id: Optional[str] = None\n    error_message: Optional[str] = None\n\n\nclass LoggingInterceptor(grpc.ServerInterceptor):\n    \"\"\"\n    Advanced logging interceptor with structured logging and metrics.\n    \n    Features:\n    - Structured logging with correlation IDs\n    - Request/response metrics collection\n    - Performance monitoring\n    - Error tracking and alerting\n    - Audit logging for sensitive operations\n    \"\"\"\n    \n    # Sensitive methods that require audit logging\n    AUDIT_METHODS = {\n        '/user.UserService/Login',\n        '/user.UserService/RegisterUser',\n        '/user.UserService/DeactivateUser',\n        '/broker_account.BrokerAccountService/UpdateBrokerAccount',\n    }\n    \n    # Methods to exclude from detailed logging (health checks, etc.)\n    EXCLUDED_METHODS = {\n        '/grpc.health.v1.Health/Check',\n    }\n    \n    def __init__(self, \n                 service_name: str = \"BEP-OMS-BROKER\",\n                 log_requests: bool = True,\n                 log_responses: bool = True,\n                 log_performance: bool = True):\n        self.service_name = service_name\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_performance = log_performance\n        \n        # Setup loggers\n        self.logger = logging.getLogger(__name__)\n        self.audit_logger = logging.getLogger(f\"{__name__}.audit\")\n        self.metrics_logger = logging.getLogger(f\"{__name__}.metrics\")\n        self.structured_logger = StructuredLogger(service_name)\n        \n        # Metrics storage (in production, use proper metrics backend)\n        self.request_metrics: Dict[str, RequestMetrics] = {}\n    \n    def intercept_service(self, continuation: Callable, handler_call_details: grpc.HandlerCallDetails) -> grpc.RpcMethodHandler:\n        \"\"\"Intercept gRPC service calls for logging.\"\"\"\n        \n        def logging_wrapper(behavior: Callable) -> Callable:\n            def new_behavior(request: Any, context: grpc.ServicerContext) -> Any:\n                method_name = handler_call_details.method\n                \n                # Skip logging for excluded methods\n                if method_name in self.EXCLUDED_METHODS:\n                    return behavior(request, context)\n                \n                # Generate request ID if not set by previous interceptor\n                request_id = getattr(context, 'request_id', str(uuid.uuid4()))\n                context.request_id = request_id\n                \n                # Initialize metrics\n                metrics = RequestMetrics(\n                    request_id=request_id,\n                    method_name=method_name,\n                    start_time=time.time(),\n                    request_size_bytes=self._calculate_message_size(request)\n                )\n                \n                # Get user context if available (set by auth interceptor)\n                auth_context = getattr(context, 'auth_context', None)\n                if auth_context:\n                    metrics.user_id = auth_context.user_id\n                \n                try:\n                    # Log incoming request\n                    self._log_request(request, context, metrics, auth_context)\n                    \n                    # Call the actual service method\n                    start_time = time.time()\n                    response = behavior(request, context)\n                    end_time = time.time()\n                    \n                    # Update metrics\n                    metrics.end_time = end_time\n                    metrics.duration_ms = (end_time - start_time) * 1000\n                    metrics.status_code = grpc.StatusCode.OK\n                    metrics.response_size_bytes = self._calculate_message_size(response)\n                    \n                    # Log successful response\n                    self._log_response(response, context, metrics, auth_context)\n                    \n                    # Log performance metrics\n                    if self.log_performance:\n                        self._log_performance_metrics(metrics)\n                    \n                    # Audit logging for sensitive operations\n                    if method_name in self.AUDIT_METHODS:\n                        self._log_audit_event(request, response, context, metrics, auth_context, success=True)\n                    \n                    return response\n                    \n                except Exception as e:\n                    # Update metrics with error information\n                    metrics.end_time = time.time()\n                    metrics.duration_ms = (metrics.end_time - metrics.start_time) * 1000\n                    metrics.error_message = str(e)\n                    \n                    # Determine gRPC status code\n                    if hasattr(context, '_state') and hasattr(context._state, 'code'):\n                        metrics.status_code = context._state.code\n                    else:\n                        metrics.status_code = grpc.StatusCode.INTERNAL\n                    \n                    # Log error\n                    self._log_error(e, context, metrics, auth_context)\n                    \n                    # Audit logging for failed sensitive operations\n                    if method_name in self.AUDIT_METHODS:\n                        self._log_audit_event(request, None, context, metrics, auth_context, success=False, error=e)\n                    \n                    # Re-raise the exception\n                    raise\n                \n                finally:\n                    # Store metrics for monitoring\n                    self.request_metrics[request_id] = metrics\n                    \n                    # Clean up old metrics (keep last 1000 requests)\n                    if len(self.request_metrics) > 1000:\n                        oldest_keys = list(self.request_metrics.keys())[:-1000]\n                        for key in oldest_keys:\n                            del self.request_metrics[key]\n            \n            return new_behavior\n        \n        # Get the original handler\n        handler = continuation(handler_call_details)\n        \n        if handler is None:\n            return None\n        \n        # Wrap the handler with logging\n        return grpc.unary_unary_rpc_method_handler(\n            logging_wrapper(handler.unary_unary),\n            request_deserializer=handler.request_deserializer,\n            response_serializer=handler.response_serializer,\n        )\n    \n    def _log_request(self, request: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log incoming gRPC request.\"\"\"\n        if not self.log_requests:\n            return\n        \n        request_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"user_id\": metrics.user_id,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"client_metadata\": dict(context.invocation_metadata()) if context.invocation_metadata() else {}\n        }\n        \n        # Add detailed request data (sanitized)\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # This is a protobuf message\n                request_data[\"request_payload\"] = self._proto_to_dict(request)\n            elif hasattr(request, 'data'):\n                request_data[\"request_data\"] = self._sanitize_request_data(request.data)\n            else:\n                # Fallback to string representation\n                request_data[\"request_str\"] = str(request)\n        except Exception as e:\n            self.logger.warning(f\"Could not serialize request data: {e}\")\n            request_data[\"request_error\"] = \"Could not serialize request\"\n        \n        self.structured_logger.info(\n            \"gRPC request received\",\n            extra=request_data\n        )\n    \n    def _log_response(self, response: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC response.\"\"\"\n        if not self.log_responses:\n            return\n        \n        response_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"SUCCESS\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"OK\",\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"user_id\": metrics.user_id\n        }\n        \n        # Add sanitized response data\n        if hasattr(response, 'code') and hasattr(response, 'message'):\n            response_data[\"response_code\"] = response.code\n            response_data[\"response_message\"] = response.message\n        \n        self.structured_logger.info(\n            \"gRPC response sent\",\n            extra=response_data\n        )\n    \n    def _log_error(self, error: Exception, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC error.\"\"\"\n        error_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"ERROR\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"INTERNAL\",\n            \"error_type\": type(error).__name__,\n            \"error_message\": str(error),\n            \"user_id\": metrics.user_id\n        }\n        \n        self.structured_logger.error(\n            \"gRPC request failed\",\n            extra=error_data,\n            exc_info=True\n        )\n    \n    def _log_performance_metrics(self, metrics: RequestMetrics) -> None:\n        \"\"\"Log performance metrics.\"\"\"\n        perf_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"duration_ms\": metrics.duration_ms,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"timestamp\": datetime.now(timezone.utc).isoformat()\n        }\n        \n        self.metrics_logger.info(\"performance_metric\", extra=perf_data)\n        \n        # Alert on slow requests (> 5 seconds)\n        if metrics.duration_ms and metrics.duration_ms > 5000:\n            self.logger.warning(\n                f\"Slow gRPC request detected: {metrics.method_name} \"\n                f\"took {metrics.duration_ms:.2f}ms\",\n                extra=perf_data\n            )\n    \n    def _log_audit_event(self, request: Any, response: Any, context: grpc.ServicerContext, \n                        metrics: RequestMetrics, auth_context: Any, success: bool, error: Exception = None) -> None:\n        \"\"\"Log audit events for sensitive operations.\"\"\"\n        audit_data = {\n            \"event_type\": \"grpc_operation\",\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"user_id\": metrics.user_id,\n            \"user_email\": auth_context.email if auth_context else None,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"success\": success,\n            \"duration_ms\": metrics.duration_ms,\n            \"client_ip\": self._extract_client_ip(context),\n            \"user_agent\": self._extract_user_agent(context)\n        }\n        \n        if not success and error:\n            audit_data[\"error_type\"] = type(error).__name__\n            audit_data[\"error_message\"] = str(error)\n        \n        # Add operation-specific audit data\n        if hasattr(request, 'data'):\n            audit_data[\"operation_data\"] = self._sanitize_audit_data(request.data, metrics.method_name)\n        \n        self.audit_logger.info(\"audit_event\", extra=audit_data)\n    \n    def _sanitize_request_data(self, request_data: Any) -> Dict[str, Any]:\n        \"\"\"Sanitize request data for logging (remove sensitive fields).\"\"\"\n        if not request_data:\n            return {}\n        \n        sensitive_fields = {'password', 'password_hash', 'token', 'secret', 'credentials', 'access_token'}\n        \n        sanitized = {}\n        for field in request_data.DESCRIPTOR.fields:\n            field_name = field.name\n            if field_name.lower() in sensitive_fields:\n                sanitized[field_name] = \"***REDACTED***\"\n            else:\n                value = getattr(request_data, field_name, None)\n                if value is not None:\n                    sanitized[field_name] = str(value)\n        \n        return sanitized\n    \n    def _sanitize_audit_data(self, request_data: Any, method_name: str) -> Dict[str, Any]:\n        \"\"\"Sanitize data for audit logging.\"\"\"\n        if not request_data:\n            return {}\n        \n        # Method-specific sanitization rules\n        if method_name == '/user.UserService/Login':\n            return {\n                \"email\": getattr(request_data, 'email', None),\n                \"login_attempt\": True\n            }\n        elif method_name == '/user.UserService/RegisterUser':\n            return {\n                \"email\": getattr(request_data, 'email', None),\n                \"username\": getattr(request_data, 'username', None),\n                \"registration_attempt\": True\n            }\n        elif method_name == '/broker_account.BrokerAccountService/UpdateBrokerAccount':\n            return {\n                \"user_id\": getattr(request_data, 'user_id', None),\n                \"broker_id\": getattr(request_data, 'broker_id', None),\n                \"account_number\": getattr(request_data, 'account_number', None),\n                \"update_attempt\": True\n            }\n        \n        return self._sanitize_request_data(request_data)\n    \n    def _calculate_message_size(self, message: Any) -> Optional[int]:\n        \"\"\"Calculate message size in bytes.\"\"\"\n        try:\n            if hasattr(message, 'SerializeToString'):\n                return len(message.SerializeToString())\n            return len(str(message).encode('utf-8'))\n        except Exception:\n            return None\n    \n    def _extract_client_ip(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract client IP from gRPC context.\"\"\"\n        try:\n            peer = context.peer()\n            if peer:\n                # Format: \"ipv4:127.0.0.1:52050\" or \"ipv6:[::1]:52050\"\n                if peer.startswith('ipv4:'):\n                    return peer.split(':')[1]\n                elif peer.startswith('ipv6:'):\n                    return peer.split('[')[1].split(']')[0]\n            return None\n        except Exception:\n            return None\n    \n    def _extract_user_agent(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract user agent from gRPC metadata.\"\"\"\n        try:\n            metadata = dict(context.invocation_metadata())\n            return metadata.get('user-agent') or metadata.get('User-Agent')\n        except Exception:\n            return None\n    \n    def get_metrics_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary of collected metrics.\"\"\"\n        if not self.request_metrics:\n            return {}\n        \n        total_requests = len(self.request_metrics)\n        successful_requests = sum(1 for m in self.request_metrics.values() \n                                if m.status_code == grpc.StatusCode.OK)\n        failed_requests = total_requests - successful_requests\n        \n        durations = [m.duration_ms for m in self.request_metrics.values() if m.duration_ms]\n        avg_duration = sum(durations) / len(durations) if durations else 0\n        \n        return {\n            \"total_requests\": total_requests,\n            \"successful_requests\": successful_requests,\n            \"failed_requests\": failed_requests,\n            \"success_rate\": successful_requests / total_requests if total_requests > 0 else 0,\n            \"average_duration_ms\": avg_duration,\n            \"methods\": list(set(m.method_name for m in self.request_metrics.values()))\n        }","newString":"\"\"\"\nAdvanced Logging gRPC Interceptor\n\nThis interceptor provides comprehensive logging, metrics collection,\nand request tracing for gRPC services.\n\"\"\"\n\nimport grpc\nimport logging\nimport time\nimport json\nimport uuid\nfrom typing import Callable, Any, Dict, Optional\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime, timezone\nfrom functools import wraps\n\nfrom src.infrastructure.logging.structured_logger import StructuredLogger\n\n\n@dataclass\nclass RequestMetrics:\n    \"\"\"Request metrics for monitoring and analysis.\"\"\"\n    request_id: str\n    method_name: str\n    start_time: float\n    end_time: Optional[float] = None\n    duration_ms: Optional[float] = None\n    status_code: Optional[grpc.StatusCode] = None\n    request_size_bytes: Optional[int] = None\n    response_size_bytes: Optional[int] = None\n    user_id: Optional[str] = None\n    error_message: Optional[str] = None\n\n\nclass LoggingInterceptor(grpc.ServerInterceptor):\n    \"\"\"\n    Advanced logging interceptor with structured logging and metrics.\n    \n    Features:\n    - Structured logging with correlation IDs\n    - Request/response metrics collection\n    - Performance monitoring\n    - Error tracking and alerting\n    - Audit logging for sensitive operations\n    \"\"\"\n    \n    # Sensitive methods that require audit logging\n    AUDIT_METHODS = {\n        '/user.UserService/Login',\n        '/user.UserService/RegisterUser',\n        '/user.UserService/DeactivateUser',\n        '/broker_account.BrokerAccountService/UpdateBrokerAccount',\n    }\n    \n    # Methods to exclude from detailed logging (health checks, etc.)\n    EXCLUDED_METHODS = {\n        '/grpc.health.v1.Health/Check',\n    }\n    \n    def __init__(self, \n                 service_name: str = \"BEP-OMS-BROKER\",\n                 log_requests: bool = True,\n                 log_responses: bool = True,\n                 log_performance: bool = True):\n        self.service_name = service_name\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_performance = log_performance\n        \n        # Setup loggers\n        self.logger = logging.getLogger(__name__)\n        self.audit_logger = logging.getLogger(f\"{__name__}.audit\")\n        self.metrics_logger = logging.getLogger(f\"{__name__}.metrics\")\n        self.structured_logger = StructuredLogger(service_name)\n        \n        # Metrics storage (in production, use proper metrics backend)\n        self.request_metrics: Dict[str, RequestMetrics] = {}\n    \n    def intercept_service(self, continuation: Callable, handler_call_details: grpc.HandlerCallDetails) -> grpc.RpcMethodHandler:\n        \"\"\"Intercept gRPC service calls for logging.\"\"\"\n        \n        def logging_wrapper(behavior: Callable) -> Callable:\n            def new_behavior(request: Any, context: grpc.ServicerContext) -> Any:\n                method_name = handler_call_details.method\n                \n                # Skip logging for excluded methods\n                if method_name in self.EXCLUDED_METHODS:\n                    return behavior(request, context)\n                \n                # Generate request ID if not set by previous interceptor\n                request_id = getattr(context, 'request_id', str(uuid.uuid4()))\n                context.request_id = request_id\n                \n                # Initialize metrics\n                metrics = RequestMetrics(\n                    request_id=request_id,\n                    method_name=method_name,\n                    start_time=time.time(),\n                    request_size_bytes=self._calculate_message_size(request)\n                )\n                \n                # Get user context if available (set by auth interceptor)\n                auth_context = getattr(context, 'auth_context', None)\n                if auth_context:\n                    metrics.user_id = auth_context.user_id\n                \n                try:\n                    # Log incoming request\n                    self._log_request(request, context, metrics, auth_context)\n                    \n                    # Call the actual service method\n                    start_time = time.time()\n                    response = behavior(request, context)\n                    end_time = time.time()\n                    \n                    # Update metrics\n                    metrics.end_time = end_time\n                    metrics.duration_ms = (end_time - start_time) * 1000\n                    metrics.status_code = grpc.StatusCode.OK\n                    metrics.response_size_bytes = self._calculate_message_size(response)\n                    \n                    # Log successful response\n                    self._log_response(response, context, metrics, auth_context)\n                    \n                    # Log performance metrics\n                    if self.log_performance:\n                        self._log_performance_metrics(metrics)\n                    \n                    # Audit logging for sensitive operations\n                    if method_name in self.AUDIT_METHODS:\n                        self._log_audit_event(request, response, context, metrics, auth_context, success=True)\n                    \n                    return response\n                    \n                except Exception as e:\n                    # Update metrics with error information\n                    metrics.end_time = time.time()\n                    metrics.duration_ms = (metrics.end_time - metrics.start_time) * 1000\n                    metrics.error_message = str(e)\n                    \n                    # Determine gRPC status code\n                    if hasattr(context, '_state') and hasattr(context._state, 'code'):\n                        metrics.status_code = context._state.code\n                    else:\n                        metrics.status_code = grpc.StatusCode.INTERNAL\n                    \n                    # Log error\n                    self._log_error(e, context, metrics, auth_context)\n                    \n                    # Audit logging for failed sensitive operations\n                    if method_name in self.AUDIT_METHODS:\n                        self._log_audit_event(request, None, context, metrics, auth_context, success=False, error=e)\n                    \n                    # Re-raise the exception\n                    raise\n                \n                finally:\n                    # Store metrics for monitoring\n                    self.request_metrics[request_id] = metrics\n                    \n                    # Clean up old metrics (keep last 1000 requests)\n                    if len(self.request_metrics) > 1000:\n                        oldest_keys = list(self.request_metrics.keys())[:-1000]\n                        for key in oldest_keys:\n                            del self.request_metrics[key]\n            \n            return new_behavior\n        \n        # Get the original handler\n        handler = continuation(handler_call_details)\n        \n        if handler is None:\n            return None\n        \n        # Wrap the handler with logging\n        return grpc.unary_unary_rpc_method_handler(\n            logging_wrapper(handler.unary_unary),\n            request_deserializer=handler.request_deserializer,\n            response_serializer=handler.response_serializer,\n        )\n    \n    def _log_request(self, request: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log incoming gRPC request.\"\"\"\n        if not self.log_requests:\n            return\n        \n        request_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"user_id\": metrics.user_id,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"client_metadata\": dict(context.invocation_metadata()) if context.invocation_metadata() else {}\n        }\n        \n        # Add detailed request data (sanitized)\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # This is a protobuf message\n                request_data[\"request_payload\"] = self._proto_to_dict(request)\n            elif hasattr(request, 'data'):\n                request_data[\"request_data\"] = self._sanitize_request_data(request.data)\n            else:\n                # Fallback to string representation\n                request_data[\"request_str\"] = str(request)\n        except Exception as e:\n            self.logger.warning(f\"Could not serialize request data: {e}\")\n            request_data[\"request_error\"] = \"Could not serialize request\"\n        \n        self.structured_logger.info(\n            \"gRPC request received\",\n            extra=request_data\n        )\n    \n    def _log_response(self, response: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC response.\"\"\"\n        if not self.log_responses:\n            return\n        \n        response_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"SUCCESS\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"OK\",\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"user_id\": metrics.user_id\n        }\n        \n        # Add detailed response data (sanitized)\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # This is a protobuf message\n                response_data[\"response_payload\"] = self._proto_to_dict(response, is_response=True)\n            elif hasattr(response, 'code') and hasattr(response, 'message'):\n                response_data[\"response_code\"] = response.code\n                response_data[\"response_message\"] = response.message\n            else:\n                # Fallback to string representation\n                response_data[\"response_str\"] = str(response)\n        except Exception as e:\n            self.logger.warning(f\"Could not serialize response data: {e}\")\n            response_data[\"response_error\"] = \"Could not serialize response\"\n        \n        self.structured_logger.info(\n            \"gRPC response sent\",\n            extra=response_data\n        )\n    \n    def _log_error(self, error: Exception, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC error.\"\"\"\n        error_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"ERROR\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"INTERNAL\",\n            \"error_type\": type(error).__name__,\n            \"error_message\": str(error),\n            \"user_id\": metrics.user_id\n        }\n        \n        self.structured_logger.error(\n            \"gRPC request failed\",\n            extra=error_data,\n            exc_info=True\n        )\n    \n    def _log_performance_metrics(self, metrics: RequestMetrics) -> None:\n        \"\"\"Log performance metrics.\"\"\"\n        perf_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"duration_ms\": metrics.duration_ms,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"timestamp\": datetime.now(timezone.utc).isoformat()\n        }\n        \n        self.metrics_logger.info(\"performance_metric\", extra=perf_data)\n        \n        # Alert on slow requests (> 5 seconds)\n        if metrics.duration_ms and metrics.duration_ms > 5000:\n            self.logger.warning(\n                f\"Slow gRPC request detected: {metrics.method_name} \"\n                f\"took {metrics.duration_ms:.2f}ms\",\n                extra=perf_data\n            )\n    \n    def _log_audit_event(self, request: Any, response: Any, context: grpc.ServicerContext, \n                        metrics: RequestMetrics, auth_context: Any, success: bool, error: Exception = None) -> None:\n        \"\"\"Log audit events for sensitive operations.\"\"\"\n        audit_data = {\n            \"event_type\": \"grpc_operation\",\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"user_id\": metrics.user_id,\n            \"user_email\": auth_context.email if auth_context else None,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"success\": success,\n            \"duration_ms\": metrics.duration_ms,\n            \"client_ip\": self._extract_client_ip(context),\n            \"user_agent\": self._extract_user_agent(context)\n        }\n        \n        if not success and error:\n            audit_data[\"error_type\"] = type(error).__name__\n            audit_data[\"error_message\"] = str(error)\n        \n        # Add operation-specific audit data\n        if hasattr(request, 'data'):\n            audit_data[\"operation_data\"] = self._sanitize_audit_data(request.data, metrics.method_name)\n        \n        self.audit_logger.info(\"audit_event\", extra=audit_data)\n    \n    def _sanitize_request_data(self, request_data: Any) -> Dict[str, Any]:\n        \"\"\"Sanitize request data for logging (remove sensitive fields).\"\"\"\n        if not request_data:\n            return {}\n        \n        sensitive_fields = {'password', 'password_hash', 'token', 'secret', 'credentials', 'access_token'}\n        \n        sanitized = {}\n        for field in request_data.DESCRIPTOR.fields:\n            field_name = field.name\n            if field_name.lower() in sensitive_fields:\n                sanitized[field_name] = \"***REDACTED***\"\n            else:\n                value = getattr(request_data, field_name, None)\n                if value is not None:\n                    sanitized[field_name] = str(value)\n        \n        return sanitized\n    \n    def _sanitize_audit_data(self, request_data: Any, method_name: str) -> Dict[str, Any]:\n        \"\"\"Sanitize data for audit logging.\"\"\"\n        if not request_data:\n            return {}\n        \n        # Method-specific sanitization rules\n        if method_name == '/user.UserService/Login':\n            return {\n                \"email\": getattr(request_data, 'email', None),\n                \"login_attempt\": True\n            }\n        elif method_name == '/user.UserService/RegisterUser':\n            return {\n                \"email\": getattr(request_data, 'email', None),\n                \"username\": getattr(request_data, 'username', None),\n                \"registration_attempt\": True\n            }\n        elif method_name == '/broker_account.BrokerAccountService/UpdateBrokerAccount':\n            return {\n                \"user_id\": getattr(request_data, 'user_id', None),\n                \"broker_id\": getattr(request_data, 'broker_id', None),\n                \"account_number\": getattr(request_data, 'account_number', None),\n                \"update_attempt\": True\n            }\n        \n        return self._sanitize_request_data(request_data)\n    \n    def _calculate_message_size(self, message: Any) -> Optional[int]:\n        \"\"\"Calculate message size in bytes.\"\"\"\n        try:\n            if hasattr(message, 'SerializeToString'):\n                return len(message.SerializeToString())\n            return len(str(message).encode('utf-8'))\n        except Exception:\n            return None\n    \n    def _extract_client_ip(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract client IP from gRPC context.\"\"\"\n        try:\n            peer = context.peer()\n            if peer:\n                # Format: \"ipv4:127.0.0.1:52050\" or \"ipv6:[::1]:52050\"\n                if peer.startswith('ipv4:'):\n                    return peer.split(':')[1]\n                elif peer.startswith('ipv6:'):\n                    return peer.split('[')[1].split(']')[0]\n            return None\n        except Exception:\n            return None\n    \n    def _extract_user_agent(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract user agent from gRPC metadata.\"\"\"\n        try:\n            metadata = dict(context.invocation_metadata())\n            return metadata.get('user-agent') or metadata.get('User-Agent')\n        except Exception:\n            return None\n    \n    def get_metrics_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary of collected metrics.\"\"\"\n        if not self.request_metrics:\n            return {}\n        \n        total_requests = len(self.request_metrics)\n        successful_requests = sum(1 for m in self.request_metrics.values() \n                                if m.status_code == grpc.StatusCode.OK)\n        failed_requests = total_requests - successful_requests\n        \n        durations = [m.duration_ms for m in self.request_metrics.values() if m.duration_ms]\n        avg_duration = sum(durations) / len(durations) if durations else 0\n        \n        return {\n            \"total_requests\": total_requests,\n            \"successful_requests\": successful_requests,\n            \"failed_requests\": failed_requests,\n            \"success_rate\": successful_requests / total_requests if total_requests > 0 else 0,\n            \"average_duration_ms\": avg_duration,\n            \"methods\": list(set(m.method_name for m in self.request_metrics.values()))\n        }","originalFile":"\"\"\"\nAdvanced Logging gRPC Interceptor\n\nThis interceptor provides comprehensive logging, metrics collection,\nand request tracing for gRPC services.\n\"\"\"\n\nimport grpc\nimport logging\nimport time\nimport json\nimport uuid\nfrom typing import Callable, Any, Dict, Optional\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime, timezone\nfrom functools import wraps\n\nfrom src.infrastructure.logging.structured_logger import StructuredLogger\n\n\n@dataclass\nclass RequestMetrics:\n    \"\"\"Request metrics for monitoring and analysis.\"\"\"\n    request_id: str\n    method_name: str\n    start_time: float\n    end_time: Optional[float] = None\n    duration_ms: Optional[float] = None\n    status_code: Optional[grpc.StatusCode] = None\n    request_size_bytes: Optional[int] = None\n    response_size_bytes: Optional[int] = None\n    user_id: Optional[str] = None\n    error_message: Optional[str] = None\n\n\nclass LoggingInterceptor(grpc.ServerInterceptor):\n    \"\"\"\n    Advanced logging interceptor with structured logging and metrics.\n    \n    Features:\n    - Structured logging with correlation IDs\n    - Request/response metrics collection\n    - Performance monitoring\n    - Error tracking and alerting\n    - Audit logging for sensitive operations\n    \"\"\"\n    \n    # Sensitive methods that require audit logging\n    AUDIT_METHODS = {\n        '/user.UserService/Login',\n        '/user.UserService/RegisterUser',\n        '/user.UserService/DeactivateUser',\n        '/broker_account.BrokerAccountService/UpdateBrokerAccount',\n    }\n    \n    # Methods to exclude from detailed logging (health checks, etc.)\n    EXCLUDED_METHODS = {\n        '/grpc.health.v1.Health/Check',\n    }\n    \n    def __init__(self, \n                 service_name: str = \"BEP-OMS-BROKER\",\n                 log_requests: bool = True,\n                 log_responses: bool = True,\n                 log_performance: bool = True):\n        self.service_name = service_name\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_performance = log_performance\n        \n        # Setup loggers\n        self.logger = logging.getLogger(__name__)\n        self.audit_logger = logging.getLogger(f\"{__name__}.audit\")\n        self.metrics_logger = logging.getLogger(f\"{__name__}.metrics\")\n        self.structured_logger = StructuredLogger(service_name)\n        \n        # Metrics storage (in production, use proper metrics backend)\n        self.request_metrics: Dict[str, RequestMetrics] = {}\n    \n    def intercept_service(self, continuation: Callable, handler_call_details: grpc.HandlerCallDetails) -> grpc.RpcMethodHandler:\n        \"\"\"Intercept gRPC service calls for logging.\"\"\"\n        \n        def logging_wrapper(behavior: Callable) -> Callable:\n            def new_behavior(request: Any, context: grpc.ServicerContext) -> Any:\n                method_name = handler_call_details.method\n                \n                # Skip logging for excluded methods\n                if method_name in self.EXCLUDED_METHODS:\n                    return behavior(request, context)\n                \n                # Generate request ID if not set by previous interceptor\n                request_id = getattr(context, 'request_id', str(uuid.uuid4()))\n                context.request_id = request_id\n                \n                # Initialize metrics\n                metrics = RequestMetrics(\n                    request_id=request_id,\n                    method_name=method_name,\n                    start_time=time.time(),\n                    request_size_bytes=self._calculate_message_size(request)\n                )\n                \n                # Get user context if available (set by auth interceptor)\n                auth_context = getattr(context, 'auth_context', None)\n                if auth_context:\n                    metrics.user_id = auth_context.user_id\n                \n                try:\n                    # Log incoming request\n                    self._log_request(request, context, metrics, auth_context)\n                    \n                    # Call the actual service method\n                    start_time = time.time()\n                    response = behavior(request, context)\n                    end_time = time.time()\n                    \n                    # Update metrics\n                    metrics.end_time = end_time\n                    metrics.duration_ms = (end_time - start_time) * 1000\n                    metrics.status_code = grpc.StatusCode.OK\n                    metrics.response_size_bytes = self._calculate_message_size(response)\n                    \n                    # Log successful response\n                    self._log_response(response, context, metrics, auth_context)\n                    \n                    # Log performance metrics\n                    if self.log_performance:\n                        self._log_performance_metrics(metrics)\n                    \n                    # Audit logging for sensitive operations\n                    if method_name in self.AUDIT_METHODS:\n                        self._log_audit_event(request, response, context, metrics, auth_context, success=True)\n                    \n                    return response\n                    \n                except Exception as e:\n                    # Update metrics with error information\n                    metrics.end_time = time.time()\n                    metrics.duration_ms = (metrics.end_time - metrics.start_time) * 1000\n                    metrics.error_message = str(e)\n                    \n                    # Determine gRPC status code\n                    if hasattr(context, '_state') and hasattr(context._state, 'code'):\n                        metrics.status_code = context._state.code\n                    else:\n                        metrics.status_code = grpc.StatusCode.INTERNAL\n                    \n                    # Log error\n                    self._log_error(e, context, metrics, auth_context)\n                    \n                    # Audit logging for failed sensitive operations\n                    if method_name in self.AUDIT_METHODS:\n                        self._log_audit_event(request, None, context, metrics, auth_context, success=False, error=e)\n                    \n                    # Re-raise the exception\n                    raise\n                \n                finally:\n                    # Store metrics for monitoring\n                    self.request_metrics[request_id] = metrics\n                    \n                    # Clean up old metrics (keep last 1000 requests)\n                    if len(self.request_metrics) > 1000:\n                        oldest_keys = list(self.request_metrics.keys())[:-1000]\n                        for key in oldest_keys:\n                            del self.request_metrics[key]\n            \n            return new_behavior\n        \n        # Get the original handler\n        handler = continuation(handler_call_details)\n        \n        if handler is None:\n            return None\n        \n        # Wrap the handler with logging\n        return grpc.unary_unary_rpc_method_handler(\n            logging_wrapper(handler.unary_unary),\n            request_deserializer=handler.request_deserializer,\n            response_serializer=handler.response_serializer,\n        )\n    \n    def _log_request(self, request: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log incoming gRPC request.\"\"\"\n        if not self.log_requests:\n            return\n        \n        request_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"user_id\": metrics.user_id,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"client_metadata\": dict(context.invocation_metadata()) if context.invocation_metadata() else {}\n        }\n        \n        # Add detailed request data (sanitized)\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # This is a protobuf message\n                request_data[\"request_payload\"] = self._proto_to_dict(request)\n            elif hasattr(request, 'data'):\n                request_data[\"request_data\"] = self._sanitize_request_data(request.data)\n            else:\n                # Fallback to string representation\n                request_data[\"request_str\"] = str(request)\n        except Exception as e:\n            self.logger.warning(f\"Could not serialize request data: {e}\")\n            request_data[\"request_error\"] = \"Could not serialize request\"\n        \n        self.structured_logger.info(\n            \"gRPC request received\",\n            extra=request_data\n        )\n    \n    def _log_response(self, response: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC response.\"\"\"\n        if not self.log_responses:\n            return\n        \n        response_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"SUCCESS\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"OK\",\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"user_id\": metrics.user_id\n        }\n        \n        # Add sanitized response data\n        if hasattr(response, 'code') and hasattr(response, 'message'):\n            response_data[\"response_code\"] = response.code\n            response_data[\"response_message\"] = response.message\n        \n        self.structured_logger.info(\n            \"gRPC response sent\",\n            extra=response_data\n        )\n    \n    def _log_error(self, error: Exception, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC error.\"\"\"\n        error_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"ERROR\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"INTERNAL\",\n            \"error_type\": type(error).__name__,\n            \"error_message\": str(error),\n            \"user_id\": metrics.user_id\n        }\n        \n        self.structured_logger.error(\n            \"gRPC request failed\",\n            extra=error_data,\n            exc_info=True\n        )\n    \n    def _log_performance_metrics(self, metrics: RequestMetrics) -> None:\n        \"\"\"Log performance metrics.\"\"\"\n        perf_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"duration_ms\": metrics.duration_ms,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"timestamp\": datetime.now(timezone.utc).isoformat()\n        }\n        \n        self.metrics_logger.info(\"performance_metric\", extra=perf_data)\n        \n        # Alert on slow requests (> 5 seconds)\n        if metrics.duration_ms and metrics.duration_ms > 5000:\n            self.logger.warning(\n                f\"Slow gRPC request detected: {metrics.method_name} \"\n                f\"took {metrics.duration_ms:.2f}ms\",\n                extra=perf_data\n            )\n    \n    def _log_audit_event(self, request: Any, response: Any, context: grpc.ServicerContext, \n                        metrics: RequestMetrics, auth_context: Any, success: bool, error: Exception = None) -> None:\n        \"\"\"Log audit events for sensitive operations.\"\"\"\n        audit_data = {\n            \"event_type\": \"grpc_operation\",\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"user_id\": metrics.user_id,\n            \"user_email\": auth_context.email if auth_context else None,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"success\": success,\n            \"duration_ms\": metrics.duration_ms,\n            \"client_ip\": self._extract_client_ip(context),\n            \"user_agent\": self._extract_user_agent(context)\n        }\n        \n        if not success and error:\n            audit_data[\"error_type\"] = type(error).__name__\n            audit_data[\"error_message\"] = str(error)\n        \n        # Add operation-specific audit data\n        if hasattr(request, 'data'):\n            audit_data[\"operation_data\"] = self._sanitize_audit_data(request.data, metrics.method_name)\n        \n        self.audit_logger.info(\"audit_event\", extra=audit_data)\n    \n    def _sanitize_request_data(self, request_data: Any) -> Dict[str, Any]:\n        \"\"\"Sanitize request data for logging (remove sensitive fields).\"\"\"\n        if not request_data:\n            return {}\n        \n        sensitive_fields = {'password', 'password_hash', 'token', 'secret', 'credentials', 'access_token'}\n        \n        sanitized = {}\n        for field in request_data.DESCRIPTOR.fields:\n            field_name = field.name\n            if field_name.lower() in sensitive_fields:\n                sanitized[field_name] = \"***REDACTED***\"\n            else:\n                value = getattr(request_data, field_name, None)\n                if value is not None:\n                    sanitized[field_name] = str(value)\n        \n        return sanitized\n    \n    def _sanitize_audit_data(self, request_data: Any, method_name: str) -> Dict[str, Any]:\n        \"\"\"Sanitize data for audit logging.\"\"\"\n        if not request_data:\n            return {}\n        \n        # Method-specific sanitization rules\n        if method_name == '/user.UserService/Login':\n            return {\n                \"email\": getattr(request_data, 'email', None),\n                \"login_attempt\": True\n            }\n        elif method_name == '/user.UserService/RegisterUser':\n            return {\n                \"email\": getattr(request_data, 'email', None),\n                \"username\": getattr(request_data, 'username', None),\n                \"registration_attempt\": True\n            }\n        elif method_name == '/broker_account.BrokerAccountService/UpdateBrokerAccount':\n            return {\n                \"user_id\": getattr(request_data, 'user_id', None),\n                \"broker_id\": getattr(request_data, 'broker_id', None),\n                \"account_number\": getattr(request_data, 'account_number', None),\n                \"update_attempt\": True\n            }\n        \n        return self._sanitize_request_data(request_data)\n    \n    def _calculate_message_size(self, message: Any) -> Optional[int]:\n        \"\"\"Calculate message size in bytes.\"\"\"\n        try:\n            if hasattr(message, 'SerializeToString'):\n                return len(message.SerializeToString())\n            return len(str(message).encode('utf-8'))\n        except Exception:\n            return None\n    \n    def _extract_client_ip(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract client IP from gRPC context.\"\"\"\n        try:\n            peer = context.peer()\n            if peer:\n                # Format: \"ipv4:127.0.0.1:52050\" or \"ipv6:[::1]:52050\"\n                if peer.startswith('ipv4:'):\n                    return peer.split(':')[1]\n                elif peer.startswith('ipv6:'):\n                    return peer.split('[')[1].split(']')[0]\n            return None\n        except Exception:\n            return None\n    \n    def _extract_user_agent(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract user agent from gRPC metadata.\"\"\"\n        try:\n            metadata = dict(context.invocation_metadata())\n            return metadata.get('user-agent') or metadata.get('User-Agent')\n        except Exception:\n            return None\n    \n    def get_metrics_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary of collected metrics.\"\"\"\n        if not self.request_metrics:\n            return {}\n        \n        total_requests = len(self.request_metrics)\n        successful_requests = sum(1 for m in self.request_metrics.values() \n                                if m.status_code == grpc.StatusCode.OK)\n        failed_requests = total_requests - successful_requests\n        \n        durations = [m.duration_ms for m in self.request_metrics.values() if m.duration_ms]\n        avg_duration = sum(durations) / len(durations) if durations else 0\n        \n        return {\n            \"total_requests\": total_requests,\n            \"successful_requests\": successful_requests,\n            \"failed_requests\": failed_requests,\n            \"success_rate\": successful_requests / total_requests if total_requests > 0 else 0,\n            \"average_duration_ms\": avg_duration,\n            \"methods\": list(set(m.method_name for m in self.request_metrics.values()))\n        }","structuredPatch":[{"oldStart":229,"oldLines":10,"newStart":229,"newLines":20,"lines":["             \"user_id\": metrics.user_id","         }","         ","-        # Add sanitized response data","-        if hasattr(response, 'code') and hasattr(response, 'message'):","-            response_data[\"response_code\"] = response.code","-            response_data[\"response_message\"] = response.message","+        # Add detailed response data (sanitized)","+        try:","+            if hasattr(response, 'DESCRIPTOR'):","+                # This is a protobuf message","+                response_data[\"response_payload\"] = self._proto_to_dict(response, is_response=True)","+            elif hasattr(response, 'code') and hasattr(response, 'message'):","+                response_data[\"response_code\"] = response.code","+                response_data[\"response_message\"] = response.message","+            else:","+                # Fallback to string representation","+                response_data[\"response_str\"] = str(response)","+        except Exception as e:","+            self.logger.warning(f\"Could not serialize response data: {e}\")","+            response_data[\"response_error\"] = \"Could not serialize response\"","         ","         self.structured_logger.info(","             \"gRPC response sent\","]}],"userModified":false,"replaceAll":false}}
{"parentUuid":"a13bcf57-1d0c-4ff5-94f9-b692e46911a3","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01W9nHUDJyLvuhGzFBRQdeEr","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Bây giờ tôi cần thêm method `_proto_to_dict` để convert protobuf messages thành dictionary:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":6621,"cache_read_input_tokens":39678,"cache_creation":{"ephemeral_5m_input_tokens":6621,"ephemeral_1h_input_tokens":0},"output_tokens":7,"service_tier":"standard"}},"requestId":"req_011CSJ8bPNzMQYSLpzQxE9hM","type":"assistant","uuid":"5d83ea9f-8e13-4320-9b6e-45eb9c7797af","timestamp":"2025-08-20T01:57:09.994Z"}
{"parentUuid":"5d83ea9f-8e13-4320-9b6e-45eb9c7797af","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01W9nHUDJyLvuhGzFBRQdeEr","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_0174krx7qnagsq5vE1RUyANJ","name":"Edit","input":{"file_path":"/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/grpc/interceptors/logging_interceptor.py","old_string":"    def _extract_user_agent(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract user agent from gRPC metadata.\"\"\"\n        try:\n            metadata = dict(context.invocation_metadata())\n            return metadata.get('user-agent') or metadata.get('User-Agent')\n        except Exception:\n            return None","new_string":"    def _extract_user_agent(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract user agent from gRPC metadata.\"\"\"\n        try:\n            metadata = dict(context.invocation_metadata())\n            return metadata.get('user-agent') or metadata.get('User-Agent')\n        except Exception:\n            return None\n    \n    def _proto_to_dict(self, proto_message: Any, is_response: bool = False) -> Dict[str, Any]:\n        \"\"\"Convert protobuf message to dictionary with sanitization.\"\"\"\n        if not proto_message or not hasattr(proto_message, 'DESCRIPTOR'):\n            return {}\n        \n        result = {}\n        sensitive_fields = {\n            'password', 'password_hash', 'token', 'secret', 'credentials', \n            'access_token', 'refresh_token', 'api_key', 'private_key'\n        }\n        \n        try:\n            for field in proto_message.DESCRIPTOR.fields:\n                field_name = field.name\n                \n                try:\n                    value = getattr(proto_message, field_name, None)\n                    \n                    # Skip None/empty values\n                    if value is None:\n                        continue\n                    \n                    # Handle sensitive fields\n                    if field_name.lower() in sensitive_fields:\n                        result[field_name] = \"***REDACTED***\"\n                        continue\n                    \n                    # Handle repeated fields (lists)\n                    if field.label == field.LABEL_REPEATED:\n                        if not value:\n                            continue\n                        result[field_name] = []\n                        for item in value:\n                            if hasattr(item, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name].append(self._proto_to_dict(item, is_response))\n                            else:\n                                result[field_name].append(self._serialize_value(item))\n                    \n                    # Handle nested protobuf messages\n                    elif hasattr(value, 'DESCRIPTOR'):\n                        result[field_name] = self._proto_to_dict(value, is_response)\n                    \n                    # Handle enum values\n                    elif hasattr(value, 'name'):\n                        result[field_name] = value.name\n                    \n                    # Handle primitive values\n                    else:\n                        result[field_name] = self._serialize_value(value)\n                        \n                except Exception as field_error:\n                    self.logger.warning(f\"Error processing field {field_name}: {field_error}\")\n                    result[field_name] = f\"<ERROR: {str(field_error)}>\"\n            \n            return result\n            \n        except Exception as e:\n            self.logger.error(f\"Error converting protobuf to dict: {e}\")\n            return {\"error\": f\"Could not serialize protobuf: {str(e)}\"}\n    \n    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a primitive value for logging.\"\"\"\n        try:\n            # Handle common Python types\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                # Convert bytes to base64 for logging\n                import base64\n                return f\"<bytes:{base64.b64encode(value).decode()[:100]}...>\"\n            elif hasattr(value, 'isoformat'):\n                # Handle datetime objects\n                return value.isoformat()\n            else:\n                # Fallback to string representation\n                str_value = str(value)\n                # Truncate very long values\n                if len(str_value) > 1000:\n                    return str_value[:1000] + \"...\"\n                return str_value\n        except Exception:\n            return \"<non-serializable>\""}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":6621,"cache_read_input_tokens":39678,"cache_creation":{"ephemeral_5m_input_tokens":6621,"ephemeral_1h_input_tokens":0},"output_tokens":1171,"service_tier":"standard"}},"requestId":"req_011CSJ8bPNzMQYSLpzQxE9hM","type":"assistant","uuid":"5e587c0b-877d-436e-b0e7-2423874e0583","timestamp":"2025-08-20T01:57:23.860Z"}
{"parentUuid":"5e587c0b-877d-436e-b0e7-2423874e0583","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_0174krx7qnagsq5vE1RUyANJ","type":"tool_result","content":"The file /Users/lee/Documents/src/bep-oms-broker/src/infrastructure/grpc/interceptors/logging_interceptor.py has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n     1→\"\"\"\n     2→Advanced Logging gRPC Interceptor\n     3→\n     4→This interceptor provides comprehensive logging, metrics collection,\n     5→and request tracing for gRPC services.\n     6→\"\"\"\n     7→\n     8→import grpc\n     9→import logging\n    10→import time\n    11→import json\n    12→import uuid\n    13→from typing import Callable, Any, Dict, Optional\n    14→from dataclasses import dataclass, asdict\n    15→from datetime import datetime, timezone\n    16→from functools import wraps\n    17→\n    18→from src.infrastructure.logging.structured_logger import StructuredLogger\n    19→\n    20→\n    21→@dataclass\n    22→class RequestMetrics:\n    23→    \"\"\"Request metrics for monitoring and analysis.\"\"\"\n    24→    request_id: str\n    25→    method_name: str\n    26→    start_time: float\n    27→    end_time: Optional[float] = None\n    28→    duration_ms: Optional[float] = None\n    29→    status_code: Optional[grpc.StatusCode] = None\n    30→    request_size_bytes: Optional[int] = None\n    31→    response_size_bytes: Optional[int] = None\n    32→    user_id: Optional[str] = None\n    33→    error_message: Optional[str] = None\n    34→\n    35→\n    36→class LoggingInterceptor(grpc.ServerInterceptor):\n    37→    \"\"\"\n    38→    Advanced logging interceptor with structured logging and metrics.\n    39→    \n    40→    Features:\n    41→    - Structured logging with correlation IDs\n    42→    - Request/response metrics collection\n    43→    - Performance monitoring\n    44→    - Error tracking and alerting\n    45→    - Audit logging for sensitive operations\n    46→    \"\"\"\n    47→    \n    48→    # Sensitive methods that require audit logging\n    49→    AUDIT_METHODS = {\n    50→        '/user.UserService/Login',\n    51→        '/user.UserService/RegisterUser',\n    52→        '/user.UserService/DeactivateUser',\n    53→        '/broker_account.BrokerAccountService/UpdateBrokerAccount',\n    54→    }\n    55→    \n    56→    # Methods to exclude from detailed logging (health checks, etc.)\n    57→    EXCLUDED_METHODS = {\n    58→        '/grpc.health.v1.Health/Check',\n    59→    }\n    60→    \n    61→    def __init__(self, \n    62→                 service_name: str = \"BEP-OMS-BROKER\",\n    63→                 log_requests: bool = True,\n    64→                 log_responses: bool = True,\n    65→                 log_performance: bool = True):\n    66→        self.service_name = service_name\n    67→        self.log_requests = log_requests\n    68→        self.log_responses = log_responses\n    69→        self.log_performance = log_performance\n    70→        \n    71→        # Setup loggers\n    72→        self.logger = logging.getLogger(__name__)\n    73→        self.audit_logger = logging.getLogger(f\"{__name__}.audit\")\n    74→        self.metrics_logger = logging.getLogger(f\"{__name__}.metrics\")\n    75→        self.structured_logger = StructuredLogger(service_name)\n    76→        \n    77→        # Metrics storage (in production, use proper metrics backend)\n    78→        self.request_metrics: Dict[str, RequestMetrics] = {}\n    79→    \n    80→    def intercept_service(self, continuation: Callable, handler_call_details: grpc.HandlerCallDetails) -> grpc.RpcMethodHandler:\n    81→        \"\"\"Intercept gRPC service calls for logging.\"\"\"\n    82→        \n    83→        def logging_wrapper(behavior: Callable) -> Callable:\n    84→            def new_behavior(request: Any, context: grpc.ServicerContext) -> Any:\n    85→                method_name = handler_call_details.method\n    86→                \n    87→                # Skip logging for excluded methods\n    88→                if method_name in self.EXCLUDED_METHODS:\n    89→                    return behavior(request, context)\n    90→                \n    91→                # Generate request ID if not set by previous interceptor\n    92→                request_id = getattr(context, 'request_id', str(uuid.uuid4()))\n    93→                context.request_id = request_id\n    94→                \n    95→                # Initialize metrics\n    96→                metrics = RequestMetrics(\n    97→                    request_id=request_id,\n    98→                    method_name=method_name,\n    99→                    start_time=time.time(),\n   100→                    request_size_bytes=self._calculate_message_size(request)\n   101→                )\n   102→                \n   103→                # Get user context if available (set by auth interceptor)\n   104→                auth_context = getattr(context, 'auth_context', None)\n   105→                if auth_context:\n   106→                    metrics.user_id = auth_context.user_id\n   107→                \n   108→                try:\n   109→                    # Log incoming request\n   110→                    self._log_request(request, context, metrics, auth_context)\n   111→                    \n   112→                    # Call the actual service method\n   113→                    start_time = time.time()\n   114→                    response = behavior(request, context)\n   115→                    end_time = time.time()\n   116→                    \n   117→                    # Update metrics\n   118→                    metrics.end_time = end_time\n   119→                    metrics.duration_ms = (end_time - start_time) * 1000\n   120→                    metrics.status_code = grpc.StatusCode.OK\n   121→                    metrics.response_size_bytes = self._calculate_message_size(response)\n   122→                    \n   123→                    # Log successful response\n   124→                    self._log_response(response, context, metrics, auth_context)\n   125→                    \n   126→                    # Log performance metrics\n   127→                    if self.log_performance:\n   128→                        self._log_performance_metrics(metrics)\n   129→                    \n   130→                    # Audit logging for sensitive operations\n   131→                    if method_name in self.AUDIT_METHODS:\n   132→                        self._log_audit_event(request, response, context, metrics, auth_context, success=True)\n   133→                    \n   134→                    return response\n   135→                    \n   136→                except Exception as e:\n   137→                    # Update metrics with error information\n   138→                    metrics.end_time = time.time()\n   139→                    metrics.duration_ms = (metrics.end_time - metrics.start_time) * 1000\n   140→                    metrics.error_message = str(e)\n   141→                    \n   142→                    # Determine gRPC status code\n   143→                    if hasattr(context, '_state') and hasattr(context._state, 'code'):\n   144→                        metrics.status_code = context._state.code\n   145→                    else:\n   146→                        metrics.status_code = grpc.StatusCode.INTERNAL\n   147→                    \n   148→                    # Log error\n   149→                    self._log_error(e, context, metrics, auth_context)\n   150→                    \n   151→                    # Audit logging for failed sensitive operations\n   152→                    if method_name in self.AUDIT_METHODS:\n   153→                        self._log_audit_event(request, None, context, metrics, auth_context, success=False, error=e)\n   154→                    \n   155→                    # Re-raise the exception\n   156→                    raise\n   157→                \n   158→                finally:\n   159→                    # Store metrics for monitoring\n   160→                    self.request_metrics[request_id] = metrics\n   161→                    \n   162→                    # Clean up old metrics (keep last 1000 requests)\n   163→                    if len(self.request_metrics) > 1000:\n   164→                        oldest_keys = list(self.request_metrics.keys())[:-1000]\n   165→                        for key in oldest_keys:\n   166→                            del self.request_metrics[key]\n   167→            \n   168→            return new_behavior\n   169→        \n   170→        # Get the original handler\n   171→        handler = continuation(handler_call_details)\n   172→        \n   173→        if handler is None:\n   174→            return None\n   175→        \n   176→        # Wrap the handler with logging\n   177→        return grpc.unary_unary_rpc_method_handler(\n   178→            logging_wrapper(handler.unary_unary),\n   179→            request_deserializer=handler.request_deserializer,\n   180→            response_serializer=handler.response_serializer,\n   181→        )\n   182→    \n   183→    def _log_request(self, request: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n   184→        \"\"\"Log incoming gRPC request.\"\"\"\n   185→        if not self.log_requests:\n   186→            return\n   187→        \n   188→        request_data = {\n   189→            \"request_id\": metrics.request_id,\n   190→            \"method\": metrics.method_name,\n   191→            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n   192→            \"user_id\": metrics.user_id,\n   193→            \"request_size_bytes\": metrics.request_size_bytes,\n   194→            \"client_metadata\": dict(context.invocation_metadata()) if context.invocation_metadata() else {}\n   195→        }\n   196→        \n   197→        # Add detailed request data (sanitized)\n   198→        try:\n   199→            if hasattr(request, 'DESCRIPTOR'):\n   200→                # This is a protobuf message\n   201→                request_data[\"request_payload\"] = self._proto_to_dict(request)\n   202→            elif hasattr(request, 'data'):\n   203→                request_data[\"request_data\"] = self._sanitize_request_data(request.data)\n   204→            else:\n   205→                # Fallback to string representation\n   206→                request_data[\"request_str\"] = str(request)\n   207→        except Exception as e:\n   208→            self.logger.warning(f\"Could not serialize request data: {e}\")\n   209→            request_data[\"request_error\"] = \"Could not serialize request\"\n   210→        \n   211→        self.structured_logger.info(\n   212→            \"gRPC request received\",\n   213→            extra=request_data\n   214→        )\n   215→    \n   216→    def _log_response(self, response: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n   217→        \"\"\"Log gRPC response.\"\"\"\n   218→        if not self.log_responses:\n   219→            return\n   220→        \n   221→        response_data = {\n   222→            \"request_id\": metrics.request_id,\n   223→            \"method\": metrics.method_name,\n   224→            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n   225→            \"duration_ms\": metrics.duration_ms,\n   226→            \"status\": \"SUCCESS\",\n   227→            \"status_code\": metrics.status_code.name if metrics.status_code else \"OK\",\n   228→            \"response_size_bytes\": metrics.response_size_bytes,\n   229→            \"user_id\": metrics.user_id\n   230→        }\n   231→        \n   232→        # Add detailed response data (sanitized)\n   233→        try:\n   234→            if hasattr(response, 'DESCRIPTOR'):\n   235→                # This is a protobuf message\n   236→                response_data[\"response_payload\"] = self._proto_to_dict(response, is_response=True)\n   237→            elif hasattr(response, 'code') and hasattr(response, 'message'):\n   238→                response_data[\"response_code\"] = response.code\n   239→                response_data[\"response_message\"] = response.message\n   240→            else:\n   241→                # Fallback to string representation\n   242→                response_data[\"response_str\"] = str(response)\n   243→        except Exception as e:\n   244→            self.logger.warning(f\"Could not serialize response data: {e}\")\n   245→            response_data[\"response_error\"] = \"Could not serialize response\"\n   246→        \n   247→        self.structured_logger.info(\n   248→            \"gRPC response sent\",\n   249→            extra=response_data\n   250→        )\n   251→    \n   252→    def _log_error(self, error: Exception, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n   253→        \"\"\"Log gRPC error.\"\"\"\n   254→        error_data = {\n   255→            \"request_id\": metrics.request_id,\n   256→            \"method\": metrics.method_name,\n   257→            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n   258→            \"duration_ms\": metrics.duration_ms,\n   259→            \"status\": \"ERROR\",\n   260→            \"status_code\": metrics.status_code.name if metrics.status_code else \"INTERNAL\",\n   261→            \"error_type\": type(error).__name__,\n   262→            \"error_message\": str(error),\n   263→            \"user_id\": metrics.user_id\n   264→        }\n   265→        \n   266→        self.structured_logger.error(\n   267→            \"gRPC request failed\",\n   268→            extra=error_data,\n   269→            exc_info=True\n   270→        )\n   271→    \n   272→    def _log_performance_metrics(self, metrics: RequestMetrics) -> None:\n   273→        \"\"\"Log performance metrics.\"\"\"\n   274→        perf_data = {\n   275→            \"request_id\": metrics.request_id,\n   276→            \"method\": metrics.method_name,\n   277→            \"duration_ms\": metrics.duration_ms,\n   278→            \"request_size_bytes\": metrics.request_size_bytes,\n   279→            \"response_size_bytes\": metrics.response_size_bytes,\n   280→            \"timestamp\": datetime.now(timezone.utc).isoformat()\n   281→        }\n   282→        \n   283→        self.metrics_logger.info(\"performance_metric\", extra=perf_data)\n   284→        \n   285→        # Alert on slow requests (> 5 seconds)\n   286→        if metrics.duration_ms and metrics.duration_ms > 5000:\n   287→            self.logger.warning(\n   288→                f\"Slow gRPC request detected: {metrics.method_name} \"\n   289→                f\"took {metrics.duration_ms:.2f}ms\",\n   290→                extra=perf_data\n   291→            )\n   292→    \n   293→    def _log_audit_event(self, request: Any, response: Any, context: grpc.ServicerContext, \n   294→                        metrics: RequestMetrics, auth_context: Any, success: bool, error: Exception = None) -> None:\n   295→        \"\"\"Log audit events for sensitive operations.\"\"\"\n   296→        audit_data = {\n   297→            \"event_type\": \"grpc_operation\",\n   298→            \"request_id\": metrics.request_id,\n   299→            \"method\": metrics.method_name,\n   300→            \"user_id\": metrics.user_id,\n   301→            \"user_email\": auth_context.email if auth_context else None,\n   302→            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n   303→            \"success\": success,\n   304→            \"duration_ms\": metrics.duration_ms,\n   305→            \"client_ip\": self._extract_client_ip(context),\n   306→            \"user_agent\": self._extract_user_agent(context)\n   307→        }\n   308→        \n   309→        if not success and error:\n   310→            audit_data[\"error_type\"] = type(error).__name__\n   311→            audit_data[\"error_message\"] = str(error)\n   312→        \n   313→        # Add operation-specific audit data\n   314→        if hasattr(request, 'data'):\n   315→            audit_data[\"operation_data\"] = self._sanitize_audit_data(request.data, metrics.method_name)\n   316→        \n   317→        self.audit_logger.info(\"audit_event\", extra=audit_data)\n   318→    \n   319→    def _sanitize_request_data(self, request_data: Any) -> Dict[str, Any]:\n   320→        \"\"\"Sanitize request data for logging (remove sensitive fields).\"\"\"\n   321→        if not request_data:\n   322→            return {}\n   323→        \n   324→        sensitive_fields = {'password', 'password_hash', 'token', 'secret', 'credentials', 'access_token'}\n   325→        \n   326→        sanitized = {}\n   327→        for field in request_data.DESCRIPTOR.fields:\n   328→            field_name = field.name\n   329→            if field_name.lower() in sensitive_fields:\n   330→                sanitized[field_name] = \"***REDACTED***\"\n   331→            else:\n   332→                value = getattr(request_data, field_name, None)\n   333→                if value is not None:\n   334→                    sanitized[field_name] = str(value)\n   335→        \n   336→        return sanitized\n   337→    \n   338→    def _sanitize_audit_data(self, request_data: Any, method_name: str) -> Dict[str, Any]:\n   339→        \"\"\"Sanitize data for audit logging.\"\"\"\n   340→        if not request_data:\n   341→            return {}\n   342→        \n   343→        # Method-specific sanitization rules\n   344→        if method_name == '/user.UserService/Login':\n   345→            return {\n   346→                \"email\": getattr(request_data, 'email', None),\n   347→                \"login_attempt\": True\n   348→            }\n   349→        elif method_name == '/user.UserService/RegisterUser':\n   350→            return {\n   351→                \"email\": getattr(request_data, 'email', None),\n   352→                \"username\": getattr(request_data, 'username', None),\n   353→                \"registration_attempt\": True\n   354→            }\n   355→        elif method_name == '/broker_account.BrokerAccountService/UpdateBrokerAccount':\n   356→            return {\n   357→                \"user_id\": getattr(request_data, 'user_id', None),\n   358→                \"broker_id\": getattr(request_data, 'broker_id', None),\n   359→                \"account_number\": getattr(request_data, 'account_number', None),\n   360→                \"update_attempt\": True\n   361→            }\n   362→        \n   363→        return self._sanitize_request_data(request_data)\n   364→    \n   365→    def _calculate_message_size(self, message: Any) -> Optional[int]:\n   366→        \"\"\"Calculate message size in bytes.\"\"\"\n   367→        try:\n   368→            if hasattr(message, 'SerializeToString'):\n   369→                return len(message.SerializeToString())\n   370→            return len(str(message).encode('utf-8'))\n   371→        except Exception:\n   372→            return None\n   373→    \n   374→    def _extract_client_ip(self, context: grpc.ServicerContext) -> Optional[str]:\n   375→        \"\"\"Extract client IP from gRPC context.\"\"\"\n   376→        try:\n   377→            peer = context.peer()\n   378→            if peer:\n   379→                # Format: \"ipv4:127.0.0.1:52050\" or \"ipv6:[::1]:52050\"\n   380→                if peer.startswith('ipv4:'):\n   381→                    return peer.split(':')[1]\n   382→                elif peer.startswith('ipv6:'):\n   383→                    return peer.split('[')[1].split(']')[0]\n   384→            return None\n   385→        except Exception:\n   386→            return None\n   387→    \n   388→    def _extract_user_agent(self, context: grpc.ServicerContext) -> Optional[str]:\n   389→        \"\"\"Extract user agent from gRPC metadata.\"\"\"\n   390→        try:\n   391→            metadata = dict(context.invocation_metadata())\n   392→            return metadata.get('user-agent') or metadata.get('User-Agent')\n   393→        except Exception:\n   394→            return None\n   395→    \n   396→    def _proto_to_dict(self, proto_message: Any, is_response: bool = False) -> Dict[str, Any]:\n   397→        \"\"\"Convert protobuf message to dictionary with sanitization.\"\"\"\n   398→        if not proto_message or not hasattr(proto_message, 'DESCRIPTOR'):\n   399→            return {}\n   400→        \n   401→        result = {}\n   402→        sensitive_fields = {\n   403→            'password', 'password_hash', 'token', 'secret', 'credentials', \n   404→            'access_token', 'refresh_token', 'api_key', 'private_key'\n   405→        }\n   406→        \n   407→        try:\n   408→            for field in proto_message.DESCRIPTOR.fields:\n   409→                field_name = field.name\n   410→                \n   411→                try:\n   412→                    value = getattr(proto_message, field_name, None)\n   413→                    \n   414→                    # Skip None/empty values\n   415→                    if value is None:\n   416→                        continue\n   417→                    \n   418→                    # Handle sensitive fields\n   419→                    if field_name.lower() in sensitive_fields:\n   420→                        result[field_name] = \"***REDACTED***\"\n   421→                        continue\n   422→                    \n   423→                    # Handle repeated fields (lists)\n   424→                    if field.label == field.LABEL_REPEATED:\n   425→                        if not value:\n   426→                            continue\n   427→                        result[field_name] = []\n   428→                        for item in value:\n   429→                            if hasattr(item, 'DESCRIPTOR'):\n   430→                                # Nested protobuf message\n   431→                                result[field_name].append(self._proto_to_dict(item, is_response))\n   432→                            else:\n   433→                                result[field_name].append(self._serialize_value(item))\n   434→                    \n   435→                    # Handle nested protobuf messages\n   436→                    elif hasattr(value, 'DESCRIPTOR'):\n   437→                        result[field_name] = self._proto_to_dict(value, is_response)\n   438→                    \n   439→                    # Handle enum values\n   440→                    elif hasattr(value, 'name'):\n   441→                        result[field_name] = value.name\n   442→                    \n   443→                    # Handle primitive values\n   444→                    else:\n   445→                        result[field_name] = self._serialize_value(value)\n   446→                        \n   447→                except Exception as field_error:\n   448→                    self.logger.warning(f\"Error processing field {field_name}: {field_error}\")\n   449→                    result[field_name] = f\"<ERROR: {str(field_error)}>\"\n   450→            \n   451→            return result\n   452→            \n   453→        except Exception as e:\n   454→            self.logger.error(f\"Error converting protobuf to dict: {e}\")\n   455→            return {\"error\": f\"Could not serialize protobuf: {str(e)}\"}\n   456→    \n   457→    def _serialize_value(self, value: Any) -> Any:\n   458→        \"\"\"Serialize a primitive value for logging.\"\"\"\n   459→        try:\n   460→            # Handle common Python types\n   461→            if isinstance(value, (str, int, float, bool)):\n   462→                return value\n   463→            elif isinstance(value, bytes):\n   464→                # Convert bytes to base64 for logging\n   465→                import base64\n   466→                return f\"<bytes:{base64.b64encode(value).decode()[:100]}...>\"\n   467→            elif hasattr(value, 'isoformat'):\n   468→                # Handle datetime objects\n   469→                return value.isoformat()\n   470→            else:\n   471→                # Fallback to string representation\n   472→                str_value = str(value)\n   473→                # Truncate very long values\n   474→                if len(str_value) > 1000:\n   475→                    return str_value[:1000] + \"...\"\n   476→                return str_value\n   477→        except Exception:\n   478→            return \"<non-serializable>\"\n   479→    \n   480→    def get_metrics_summary(self) -> Dict[str, Any]:\n   481→        \"\"\"Get summary of collected metrics.\"\"\"\n   482→        if not self.request_metrics:\n   483→            return {}\n   484→        \n   485→        total_requests = len(self.request_metrics)\n   486→        successful_requests = sum(1 for m in self.request_metrics.values() \n   487→                                if m.status_code == grpc.StatusCode.OK)\n   488→        failed_requests = total_requests - successful_requests\n   489→        \n   490→        durations = [m.duration_ms for m in self.request_metrics.values() if m.duration_ms]\n   491→        avg_duration = sum(durations) / len(durations) if durations else 0\n   492→        \n   493→        return {\n   494→            \"total_requests\": total_requests,\n   495→            \"successful_requests\": successful_requests,\n   496→            \"failed_requests\": failed_requests,\n   497→            \"success_rate\": successful_requests / total_requests if total_requests > 0 else 0,\n   498→            \"average_duration_ms\": avg_duration,\n   499→            \"methods\": list(set(m.method_name for m in self.request_metrics.values()))\n   500→        }"}]},"uuid":"1f249b35-f1a5-47a5-b763-22d6af0a4d2e","timestamp":"2025-08-20T01:57:34.443Z","toolUseResult":{"filePath":"/Users/lee/Documents/src/bep-oms-broker/src/infrastructure/grpc/interceptors/logging_interceptor.py","oldString":"\"\"\"\nAdvanced Logging gRPC Interceptor\n\nThis interceptor provides comprehensive logging, metrics collection,\nand request tracing for gRPC services.\n\"\"\"\n\nimport grpc\nimport logging\nimport time\nimport json\nimport uuid\nfrom typing import Callable, Any, Dict, Optional\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime, timezone\nfrom functools import wraps\n\nfrom src.infrastructure.logging.structured_logger import StructuredLogger\n\n\n@dataclass\nclass RequestMetrics:\n    \"\"\"Request metrics for monitoring and analysis.\"\"\"\n    request_id: str\n    method_name: str\n    start_time: float\n    end_time: Optional[float] = None\n    duration_ms: Optional[float] = None\n    status_code: Optional[grpc.StatusCode] = None\n    request_size_bytes: Optional[int] = None\n    response_size_bytes: Optional[int] = None\n    user_id: Optional[str] = None\n    error_message: Optional[str] = None\n\n\nclass LoggingInterceptor(grpc.ServerInterceptor):\n    \"\"\"\n    Advanced logging interceptor with structured logging and metrics.\n    \n    Features:\n    - Structured logging with correlation IDs\n    - Request/response metrics collection\n    - Performance monitoring\n    - Error tracking and alerting\n    - Audit logging for sensitive operations\n    \"\"\"\n    \n    # Sensitive methods that require audit logging\n    AUDIT_METHODS = {\n        '/user.UserService/Login',\n        '/user.UserService/RegisterUser',\n        '/user.UserService/DeactivateUser',\n        '/broker_account.BrokerAccountService/UpdateBrokerAccount',\n    }\n    \n    # Methods to exclude from detailed logging (health checks, etc.)\n    EXCLUDED_METHODS = {\n        '/grpc.health.v1.Health/Check',\n    }\n    \n    def __init__(self, \n                 service_name: str = \"BEP-OMS-BROKER\",\n                 log_requests: bool = True,\n                 log_responses: bool = True,\n                 log_performance: bool = True):\n        self.service_name = service_name\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_performance = log_performance\n        \n        # Setup loggers\n        self.logger = logging.getLogger(__name__)\n        self.audit_logger = logging.getLogger(f\"{__name__}.audit\")\n        self.metrics_logger = logging.getLogger(f\"{__name__}.metrics\")\n        self.structured_logger = StructuredLogger(service_name)\n        \n        # Metrics storage (in production, use proper metrics backend)\n        self.request_metrics: Dict[str, RequestMetrics] = {}\n    \n    def intercept_service(self, continuation: Callable, handler_call_details: grpc.HandlerCallDetails) -> grpc.RpcMethodHandler:\n        \"\"\"Intercept gRPC service calls for logging.\"\"\"\n        \n        def logging_wrapper(behavior: Callable) -> Callable:\n            def new_behavior(request: Any, context: grpc.ServicerContext) -> Any:\n                method_name = handler_call_details.method\n                \n                # Skip logging for excluded methods\n                if method_name in self.EXCLUDED_METHODS:\n                    return behavior(request, context)\n                \n                # Generate request ID if not set by previous interceptor\n                request_id = getattr(context, 'request_id', str(uuid.uuid4()))\n                context.request_id = request_id\n                \n                # Initialize metrics\n                metrics = RequestMetrics(\n                    request_id=request_id,\n                    method_name=method_name,\n                    start_time=time.time(),\n                    request_size_bytes=self._calculate_message_size(request)\n                )\n                \n                # Get user context if available (set by auth interceptor)\n                auth_context = getattr(context, 'auth_context', None)\n                if auth_context:\n                    metrics.user_id = auth_context.user_id\n                \n                try:\n                    # Log incoming request\n                    self._log_request(request, context, metrics, auth_context)\n                    \n                    # Call the actual service method\n                    start_time = time.time()\n                    response = behavior(request, context)\n                    end_time = time.time()\n                    \n                    # Update metrics\n                    metrics.end_time = end_time\n                    metrics.duration_ms = (end_time - start_time) * 1000\n                    metrics.status_code = grpc.StatusCode.OK\n                    metrics.response_size_bytes = self._calculate_message_size(response)\n                    \n                    # Log successful response\n                    self._log_response(response, context, metrics, auth_context)\n                    \n                    # Log performance metrics\n                    if self.log_performance:\n                        self._log_performance_metrics(metrics)\n                    \n                    # Audit logging for sensitive operations\n                    if method_name in self.AUDIT_METHODS:\n                        self._log_audit_event(request, response, context, metrics, auth_context, success=True)\n                    \n                    return response\n                    \n                except Exception as e:\n                    # Update metrics with error information\n                    metrics.end_time = time.time()\n                    metrics.duration_ms = (metrics.end_time - metrics.start_time) * 1000\n                    metrics.error_message = str(e)\n                    \n                    # Determine gRPC status code\n                    if hasattr(context, '_state') and hasattr(context._state, 'code'):\n                        metrics.status_code = context._state.code\n                    else:\n                        metrics.status_code = grpc.StatusCode.INTERNAL\n                    \n                    # Log error\n                    self._log_error(e, context, metrics, auth_context)\n                    \n                    # Audit logging for failed sensitive operations\n                    if method_name in self.AUDIT_METHODS:\n                        self._log_audit_event(request, None, context, metrics, auth_context, success=False, error=e)\n                    \n                    # Re-raise the exception\n                    raise\n                \n                finally:\n                    # Store metrics for monitoring\n                    self.request_metrics[request_id] = metrics\n                    \n                    # Clean up old metrics (keep last 1000 requests)\n                    if len(self.request_metrics) > 1000:\n                        oldest_keys = list(self.request_metrics.keys())[:-1000]\n                        for key in oldest_keys:\n                            del self.request_metrics[key]\n            \n            return new_behavior\n        \n        # Get the original handler\n        handler = continuation(handler_call_details)\n        \n        if handler is None:\n            return None\n        \n        # Wrap the handler with logging\n        return grpc.unary_unary_rpc_method_handler(\n            logging_wrapper(handler.unary_unary),\n            request_deserializer=handler.request_deserializer,\n            response_serializer=handler.response_serializer,\n        )\n    \n    def _log_request(self, request: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log incoming gRPC request.\"\"\"\n        if not self.log_requests:\n            return\n        \n        request_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"user_id\": metrics.user_id,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"client_metadata\": dict(context.invocation_metadata()) if context.invocation_metadata() else {}\n        }\n        \n        # Add detailed request data (sanitized)\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # This is a protobuf message\n                request_data[\"request_payload\"] = self._proto_to_dict(request)\n            elif hasattr(request, 'data'):\n                request_data[\"request_data\"] = self._sanitize_request_data(request.data)\n            else:\n                # Fallback to string representation\n                request_data[\"request_str\"] = str(request)\n        except Exception as e:\n            self.logger.warning(f\"Could not serialize request data: {e}\")\n            request_data[\"request_error\"] = \"Could not serialize request\"\n        \n        self.structured_logger.info(\n            \"gRPC request received\",\n            extra=request_data\n        )\n    \n    def _log_response(self, response: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC response.\"\"\"\n        if not self.log_responses:\n            return\n        \n        response_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"SUCCESS\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"OK\",\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"user_id\": metrics.user_id\n        }\n        \n        # Add detailed response data (sanitized)\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # This is a protobuf message\n                response_data[\"response_payload\"] = self._proto_to_dict(response, is_response=True)\n            elif hasattr(response, 'code') and hasattr(response, 'message'):\n                response_data[\"response_code\"] = response.code\n                response_data[\"response_message\"] = response.message\n            else:\n                # Fallback to string representation\n                response_data[\"response_str\"] = str(response)\n        except Exception as e:\n            self.logger.warning(f\"Could not serialize response data: {e}\")\n            response_data[\"response_error\"] = \"Could not serialize response\"\n        \n        self.structured_logger.info(\n            \"gRPC response sent\",\n            extra=response_data\n        )\n    \n    def _log_error(self, error: Exception, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC error.\"\"\"\n        error_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"ERROR\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"INTERNAL\",\n            \"error_type\": type(error).__name__,\n            \"error_message\": str(error),\n            \"user_id\": metrics.user_id\n        }\n        \n        self.structured_logger.error(\n            \"gRPC request failed\",\n            extra=error_data,\n            exc_info=True\n        )\n    \n    def _log_performance_metrics(self, metrics: RequestMetrics) -> None:\n        \"\"\"Log performance metrics.\"\"\"\n        perf_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"duration_ms\": metrics.duration_ms,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"timestamp\": datetime.now(timezone.utc).isoformat()\n        }\n        \n        self.metrics_logger.info(\"performance_metric\", extra=perf_data)\n        \n        # Alert on slow requests (> 5 seconds)\n        if metrics.duration_ms and metrics.duration_ms > 5000:\n            self.logger.warning(\n                f\"Slow gRPC request detected: {metrics.method_name} \"\n                f\"took {metrics.duration_ms:.2f}ms\",\n                extra=perf_data\n            )\n    \n    def _log_audit_event(self, request: Any, response: Any, context: grpc.ServicerContext, \n                        metrics: RequestMetrics, auth_context: Any, success: bool, error: Exception = None) -> None:\n        \"\"\"Log audit events for sensitive operations.\"\"\"\n        audit_data = {\n            \"event_type\": \"grpc_operation\",\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"user_id\": metrics.user_id,\n            \"user_email\": auth_context.email if auth_context else None,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"success\": success,\n            \"duration_ms\": metrics.duration_ms,\n            \"client_ip\": self._extract_client_ip(context),\n            \"user_agent\": self._extract_user_agent(context)\n        }\n        \n        if not success and error:\n            audit_data[\"error_type\"] = type(error).__name__\n            audit_data[\"error_message\"] = str(error)\n        \n        # Add operation-specific audit data\n        if hasattr(request, 'data'):\n            audit_data[\"operation_data\"] = self._sanitize_audit_data(request.data, metrics.method_name)\n        \n        self.audit_logger.info(\"audit_event\", extra=audit_data)\n    \n    def _sanitize_request_data(self, request_data: Any) -> Dict[str, Any]:\n        \"\"\"Sanitize request data for logging (remove sensitive fields).\"\"\"\n        if not request_data:\n            return {}\n        \n        sensitive_fields = {'password', 'password_hash', 'token', 'secret', 'credentials', 'access_token'}\n        \n        sanitized = {}\n        for field in request_data.DESCRIPTOR.fields:\n            field_name = field.name\n            if field_name.lower() in sensitive_fields:\n                sanitized[field_name] = \"***REDACTED***\"\n            else:\n                value = getattr(request_data, field_name, None)\n                if value is not None:\n                    sanitized[field_name] = str(value)\n        \n        return sanitized\n    \n    def _sanitize_audit_data(self, request_data: Any, method_name: str) -> Dict[str, Any]:\n        \"\"\"Sanitize data for audit logging.\"\"\"\n        if not request_data:\n            return {}\n        \n        # Method-specific sanitization rules\n        if method_name == '/user.UserService/Login':\n            return {\n                \"email\": getattr(request_data, 'email', None),\n                \"login_attempt\": True\n            }\n        elif method_name == '/user.UserService/RegisterUser':\n            return {\n                \"email\": getattr(request_data, 'email', None),\n                \"username\": getattr(request_data, 'username', None),\n                \"registration_attempt\": True\n            }\n        elif method_name == '/broker_account.BrokerAccountService/UpdateBrokerAccount':\n            return {\n                \"user_id\": getattr(request_data, 'user_id', None),\n                \"broker_id\": getattr(request_data, 'broker_id', None),\n                \"account_number\": getattr(request_data, 'account_number', None),\n                \"update_attempt\": True\n            }\n        \n        return self._sanitize_request_data(request_data)\n    \n    def _calculate_message_size(self, message: Any) -> Optional[int]:\n        \"\"\"Calculate message size in bytes.\"\"\"\n        try:\n            if hasattr(message, 'SerializeToString'):\n                return len(message.SerializeToString())\n            return len(str(message).encode('utf-8'))\n        except Exception:\n            return None\n    \n    def _extract_client_ip(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract client IP from gRPC context.\"\"\"\n        try:\n            peer = context.peer()\n            if peer:\n                # Format: \"ipv4:127.0.0.1:52050\" or \"ipv6:[::1]:52050\"\n                if peer.startswith('ipv4:'):\n                    return peer.split(':')[1]\n                elif peer.startswith('ipv6:'):\n                    return peer.split('[')[1].split(']')[0]\n            return None\n        except Exception:\n            return None\n    \n    def _extract_user_agent(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract user agent from gRPC metadata.\"\"\"\n        try:\n            metadata = dict(context.invocation_metadata())\n            return metadata.get('user-agent') or metadata.get('User-Agent')\n        except Exception:\n            return None\n    \n    def get_metrics_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary of collected metrics.\"\"\"\n        if not self.request_metrics:\n            return {}\n        \n        total_requests = len(self.request_metrics)\n        successful_requests = sum(1 for m in self.request_metrics.values() \n                                if m.status_code == grpc.StatusCode.OK)\n        failed_requests = total_requests - successful_requests\n        \n        durations = [m.duration_ms for m in self.request_metrics.values() if m.duration_ms]\n        avg_duration = sum(durations) / len(durations) if durations else 0\n        \n        return {\n            \"total_requests\": total_requests,\n            \"successful_requests\": successful_requests,\n            \"failed_requests\": failed_requests,\n            \"success_rate\": successful_requests / total_requests if total_requests > 0 else 0,\n            \"average_duration_ms\": avg_duration,\n            \"methods\": list(set(m.method_name for m in self.request_metrics.values()))\n        }","newString":"\"\"\"\nAdvanced Logging gRPC Interceptor\n\nThis interceptor provides comprehensive logging, metrics collection,\nand request tracing for gRPC services.\n\"\"\"\n\nimport grpc\nimport logging\nimport time\nimport json\nimport uuid\nfrom typing import Callable, Any, Dict, Optional\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime, timezone\nfrom functools import wraps\n\nfrom src.infrastructure.logging.structured_logger import StructuredLogger\n\n\n@dataclass\nclass RequestMetrics:\n    \"\"\"Request metrics for monitoring and analysis.\"\"\"\n    request_id: str\n    method_name: str\n    start_time: float\n    end_time: Optional[float] = None\n    duration_ms: Optional[float] = None\n    status_code: Optional[grpc.StatusCode] = None\n    request_size_bytes: Optional[int] = None\n    response_size_bytes: Optional[int] = None\n    user_id: Optional[str] = None\n    error_message: Optional[str] = None\n\n\nclass LoggingInterceptor(grpc.ServerInterceptor):\n    \"\"\"\n    Advanced logging interceptor with structured logging and metrics.\n    \n    Features:\n    - Structured logging with correlation IDs\n    - Request/response metrics collection\n    - Performance monitoring\n    - Error tracking and alerting\n    - Audit logging for sensitive operations\n    \"\"\"\n    \n    # Sensitive methods that require audit logging\n    AUDIT_METHODS = {\n        '/user.UserService/Login',\n        '/user.UserService/RegisterUser',\n        '/user.UserService/DeactivateUser',\n        '/broker_account.BrokerAccountService/UpdateBrokerAccount',\n    }\n    \n    # Methods to exclude from detailed logging (health checks, etc.)\n    EXCLUDED_METHODS = {\n        '/grpc.health.v1.Health/Check',\n    }\n    \n    def __init__(self, \n                 service_name: str = \"BEP-OMS-BROKER\",\n                 log_requests: bool = True,\n                 log_responses: bool = True,\n                 log_performance: bool = True):\n        self.service_name = service_name\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_performance = log_performance\n        \n        # Setup loggers\n        self.logger = logging.getLogger(__name__)\n        self.audit_logger = logging.getLogger(f\"{__name__}.audit\")\n        self.metrics_logger = logging.getLogger(f\"{__name__}.metrics\")\n        self.structured_logger = StructuredLogger(service_name)\n        \n        # Metrics storage (in production, use proper metrics backend)\n        self.request_metrics: Dict[str, RequestMetrics] = {}\n    \n    def intercept_service(self, continuation: Callable, handler_call_details: grpc.HandlerCallDetails) -> grpc.RpcMethodHandler:\n        \"\"\"Intercept gRPC service calls for logging.\"\"\"\n        \n        def logging_wrapper(behavior: Callable) -> Callable:\n            def new_behavior(request: Any, context: grpc.ServicerContext) -> Any:\n                method_name = handler_call_details.method\n                \n                # Skip logging for excluded methods\n                if method_name in self.EXCLUDED_METHODS:\n                    return behavior(request, context)\n                \n                # Generate request ID if not set by previous interceptor\n                request_id = getattr(context, 'request_id', str(uuid.uuid4()))\n                context.request_id = request_id\n                \n                # Initialize metrics\n                metrics = RequestMetrics(\n                    request_id=request_id,\n                    method_name=method_name,\n                    start_time=time.time(),\n                    request_size_bytes=self._calculate_message_size(request)\n                )\n                \n                # Get user context if available (set by auth interceptor)\n                auth_context = getattr(context, 'auth_context', None)\n                if auth_context:\n                    metrics.user_id = auth_context.user_id\n                \n                try:\n                    # Log incoming request\n                    self._log_request(request, context, metrics, auth_context)\n                    \n                    # Call the actual service method\n                    start_time = time.time()\n                    response = behavior(request, context)\n                    end_time = time.time()\n                    \n                    # Update metrics\n                    metrics.end_time = end_time\n                    metrics.duration_ms = (end_time - start_time) * 1000\n                    metrics.status_code = grpc.StatusCode.OK\n                    metrics.response_size_bytes = self._calculate_message_size(response)\n                    \n                    # Log successful response\n                    self._log_response(response, context, metrics, auth_context)\n                    \n                    # Log performance metrics\n                    if self.log_performance:\n                        self._log_performance_metrics(metrics)\n                    \n                    # Audit logging for sensitive operations\n                    if method_name in self.AUDIT_METHODS:\n                        self._log_audit_event(request, response, context, metrics, auth_context, success=True)\n                    \n                    return response\n                    \n                except Exception as e:\n                    # Update metrics with error information\n                    metrics.end_time = time.time()\n                    metrics.duration_ms = (metrics.end_time - metrics.start_time) * 1000\n                    metrics.error_message = str(e)\n                    \n                    # Determine gRPC status code\n                    if hasattr(context, '_state') and hasattr(context._state, 'code'):\n                        metrics.status_code = context._state.code\n                    else:\n                        metrics.status_code = grpc.StatusCode.INTERNAL\n                    \n                    # Log error\n                    self._log_error(e, context, metrics, auth_context)\n                    \n                    # Audit logging for failed sensitive operations\n                    if method_name in self.AUDIT_METHODS:\n                        self._log_audit_event(request, None, context, metrics, auth_context, success=False, error=e)\n                    \n                    # Re-raise the exception\n                    raise\n                \n                finally:\n                    # Store metrics for monitoring\n                    self.request_metrics[request_id] = metrics\n                    \n                    # Clean up old metrics (keep last 1000 requests)\n                    if len(self.request_metrics) > 1000:\n                        oldest_keys = list(self.request_metrics.keys())[:-1000]\n                        for key in oldest_keys:\n                            del self.request_metrics[key]\n            \n            return new_behavior\n        \n        # Get the original handler\n        handler = continuation(handler_call_details)\n        \n        if handler is None:\n            return None\n        \n        # Wrap the handler with logging\n        return grpc.unary_unary_rpc_method_handler(\n            logging_wrapper(handler.unary_unary),\n            request_deserializer=handler.request_deserializer,\n            response_serializer=handler.response_serializer,\n        )\n    \n    def _log_request(self, request: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log incoming gRPC request.\"\"\"\n        if not self.log_requests:\n            return\n        \n        request_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"user_id\": metrics.user_id,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"client_metadata\": dict(context.invocation_metadata()) if context.invocation_metadata() else {}\n        }\n        \n        # Add detailed request data (sanitized)\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # This is a protobuf message\n                request_data[\"request_payload\"] = self._proto_to_dict(request)\n            elif hasattr(request, 'data'):\n                request_data[\"request_data\"] = self._sanitize_request_data(request.data)\n            else:\n                # Fallback to string representation\n                request_data[\"request_str\"] = str(request)\n        except Exception as e:\n            self.logger.warning(f\"Could not serialize request data: {e}\")\n            request_data[\"request_error\"] = \"Could not serialize request\"\n        \n        self.structured_logger.info(\n            \"gRPC request received\",\n            extra=request_data\n        )\n    \n    def _log_response(self, response: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC response.\"\"\"\n        if not self.log_responses:\n            return\n        \n        response_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"SUCCESS\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"OK\",\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"user_id\": metrics.user_id\n        }\n        \n        # Add detailed response data (sanitized)\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # This is a protobuf message\n                response_data[\"response_payload\"] = self._proto_to_dict(response, is_response=True)\n            elif hasattr(response, 'code') and hasattr(response, 'message'):\n                response_data[\"response_code\"] = response.code\n                response_data[\"response_message\"] = response.message\n            else:\n                # Fallback to string representation\n                response_data[\"response_str\"] = str(response)\n        except Exception as e:\n            self.logger.warning(f\"Could not serialize response data: {e}\")\n            response_data[\"response_error\"] = \"Could not serialize response\"\n        \n        self.structured_logger.info(\n            \"gRPC response sent\",\n            extra=response_data\n        )\n    \n    def _log_error(self, error: Exception, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC error.\"\"\"\n        error_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"ERROR\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"INTERNAL\",\n            \"error_type\": type(error).__name__,\n            \"error_message\": str(error),\n            \"user_id\": metrics.user_id\n        }\n        \n        self.structured_logger.error(\n            \"gRPC request failed\",\n            extra=error_data,\n            exc_info=True\n        )\n    \n    def _log_performance_metrics(self, metrics: RequestMetrics) -> None:\n        \"\"\"Log performance metrics.\"\"\"\n        perf_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"duration_ms\": metrics.duration_ms,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"timestamp\": datetime.now(timezone.utc).isoformat()\n        }\n        \n        self.metrics_logger.info(\"performance_metric\", extra=perf_data)\n        \n        # Alert on slow requests (> 5 seconds)\n        if metrics.duration_ms and metrics.duration_ms > 5000:\n            self.logger.warning(\n                f\"Slow gRPC request detected: {metrics.method_name} \"\n                f\"took {metrics.duration_ms:.2f}ms\",\n                extra=perf_data\n            )\n    \n    def _log_audit_event(self, request: Any, response: Any, context: grpc.ServicerContext, \n                        metrics: RequestMetrics, auth_context: Any, success: bool, error: Exception = None) -> None:\n        \"\"\"Log audit events for sensitive operations.\"\"\"\n        audit_data = {\n            \"event_type\": \"grpc_operation\",\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"user_id\": metrics.user_id,\n            \"user_email\": auth_context.email if auth_context else None,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"success\": success,\n            \"duration_ms\": metrics.duration_ms,\n            \"client_ip\": self._extract_client_ip(context),\n            \"user_agent\": self._extract_user_agent(context)\n        }\n        \n        if not success and error:\n            audit_data[\"error_type\"] = type(error).__name__\n            audit_data[\"error_message\"] = str(error)\n        \n        # Add operation-specific audit data\n        if hasattr(request, 'data'):\n            audit_data[\"operation_data\"] = self._sanitize_audit_data(request.data, metrics.method_name)\n        \n        self.audit_logger.info(\"audit_event\", extra=audit_data)\n    \n    def _sanitize_request_data(self, request_data: Any) -> Dict[str, Any]:\n        \"\"\"Sanitize request data for logging (remove sensitive fields).\"\"\"\n        if not request_data:\n            return {}\n        \n        sensitive_fields = {'password', 'password_hash', 'token', 'secret', 'credentials', 'access_token'}\n        \n        sanitized = {}\n        for field in request_data.DESCRIPTOR.fields:\n            field_name = field.name\n            if field_name.lower() in sensitive_fields:\n                sanitized[field_name] = \"***REDACTED***\"\n            else:\n                value = getattr(request_data, field_name, None)\n                if value is not None:\n                    sanitized[field_name] = str(value)\n        \n        return sanitized\n    \n    def _sanitize_audit_data(self, request_data: Any, method_name: str) -> Dict[str, Any]:\n        \"\"\"Sanitize data for audit logging.\"\"\"\n        if not request_data:\n            return {}\n        \n        # Method-specific sanitization rules\n        if method_name == '/user.UserService/Login':\n            return {\n                \"email\": getattr(request_data, 'email', None),\n                \"login_attempt\": True\n            }\n        elif method_name == '/user.UserService/RegisterUser':\n            return {\n                \"email\": getattr(request_data, 'email', None),\n                \"username\": getattr(request_data, 'username', None),\n                \"registration_attempt\": True\n            }\n        elif method_name == '/broker_account.BrokerAccountService/UpdateBrokerAccount':\n            return {\n                \"user_id\": getattr(request_data, 'user_id', None),\n                \"broker_id\": getattr(request_data, 'broker_id', None),\n                \"account_number\": getattr(request_data, 'account_number', None),\n                \"update_attempt\": True\n            }\n        \n        return self._sanitize_request_data(request_data)\n    \n    def _calculate_message_size(self, message: Any) -> Optional[int]:\n        \"\"\"Calculate message size in bytes.\"\"\"\n        try:\n            if hasattr(message, 'SerializeToString'):\n                return len(message.SerializeToString())\n            return len(str(message).encode('utf-8'))\n        except Exception:\n            return None\n    \n    def _extract_client_ip(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract client IP from gRPC context.\"\"\"\n        try:\n            peer = context.peer()\n            if peer:\n                # Format: \"ipv4:127.0.0.1:52050\" or \"ipv6:[::1]:52050\"\n                if peer.startswith('ipv4:'):\n                    return peer.split(':')[1]\n                elif peer.startswith('ipv6:'):\n                    return peer.split('[')[1].split(']')[0]\n            return None\n        except Exception:\n            return None\n    \n    def _extract_user_agent(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract user agent from gRPC metadata.\"\"\"\n        try:\n            metadata = dict(context.invocation_metadata())\n            return metadata.get('user-agent') or metadata.get('User-Agent')\n        except Exception:\n            return None\n    \n    def _proto_to_dict(self, proto_message: Any, is_response: bool = False) -> Dict[str, Any]:\n        \"\"\"Convert protobuf message to dictionary with sanitization.\"\"\"\n        if not proto_message or not hasattr(proto_message, 'DESCRIPTOR'):\n            return {}\n        \n        result = {}\n        sensitive_fields = {\n            'password', 'password_hash', 'token', 'secret', 'credentials', \n            'access_token', 'refresh_token', 'api_key', 'private_key'\n        }\n        \n        try:\n            for field in proto_message.DESCRIPTOR.fields:\n                field_name = field.name\n                \n                try:\n                    value = getattr(proto_message, field_name, None)\n                    \n                    # Skip None/empty values\n                    if value is None:\n                        continue\n                    \n                    # Handle sensitive fields\n                    if field_name.lower() in sensitive_fields:\n                        result[field_name] = \"***REDACTED***\"\n                        continue\n                    \n                    # Handle repeated fields (lists)\n                    if field.label == field.LABEL_REPEATED:\n                        if not value:\n                            continue\n                        result[field_name] = []\n                        for item in value:\n                            if hasattr(item, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name].append(self._proto_to_dict(item, is_response))\n                            else:\n                                result[field_name].append(self._serialize_value(item))\n                    \n                    # Handle nested protobuf messages\n                    elif hasattr(value, 'DESCRIPTOR'):\n                        result[field_name] = self._proto_to_dict(value, is_response)\n                    \n                    # Handle enum values\n                    elif hasattr(value, 'name'):\n                        result[field_name] = value.name\n                    \n                    # Handle primitive values\n                    else:\n                        result[field_name] = self._serialize_value(value)\n                        \n                except Exception as field_error:\n                    self.logger.warning(f\"Error processing field {field_name}: {field_error}\")\n                    result[field_name] = f\"<ERROR: {str(field_error)}>\"\n            \n            return result\n            \n        except Exception as e:\n            self.logger.error(f\"Error converting protobuf to dict: {e}\")\n            return {\"error\": f\"Could not serialize protobuf: {str(e)}\"}\n    \n    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a primitive value for logging.\"\"\"\n        try:\n            # Handle common Python types\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                # Convert bytes to base64 for logging\n                import base64\n                return f\"<bytes:{base64.b64encode(value).decode()[:100]}...>\"\n            elif hasattr(value, 'isoformat'):\n                # Handle datetime objects\n                return value.isoformat()\n            else:\n                # Fallback to string representation\n                str_value = str(value)\n                # Truncate very long values\n                if len(str_value) > 1000:\n                    return str_value[:1000] + \"...\"\n                return str_value\n        except Exception:\n            return \"<non-serializable>\"\n    \n    def get_metrics_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary of collected metrics.\"\"\"\n        if not self.request_metrics:\n            return {}\n        \n        total_requests = len(self.request_metrics)\n        successful_requests = sum(1 for m in self.request_metrics.values() \n                                if m.status_code == grpc.StatusCode.OK)\n        failed_requests = total_requests - successful_requests\n        \n        durations = [m.duration_ms for m in self.request_metrics.values() if m.duration_ms]\n        avg_duration = sum(durations) / len(durations) if durations else 0\n        \n        return {\n            \"total_requests\": total_requests,\n            \"successful_requests\": successful_requests,\n            \"failed_requests\": failed_requests,\n            \"success_rate\": successful_requests / total_requests if total_requests > 0 else 0,\n            \"average_duration_ms\": avg_duration,\n            \"methods\": list(set(m.method_name for m in self.request_metrics.values()))\n        }","originalFile":"\"\"\"\nAdvanced Logging gRPC Interceptor\n\nThis interceptor provides comprehensive logging, metrics collection,\nand request tracing for gRPC services.\n\"\"\"\n\nimport grpc\nimport logging\nimport time\nimport json\nimport uuid\nfrom typing import Callable, Any, Dict, Optional\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime, timezone\nfrom functools import wraps\n\nfrom src.infrastructure.logging.structured_logger import StructuredLogger\n\n\n@dataclass\nclass RequestMetrics:\n    \"\"\"Request metrics for monitoring and analysis.\"\"\"\n    request_id: str\n    method_name: str\n    start_time: float\n    end_time: Optional[float] = None\n    duration_ms: Optional[float] = None\n    status_code: Optional[grpc.StatusCode] = None\n    request_size_bytes: Optional[int] = None\n    response_size_bytes: Optional[int] = None\n    user_id: Optional[str] = None\n    error_message: Optional[str] = None\n\n\nclass LoggingInterceptor(grpc.ServerInterceptor):\n    \"\"\"\n    Advanced logging interceptor with structured logging and metrics.\n    \n    Features:\n    - Structured logging with correlation IDs\n    - Request/response metrics collection\n    - Performance monitoring\n    - Error tracking and alerting\n    - Audit logging for sensitive operations\n    \"\"\"\n    \n    # Sensitive methods that require audit logging\n    AUDIT_METHODS = {\n        '/user.UserService/Login',\n        '/user.UserService/RegisterUser',\n        '/user.UserService/DeactivateUser',\n        '/broker_account.BrokerAccountService/UpdateBrokerAccount',\n    }\n    \n    # Methods to exclude from detailed logging (health checks, etc.)\n    EXCLUDED_METHODS = {\n        '/grpc.health.v1.Health/Check',\n    }\n    \n    def __init__(self, \n                 service_name: str = \"BEP-OMS-BROKER\",\n                 log_requests: bool = True,\n                 log_responses: bool = True,\n                 log_performance: bool = True):\n        self.service_name = service_name\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_performance = log_performance\n        \n        # Setup loggers\n        self.logger = logging.getLogger(__name__)\n        self.audit_logger = logging.getLogger(f\"{__name__}.audit\")\n        self.metrics_logger = logging.getLogger(f\"{__name__}.metrics\")\n        self.structured_logger = StructuredLogger(service_name)\n        \n        # Metrics storage (in production, use proper metrics backend)\n        self.request_metrics: Dict[str, RequestMetrics] = {}\n    \n    def intercept_service(self, continuation: Callable, handler_call_details: grpc.HandlerCallDetails) -> grpc.RpcMethodHandler:\n        \"\"\"Intercept gRPC service calls for logging.\"\"\"\n        \n        def logging_wrapper(behavior: Callable) -> Callable:\n            def new_behavior(request: Any, context: grpc.ServicerContext) -> Any:\n                method_name = handler_call_details.method\n                \n                # Skip logging for excluded methods\n                if method_name in self.EXCLUDED_METHODS:\n                    return behavior(request, context)\n                \n                # Generate request ID if not set by previous interceptor\n                request_id = getattr(context, 'request_id', str(uuid.uuid4()))\n                context.request_id = request_id\n                \n                # Initialize metrics\n                metrics = RequestMetrics(\n                    request_id=request_id,\n                    method_name=method_name,\n                    start_time=time.time(),\n                    request_size_bytes=self._calculate_message_size(request)\n                )\n                \n                # Get user context if available (set by auth interceptor)\n                auth_context = getattr(context, 'auth_context', None)\n                if auth_context:\n                    metrics.user_id = auth_context.user_id\n                \n                try:\n                    # Log incoming request\n                    self._log_request(request, context, metrics, auth_context)\n                    \n                    # Call the actual service method\n                    start_time = time.time()\n                    response = behavior(request, context)\n                    end_time = time.time()\n                    \n                    # Update metrics\n                    metrics.end_time = end_time\n                    metrics.duration_ms = (end_time - start_time) * 1000\n                    metrics.status_code = grpc.StatusCode.OK\n                    metrics.response_size_bytes = self._calculate_message_size(response)\n                    \n                    # Log successful response\n                    self._log_response(response, context, metrics, auth_context)\n                    \n                    # Log performance metrics\n                    if self.log_performance:\n                        self._log_performance_metrics(metrics)\n                    \n                    # Audit logging for sensitive operations\n                    if method_name in self.AUDIT_METHODS:\n                        self._log_audit_event(request, response, context, metrics, auth_context, success=True)\n                    \n                    return response\n                    \n                except Exception as e:\n                    # Update metrics with error information\n                    metrics.end_time = time.time()\n                    metrics.duration_ms = (metrics.end_time - metrics.start_time) * 1000\n                    metrics.error_message = str(e)\n                    \n                    # Determine gRPC status code\n                    if hasattr(context, '_state') and hasattr(context._state, 'code'):\n                        metrics.status_code = context._state.code\n                    else:\n                        metrics.status_code = grpc.StatusCode.INTERNAL\n                    \n                    # Log error\n                    self._log_error(e, context, metrics, auth_context)\n                    \n                    # Audit logging for failed sensitive operations\n                    if method_name in self.AUDIT_METHODS:\n                        self._log_audit_event(request, None, context, metrics, auth_context, success=False, error=e)\n                    \n                    # Re-raise the exception\n                    raise\n                \n                finally:\n                    # Store metrics for monitoring\n                    self.request_metrics[request_id] = metrics\n                    \n                    # Clean up old metrics (keep last 1000 requests)\n                    if len(self.request_metrics) > 1000:\n                        oldest_keys = list(self.request_metrics.keys())[:-1000]\n                        for key in oldest_keys:\n                            del self.request_metrics[key]\n            \n            return new_behavior\n        \n        # Get the original handler\n        handler = continuation(handler_call_details)\n        \n        if handler is None:\n            return None\n        \n        # Wrap the handler with logging\n        return grpc.unary_unary_rpc_method_handler(\n            logging_wrapper(handler.unary_unary),\n            request_deserializer=handler.request_deserializer,\n            response_serializer=handler.response_serializer,\n        )\n    \n    def _log_request(self, request: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log incoming gRPC request.\"\"\"\n        if not self.log_requests:\n            return\n        \n        request_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"user_id\": metrics.user_id,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"client_metadata\": dict(context.invocation_metadata()) if context.invocation_metadata() else {}\n        }\n        \n        # Add detailed request data (sanitized)\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # This is a protobuf message\n                request_data[\"request_payload\"] = self._proto_to_dict(request)\n            elif hasattr(request, 'data'):\n                request_data[\"request_data\"] = self._sanitize_request_data(request.data)\n            else:\n                # Fallback to string representation\n                request_data[\"request_str\"] = str(request)\n        except Exception as e:\n            self.logger.warning(f\"Could not serialize request data: {e}\")\n            request_data[\"request_error\"] = \"Could not serialize request\"\n        \n        self.structured_logger.info(\n            \"gRPC request received\",\n            extra=request_data\n        )\n    \n    def _log_response(self, response: Any, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC response.\"\"\"\n        if not self.log_responses:\n            return\n        \n        response_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"SUCCESS\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"OK\",\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"user_id\": metrics.user_id\n        }\n        \n        # Add detailed response data (sanitized)\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # This is a protobuf message\n                response_data[\"response_payload\"] = self._proto_to_dict(response, is_response=True)\n            elif hasattr(response, 'code') and hasattr(response, 'message'):\n                response_data[\"response_code\"] = response.code\n                response_data[\"response_message\"] = response.message\n            else:\n                # Fallback to string representation\n                response_data[\"response_str\"] = str(response)\n        except Exception as e:\n            self.logger.warning(f\"Could not serialize response data: {e}\")\n            response_data[\"response_error\"] = \"Could not serialize response\"\n        \n        self.structured_logger.info(\n            \"gRPC response sent\",\n            extra=response_data\n        )\n    \n    def _log_error(self, error: Exception, context: grpc.ServicerContext, metrics: RequestMetrics, auth_context: Any) -> None:\n        \"\"\"Log gRPC error.\"\"\"\n        error_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": metrics.duration_ms,\n            \"status\": \"ERROR\",\n            \"status_code\": metrics.status_code.name if metrics.status_code else \"INTERNAL\",\n            \"error_type\": type(error).__name__,\n            \"error_message\": str(error),\n            \"user_id\": metrics.user_id\n        }\n        \n        self.structured_logger.error(\n            \"gRPC request failed\",\n            extra=error_data,\n            exc_info=True\n        )\n    \n    def _log_performance_metrics(self, metrics: RequestMetrics) -> None:\n        \"\"\"Log performance metrics.\"\"\"\n        perf_data = {\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"duration_ms\": metrics.duration_ms,\n            \"request_size_bytes\": metrics.request_size_bytes,\n            \"response_size_bytes\": metrics.response_size_bytes,\n            \"timestamp\": datetime.now(timezone.utc).isoformat()\n        }\n        \n        self.metrics_logger.info(\"performance_metric\", extra=perf_data)\n        \n        # Alert on slow requests (> 5 seconds)\n        if metrics.duration_ms and metrics.duration_ms > 5000:\n            self.logger.warning(\n                f\"Slow gRPC request detected: {metrics.method_name} \"\n                f\"took {metrics.duration_ms:.2f}ms\",\n                extra=perf_data\n            )\n    \n    def _log_audit_event(self, request: Any, response: Any, context: grpc.ServicerContext, \n                        metrics: RequestMetrics, auth_context: Any, success: bool, error: Exception = None) -> None:\n        \"\"\"Log audit events for sensitive operations.\"\"\"\n        audit_data = {\n            \"event_type\": \"grpc_operation\",\n            \"request_id\": metrics.request_id,\n            \"method\": metrics.method_name,\n            \"user_id\": metrics.user_id,\n            \"user_email\": auth_context.email if auth_context else None,\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"success\": success,\n            \"duration_ms\": metrics.duration_ms,\n            \"client_ip\": self._extract_client_ip(context),\n            \"user_agent\": self._extract_user_agent(context)\n        }\n        \n        if not success and error:\n            audit_data[\"error_type\"] = type(error).__name__\n            audit_data[\"error_message\"] = str(error)\n        \n        # Add operation-specific audit data\n        if hasattr(request, 'data'):\n            audit_data[\"operation_data\"] = self._sanitize_audit_data(request.data, metrics.method_name)\n        \n        self.audit_logger.info(\"audit_event\", extra=audit_data)\n    \n    def _sanitize_request_data(self, request_data: Any) -> Dict[str, Any]:\n        \"\"\"Sanitize request data for logging (remove sensitive fields).\"\"\"\n        if not request_data:\n            return {}\n        \n        sensitive_fields = {'password', 'password_hash', 'token', 'secret', 'credentials', 'access_token'}\n        \n        sanitized = {}\n        for field in request_data.DESCRIPTOR.fields:\n            field_name = field.name\n            if field_name.lower() in sensitive_fields:\n                sanitized[field_name] = \"***REDACTED***\"\n            else:\n                value = getattr(request_data, field_name, None)\n                if value is not None:\n                    sanitized[field_name] = str(value)\n        \n        return sanitized\n    \n    def _sanitize_audit_data(self, request_data: Any, method_name: str) -> Dict[str, Any]:\n        \"\"\"Sanitize data for audit logging.\"\"\"\n        if not request_data:\n            return {}\n        \n        # Method-specific sanitization rules\n        if method_name == '/user.UserService/Login':\n            return {\n                \"email\": getattr(request_data, 'email', None),\n                \"login_attempt\": True\n            }\n        elif method_name == '/user.UserService/RegisterUser':\n            return {\n                \"email\": getattr(request_data, 'email', None),\n                \"username\": getattr(request_data, 'username', None),\n                \"registration_attempt\": True\n            }\n        elif method_name == '/broker_account.BrokerAccountService/UpdateBrokerAccount':\n            return {\n                \"user_id\": getattr(request_data, 'user_id', None),\n                \"broker_id\": getattr(request_data, 'broker_id', None),\n                \"account_number\": getattr(request_data, 'account_number', None),\n                \"update_attempt\": True\n            }\n        \n        return self._sanitize_request_data(request_data)\n    \n    def _calculate_message_size(self, message: Any) -> Optional[int]:\n        \"\"\"Calculate message size in bytes.\"\"\"\n        try:\n            if hasattr(message, 'SerializeToString'):\n                return len(message.SerializeToString())\n            return len(str(message).encode('utf-8'))\n        except Exception:\n            return None\n    \n    def _extract_client_ip(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract client IP from gRPC context.\"\"\"\n        try:\n            peer = context.peer()\n            if peer:\n                # Format: \"ipv4:127.0.0.1:52050\" or \"ipv6:[::1]:52050\"\n                if peer.startswith('ipv4:'):\n                    return peer.split(':')[1]\n                elif peer.startswith('ipv6:'):\n                    return peer.split('[')[1].split(']')[0]\n            return None\n        except Exception:\n            return None\n    \n    def _extract_user_agent(self, context: grpc.ServicerContext) -> Optional[str]:\n        \"\"\"Extract user agent from gRPC metadata.\"\"\"\n        try:\n            metadata = dict(context.invocation_metadata())\n            return metadata.get('user-agent') or metadata.get('User-Agent')\n        except Exception:\n            return None\n    \n    def get_metrics_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary of collected metrics.\"\"\"\n        if not self.request_metrics:\n            return {}\n        \n        total_requests = len(self.request_metrics)\n        successful_requests = sum(1 for m in self.request_metrics.values() \n                                if m.status_code == grpc.StatusCode.OK)\n        failed_requests = total_requests - successful_requests\n        \n        durations = [m.duration_ms for m in self.request_metrics.values() if m.duration_ms]\n        avg_duration = sum(durations) / len(durations) if durations else 0\n        \n        return {\n            \"total_requests\": total_requests,\n            \"successful_requests\": successful_requests,\n            \"failed_requests\": failed_requests,\n            \"success_rate\": successful_requests / total_requests if total_requests > 0 else 0,\n            \"average_duration_ms\": avg_duration,\n            \"methods\": list(set(m.method_name for m in self.request_metrics.values()))\n        }","structuredPatch":[{"oldStart":393,"oldLines":6,"newStart":393,"newLines":90,"lines":["         except Exception:","             return None","     ","+    def _proto_to_dict(self, proto_message: Any, is_response: bool = False) -> Dict[str, Any]:","+        \"\"\"Convert protobuf message to dictionary with sanitization.\"\"\"","+        if not proto_message or not hasattr(proto_message, 'DESCRIPTOR'):","+            return {}","+        ","+        result = {}","+        sensitive_fields = {","+            'password', 'password_hash', 'token', 'secret', 'credentials', ","+            'access_token', 'refresh_token', 'api_key', 'private_key'","+        }","+        ","+        try:","+            for field in proto_message.DESCRIPTOR.fields:","+                field_name = field.name","+                ","+                try:","+                    value = getattr(proto_message, field_name, None)","+                    ","+                    # Skip None/empty values","+                    if value is None:","+                        continue","+                    ","+                    # Handle sensitive fields","+                    if field_name.lower() in sensitive_fields:","+                        result[field_name] = \"***REDACTED***\"","+                        continue","+                    ","+                    # Handle repeated fields (lists)","+                    if field.label == field.LABEL_REPEATED:","+                        if not value:","+                            continue","+                        result[field_name] = []","+                        for item in value:","+                            if hasattr(item, 'DESCRIPTOR'):","+                                # Nested protobuf message","+                                result[field_name].append(self._proto_to_dict(item, is_response))","+                            else:","+                                result[field_name].append(self._serialize_value(item))","+                    ","+                    # Handle nested protobuf messages","+                    elif hasattr(value, 'DESCRIPTOR'):","+                        result[field_name] = self._proto_to_dict(value, is_response)","+                    ","+                    # Handle enum values","+                    elif hasattr(value, 'name'):","+                        result[field_name] = value.name","+                    ","+                    # Handle primitive values","+                    else:","+                        result[field_name] = self._serialize_value(value)","+                        ","+                except Exception as field_error:","+                    self.logger.warning(f\"Error processing field {field_name}: {field_error}\")","+                    result[field_name] = f\"<ERROR: {str(field_error)}>\"","+            ","+            return result","+            ","+        except Exception as e:","+            self.logger.error(f\"Error converting protobuf to dict: {e}\")","+            return {\"error\": f\"Could not serialize protobuf: {str(e)}\"}","+    ","+    def _serialize_value(self, value: Any) -> Any:","+        \"\"\"Serialize a primitive value for logging.\"\"\"","+        try:","+            # Handle common Python types","+            if isinstance(value, (str, int, float, bool)):","+                return value","+            elif isinstance(value, bytes):","+                # Convert bytes to base64 for logging","+                import base64","+                return f\"<bytes:{base64.b64encode(value).decode()[:100]}...>\"","+            elif hasattr(value, 'isoformat'):","+                # Handle datetime objects","+                return value.isoformat()","+            else:","+                # Fallback to string representation","+                str_value = str(value)","+                # Truncate very long values","+                if len(str_value) > 1000:","+                    return str_value[:1000] + \"...\"","+                return str_value","+        except Exception:","+            return \"<non-serializable>\"","+    ","     def get_metrics_summary(self) -> Dict[str, Any]:","         \"\"\"Get summary of collected metrics.\"\"\"","         if not self.request_metrics:"]}],"userModified":false,"replaceAll":false}}
{"parentUuid":"1f249b35-f1a5-47a5-b763-22d6af0a4d2e","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01JK754qs3b8uhyDx32DM4cF","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01PnqnjTYzTNydpJ1FNNcxja","name":"TodoWrite","input":{"todos":[{"id":"1","content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"completed","priority":"high"},{"id":"2","content":"Tạo interceptor gRPC để log request/response","status":"completed","priority":"high"},{"id":"3","content":"Thêm logging vào các servicer chính","status":"in_progress","priority":"high"},{"id":"4","content":"Cấu hình log format và level phù hợp","status":"pending","priority":"medium"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":8043,"cache_read_input_tokens":46299,"cache_creation":{"ephemeral_5m_input_tokens":8043,"ephemeral_1h_input_tokens":0},"output_tokens":283,"service_tier":"standard"}},"requestId":"req_011CSJ8dYFqdAUYT3GNYQoWF","type":"assistant","uuid":"9d7df522-3437-440f-ab96-17c85ccf00bc","timestamp":"2025-08-20T01:57:42.648Z"}
{"parentUuid":"9d7df522-3437-440f-ab96-17c85ccf00bc","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01PnqnjTYzTNydpJ1FNNcxja","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"d1155ad0-12ed-4b49-a08d-0a6a19b84f06","timestamp":"2025-08-20T01:57:42.684Z","toolUseResult":{"oldTodos":[{"content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"completed","priority":"high","id":"1"},{"content":"Tạo interceptor gRPC để log request/response","status":"in_progress","priority":"high","id":"2"},{"content":"Thêm logging vào các servicer chính","status":"pending","priority":"high","id":"3"},{"content":"Cấu hình log format và level phù hợp","status":"pending","priority":"medium","id":"4"}],"newTodos":[{"content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"completed","priority":"high","id":"1"},{"content":"Tạo interceptor gRPC để log request/response","status":"completed","priority":"high","id":"2"},{"content":"Thêm logging vào các servicer chính","status":"in_progress","priority":"high","id":"3"},{"content":"Cấu hình log format và level phù hợp","status":"pending","priority":"medium","id":"4"}]}}
{"parentUuid":"d1155ad0-12ed-4b49-a08d-0a6a19b84f06","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01F5GwhD1P4794qMfVjYYVrA","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Bây giờ tôi cần kiểm tra xem interceptor này có được sử dụng trong server configuration không và thêm logging vào các servicer chính:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":503,"cache_read_input_tokens":54342,"cache_creation":{"ephemeral_5m_input_tokens":503,"ephemeral_1h_input_tokens":0},"output_tokens":2,"service_tier":"standard"}},"requestId":"req_011CSJ8e8a5n2hRNQtPfeNNo","type":"assistant","uuid":"9677aedf-41a8-4d25-bd52-3925af131e22","timestamp":"2025-08-20T01:57:48.765Z"}
{"parentUuid":"9677aedf-41a8-4d25-bd52-3925af131e22","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01F5GwhD1P4794qMfVjYYVrA","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01GrCUoSGT3AAXaZTvNvhp92","name":"Read","input":{"file_path":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/server_config.py","limit":50}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":503,"cache_read_input_tokens":54342,"cache_creation":{"ephemeral_5m_input_tokens":503,"ephemeral_1h_input_tokens":0},"output_tokens":143,"service_tier":"standard"}},"requestId":"req_011CSJ8e8a5n2hRNQtPfeNNo","type":"assistant","uuid":"5b8141d2-0df1-4fd6-8c74-f09b5bb112be","timestamp":"2025-08-20T01:57:49.973Z"}
{"parentUuid":"5b8141d2-0df1-4fd6-8c74-f09b5bb112be","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GrCUoSGT3AAXaZTvNvhp92","type":"tool_result","content":"     1→\"\"\"Enhanced gRPC Server Configuration with Enterprise Patterns.\n     2→\n     3→This module provides a complete server configuration that integrates:\n     4→- Enhanced servicers with Phase 1 & 2 patterns\n     5→- Advanced interceptors (logging, auth, rate limiting)\n     6→- Health check service\n     7→- Performance monitoring and metrics\n     8→- Comprehensive error handling\n     9→- Production-ready defaults\n    10→\"\"\"\n    11→import asyncio\n    12→import logging\n    13→import signal\n    14→import sys\n    15→from concurrent import futures\n    16→from typing import List, Optional, Dict, Any\n    17→import grpc.aio as grpc\n    18→from grpc_health.v1 import health_pb2_grpc\n    19→from grpc_reflection.v1alpha import reflection\n    20→\n    21→# Enhanced servicers\n    22→from src.presentation.grpc.servicers.user_servicer import UserServicer\n    23→from src.presentation.grpc.servicers.broker_account_servicer import BrokerAccountServicer\n    24→from src.presentation.grpc.servicers.market_data_servicer import MarketDataServicer\n    25→from src.presentation.grpc.servicers.order_servicer import OrderServicer\n    26→from src.presentation.grpc.servicers.otp_servicer import OTPServicer\n    27→from src.presentation.grpc.servicers.portfolio_servicer import PortfolioServicer\n    28→\n    29→# Interceptors\n    30→from src.presentation.grpc.interceptors.logging_interceptor import create_logging_interceptor\n    31→from src.presentation.grpc.interceptors.authentication_interceptor import create_jwt_auth_interceptor\n    32→from src.presentation.grpc.interceptors.rate_limiting_interceptor import create_token_bucket_interceptor\n    33→\n    34→# Health and monitoring\n    35→from src.presentation.grpc.health.health_service import (\n    36→    HealthCheckService, ServiceHealthCheckProvider\n    37→)\n    38→from src.presentation.grpc.health.providers import (\n    39→    UserServiceHealthProvider,\n    40→    BrokerAccountServiceHealthProvider,\n    41→    OrderServiceHealthProvider,\n    42→    MarketDataServiceHealthProvider,\n    43→)\n    44→from src.presentation.grpc.monitoring.metrics_collector import get_metrics_collector\n    45→\n    46→# Generated gRPC modules\n    47→from src.presentation.grpc.generated import user_pb2_grpc, broker_account_pb2_grpc, market_data_pb2_grpc, user_pb2, broker_account_pb2, market_data_pb2, order_pb2_grpc, order_pb2, otp_pb2_grpc, otp_pb2, auth_pb2_grpc, auth_pb2, portfolio_pb2_grpc, portfolio_pb2\n    48→\n    49→# Application services\n    50→from src.application.services.user_application_service import UserApplicationService\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"b9e189aa-7292-4121-b2be-9e5caba6cc57","timestamp":"2025-08-20T01:57:50.015Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/server_config.py","content":"\"\"\"Enhanced gRPC Server Configuration with Enterprise Patterns.\n\nThis module provides a complete server configuration that integrates:\n- Enhanced servicers with Phase 1 & 2 patterns\n- Advanced interceptors (logging, auth, rate limiting)\n- Health check service\n- Performance monitoring and metrics\n- Comprehensive error handling\n- Production-ready defaults\n\"\"\"\nimport asyncio\nimport logging\nimport signal\nimport sys\nfrom concurrent import futures\nfrom typing import List, Optional, Dict, Any\nimport grpc.aio as grpc\nfrom grpc_health.v1 import health_pb2_grpc\nfrom grpc_reflection.v1alpha import reflection\n\n# Enhanced servicers\nfrom src.presentation.grpc.servicers.user_servicer import UserServicer\nfrom src.presentation.grpc.servicers.broker_account_servicer import BrokerAccountServicer\nfrom src.presentation.grpc.servicers.market_data_servicer import MarketDataServicer\nfrom src.presentation.grpc.servicers.order_servicer import OrderServicer\nfrom src.presentation.grpc.servicers.otp_servicer import OTPServicer\nfrom src.presentation.grpc.servicers.portfolio_servicer import PortfolioServicer\n\n# Interceptors\nfrom src.presentation.grpc.interceptors.logging_interceptor import create_logging_interceptor\nfrom src.presentation.grpc.interceptors.authentication_interceptor import create_jwt_auth_interceptor\nfrom src.presentation.grpc.interceptors.rate_limiting_interceptor import create_token_bucket_interceptor\n\n# Health and monitoring\nfrom src.presentation.grpc.health.health_service import (\n    HealthCheckService, ServiceHealthCheckProvider\n)\nfrom src.presentation.grpc.health.providers import (\n    UserServiceHealthProvider,\n    BrokerAccountServiceHealthProvider,\n    OrderServiceHealthProvider,\n    MarketDataServiceHealthProvider,\n)\nfrom src.presentation.grpc.monitoring.metrics_collector import get_metrics_collector\n\n# Generated gRPC modules\nfrom src.presentation.grpc.generated import user_pb2_grpc, broker_account_pb2_grpc, market_data_pb2_grpc, user_pb2, broker_account_pb2, market_data_pb2, order_pb2_grpc, order_pb2, otp_pb2_grpc, otp_pb2, auth_pb2_grpc, auth_pb2, portfolio_pb2_grpc, portfolio_pb2\n\n# Application services\nfrom src.application.services.user_application_service import UserApplicationService","numLines":50,"startLine":1,"totalLines":490}}}
{"parentUuid":"b9e189aa-7292-4121-b2be-9e5caba6cc57","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01GHLu8bwYxM5KYDeJhCc4pv","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Tôi thấy đang import `create_logging_interceptor` từ `src.presentation.grpc.interceptors.logging_interceptor`. Hãy kiểm tra interceptor đó:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":985,"cache_read_input_tokens":54845,"cache_creation":{"ephemeral_5m_input_tokens":985,"ephemeral_1h_input_tokens":0},"output_tokens":5,"service_tier":"standard"}},"requestId":"req_011CSJ8efiLVgjUwnhhNKDCM","type":"assistant","uuid":"a793bed0-2c85-424e-b255-2ce91f386f47","timestamp":"2025-08-20T01:57:54.625Z"}
{"parentUuid":"a793bed0-2c85-424e-b255-2ce91f386f47","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01GHLu8bwYxM5KYDeJhCc4pv","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01Kq4WZy4nLqAJTk2W8YdRMu","name":"Read","input":{"file_path":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":985,"cache_read_input_tokens":54845,"cache_creation":{"ephemeral_5m_input_tokens":985,"ephemeral_1h_input_tokens":0},"output_tokens":135,"service_tier":"standard"}},"requestId":"req_011CSJ8efiLVgjUwnhhNKDCM","type":"assistant","uuid":"16be9761-0cca-45a7-be29-e30302e948b4","timestamp":"2025-08-20T01:57:55.166Z"}
{"parentUuid":"16be9761-0cca-45a7-be29-e30302e948b4","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Kq4WZy4nLqAJTk2W8YdRMu","type":"tool_result","content":"     1→\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n     2→\n     3→This interceptor provides:\n     4→- Detailed request/response logging\n     5→- Performance metrics tracking\n     6→- Error logging and analysis\n     7→- Request correlation and tracing\n     8→- Structured logging with context\n     9→\"\"\"\n    10→import json\n    11→import logging\n    12→import time\n    13→from typing import Any, Callable, Iterator, Optional, Dict\n    14→from uuid import uuid4\n    15→\n    16→import grpc\n    17→import grpc.aio\n    18→from grpc.aio import ServerInterceptor, ServicerContext\n    19→\n    20→\n    21→class LoggingInterceptor(grpc.aio.ServerInterceptor):\n    22→    \"\"\"Advanced logging interceptor for gRPC services.\n    23→    \n    24→    Features:\n    25→    - Request/response logging with structured data\n    26→    - Performance metrics and timing\n    27→    - Error tracking and analysis\n    28→    - Request correlation IDs\n    29→    - Configurable log levels and filters\n    30→    - Security-aware logging (excludes sensitive data)\n    31→    \"\"\"\n    32→    \n    33→    def __init__(\n    34→        self,\n    35→        logger: Optional[logging.Logger] = None,\n    36→        log_level: int = logging.INFO,\n    37→        log_requests: bool = True,\n    38→        log_responses: bool = True,\n    39→        log_errors: bool = True,\n    40→        log_performance: bool = True,\n    41→        exclude_methods: Optional[list] = None,\n    42→        sensitive_fields: Optional[list] = None\n    43→    ):\n    44→        self.logger = logger or logging.getLogger(__name__)\n    45→        self.log_level = log_level\n    46→        self.log_requests = log_requests\n    47→        self.log_responses = log_responses\n    48→        self.log_errors = log_errors\n    49→        self.log_performance = log_performance\n    50→        self.exclude_methods = exclude_methods or []\n    51→        self.sensitive_fields = sensitive_fields or [\n    52→            'password', 'token', 'secret', 'key', 'credential', 'auth'\n    53→        ]\n    54→        \n    55→        # Performance metrics\n    56→        self._metrics = {\n    57→            'total_requests': 0,\n    58→            'successful_requests': 0,\n    59→            'failed_requests': 0,\n    60→            'avg_response_time': 0.0,\n    61→            'method_stats': {}\n    62→        }\n    63→    \n    64→    async def intercept_service(\n    65→        self,\n    66→        continuation: Callable[[Any], Any],\n    67→        handler_call_details: Any,\n    68→    ) -> Any:\n    69→        \"\"\"Intercept service calls for logging.\"\"\"\n    70→        \n    71→        # Skip excluded methods\n    72→        method_name = handler_call_details.method\n    73→        if method_name in self.exclude_methods:\n    74→            return await continuation(handler_call_details)\n    75→        \n    76→        # Get the original handler\n    77→        handler = await continuation(handler_call_details)\n    78→        if handler is None:\n    79→            return None\n    80→        \n    81→        # Wrap the handler based on its type; bypass reflection and health entirely\n    82→        if method_name in {\n    83→            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n    84→            '/grpc.health.v1.Health/Check',\n    85→            '/grpc.health.v1.Health/Watch',\n    86→        }:\n    87→            return handler\n    88→        if handler.unary_unary:\n    89→            return grpc.unary_unary_rpc_method_handler(\n    90→                self._wrap_unary_unary(handler.unary_unary, method_name),\n    91→                request_deserializer=handler.request_deserializer,\n    92→                response_serializer=handler.response_serializer,\n    93→            )\n    94→        elif handler.unary_stream:\n    95→            return grpc.unary_stream_rpc_method_handler(\n    96→                self._wrap_unary_stream(handler.unary_stream, method_name),\n    97→                request_deserializer=handler.request_deserializer,\n    98→                response_serializer=handler.response_serializer,\n    99→            )\n   100→        elif handler.stream_unary:\n   101→            return grpc.stream_unary_rpc_method_handler(\n   102→                self._wrap_stream_unary(handler.stream_unary, method_name),\n   103→                request_deserializer=handler.request_deserializer,\n   104→                response_serializer=handler.response_serializer,\n   105→            )\n   106→        elif handler.stream_stream:\n   107→            return grpc.stream_stream_rpc_method_handler(\n   108→                self._wrap_stream_stream(handler.stream_stream, method_name),\n   109→                request_deserializer=handler.request_deserializer,\n   110→                response_serializer=handler.response_serializer,\n   111→            )\n   112→        else:\n   113→            return handler\n   114→    \n   115→    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n   116→        \"\"\"Wrap unary-unary RPC method.\"\"\"\n   117→        \n   118→        async def wrapper(request: Any, context: ServicerContext) -> Any:\n   119→            return await self._handle_request(\n   120→                behavior, request, context, method_name, is_streaming=False\n   121→            )\n   122→        \n   123→        return wrapper\n   124→    \n   125→    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n   126→        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n   127→        async def wrapper(request: Any, context: ServicerContext):\n   128→            # For streaming responses, return the iterator directly (do not await)\n   129→            try:\n   130→                return behavior(request, context)\n   131→            finally:\n   132→                # Minimal logging to avoid interfering with stream lifecycle\n   133→                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n   134→        return wrapper\n   135→    \n   136→    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n   137→        \"\"\"Wrap stream-unary RPC method.\"\"\"\n   138→        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n   139→            # Request is streaming, response is unary; await the final response\n   140→            try:\n   141→                return await behavior(request_iterator, context)\n   142→            finally:\n   143→                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n   144→        return wrapper\n   145→    \n   146→    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n   147→        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n   148→        async def wrapper(request_iterator, context: ServicerContext):\n   149→            # For bi-di streaming, return the response iterator directly\n   150→            try:\n   151→                return behavior(request_iterator, context)\n   152→            finally:\n   153→                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n   154→        return wrapper\n   155→    \n   156→    async def _handle_request(\n   157→        self, \n   158→        behavior: Callable, \n   159→        request: Any, \n   160→        context: ServicerContext, \n   161→        method_name: str,\n   162→        is_streaming: bool = False\n   163→    ) -> Any:\n   164→        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n   165→        \n   166→        # Generate correlation ID\n   167→        correlation_id = str(uuid4())\n   168→        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n   169→        try:\n   170→            context.set_trailing_metadata([\n   171→                ('correlation_id', correlation_id),\n   172→                ('request_id', correlation_id)\n   173→            ])\n   174→        except Exception:\n   175→            pass\n   176→        \n   177→        start_time = time.time()\n   178→        \n   179→        # Update metrics\n   180→        self._metrics['total_requests'] += 1\n   181→        if method_name not in self._metrics['method_stats']:\n   182→            self._metrics['method_stats'][method_name] = {\n   183→                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n   184→            }\n   185→        self._metrics['method_stats'][method_name]['count'] += 1\n   186→        \n   187→        try:\n   188→            # Log request\n   189→            if self.log_requests:\n   190→                self._log_request(method_name, request, correlation_id, context)\n   191→            \n   192→            # Execute the actual method\n   193→            response = await behavior(request, context)\n   194→            \n   195→            # Calculate duration\n   196→            duration = time.time() - start_time\n   197→            \n   198→            # Update success metrics\n   199→            self._metrics['successful_requests'] += 1\n   200→            self._metrics['method_stats'][method_name]['success'] += 1\n   201→            self._update_timing_metrics(method_name, duration)\n   202→            \n   203→            # Log response\n   204→            if self.log_responses:\n   205→                self._log_response(method_name, response, correlation_id, duration, context)\n   206→            \n   207→            # Log performance\n   208→            if self.log_performance:\n   209→                self._log_performance(method_name, duration, correlation_id)\n   210→            \n   211→            return response\n   212→            \n   213→        except Exception as e:\n   214→            # Calculate duration\n   215→            duration = time.time() - start_time\n   216→            \n   217→            # Update error metrics\n   218→            self._metrics['failed_requests'] += 1\n   219→            self._metrics['method_stats'][method_name]['error'] += 1\n   220→            self._update_timing_metrics(method_name, duration)\n   221→            \n   222→            # Log error\n   223→            if self.log_errors:\n   224→                self._log_error(method_name, e, correlation_id, duration, context)\n   225→            \n   226→            # Re-raise the exception\n   227→            raise\n   228→    \n   229→    async def _handle_streaming_request(\n   230→        self,\n   231→        behavior: Callable,\n   232→        request_data: Any,\n   233→        context: ServicerContext,\n   234→        method_name: str,\n   235→        request_streaming: bool = False,\n   236→        response_streaming: bool = True\n   237→    ) -> Any:\n   238→        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n   239→        # For Phase 3, we're focusing on request/response patterns\n   240→        # Streaming would be implemented in a future phase\n   241→        correlation_id = str(uuid4())\n   242→        # Store correlation ID in invocation metadata instead of setting attributes\n   243→        try:\n   244→            context.set_trailing_metadata([\n   245→                ('correlation_id', correlation_id),\n   246→                ('request_id', correlation_id)\n   247→            ])\n   248→        except Exception:\n   249→            # If metadata setting fails, continue without it\n   250→            pass\n   251→        \n   252→        self.logger.info(\n   253→            f\"[{correlation_id}] Streaming method {method_name} called \"\n   254→            f\"(streaming implementation pending)\"\n   255→        )\n   256→        \n   257→        return behavior(request_data, context)\n   258→    \n   259→    def _log_request(\n   260→        self, \n   261→        method_name: str, \n   262→        request: Any, \n   263→        correlation_id: str, \n   264→        context: ServicerContext\n   265→    ):\n   266→        \"\"\"Log incoming request with structured data.\"\"\"\n   267→        \n   268→        # Extract client information\n   269→        peer = context.peer() if hasattr(context, 'peer') else 'unknown'\n   270→        \n   271→        # Sanitize request data\n   272→        request_data = self._sanitize_data(self._extract_request_data(request))\n   273→        \n   274→        log_data = {\n   275→            'event': 'grpc_request',\n   276→            'correlation_id': correlation_id,\n   277→            'method': method_name,\n   278→            'peer': peer,\n   279→            'request_data': request_data,\n   280→            'timestamp': time.time()\n   281→        }\n   282→        \n   283→        self.logger.log(\n   284→            self.log_level,\n   285→            f\"[{correlation_id}] gRPC Request: {method_name}\",\n   286→            extra={'structured_data': log_data}\n   287→        )\n   288→    \n   289→    def _log_response(\n   290→        self, \n   291→        method_name: str, \n   292→        response: Any, \n   293→        correlation_id: str, \n   294→        duration: float,\n   295→        context: ServicerContext\n   296→    ):\n   297→        \"\"\"Log outgoing response with structured data.\"\"\"\n   298→        \n   299→        # Sanitize response data\n   300→        response_data = self._sanitize_data(self._extract_response_data(response))\n   301→        \n   302→        log_data = {\n   303→            'event': 'grpc_response',\n   304→            'correlation_id': correlation_id,\n   305→            'method': method_name,\n   306→            'duration_ms': round(duration * 1000, 2),\n   307→            'response_data': response_data,\n   308→            'timestamp': time.time()\n   309→        }\n   310→        \n   311→        self.logger.log(\n   312→            self.log_level,\n   313→            f\"[{correlation_id}] gRPC Response: {method_name} ({duration*1000:.2f}ms)\",\n   314→            extra={'structured_data': log_data}\n   315→        )\n   316→    \n   317→    def _log_error(\n   318→        self, \n   319→        method_name: str, \n   320→        error: Exception, \n   321→        correlation_id: str, \n   322→        duration: float,\n   323→        context: ServicerContext\n   324→    ):\n   325→        \"\"\"Log error with comprehensive context.\"\"\"\n   326→        \n   327→        log_data = {\n   328→            'event': 'grpc_error',\n   329→            'correlation_id': correlation_id,\n   330→            'method': method_name,\n   331→            'error_type': type(error).__name__,\n   332→            'error_message': str(error),\n   333→            'duration_ms': round(duration * 1000, 2),\n   334→            'timestamp': time.time()\n   335→        }\n   336→        \n   337→        self.logger.error(\n   338→            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n   339→            extra={'structured_data': log_data},\n   340→            exc_info=True\n   341→        )\n   342→    \n   343→    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n   344→        \"\"\"Log performance metrics.\"\"\"\n   345→        \n   346→        if duration > 1.0:  # Log slow requests (>1 second)\n   347→            self.logger.warning(\n   348→                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n   349→            )\n   350→    \n   351→    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n   352→        \"\"\"Extract relevant data from request for logging.\"\"\"\n   353→        try:\n   354→            if hasattr(request, '__dict__'):\n   355→                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n   356→            elif hasattr(request, 'DESCRIPTOR'):\n   357→                # Protobuf message\n   358→                result = {}\n   359→                for field in request.DESCRIPTOR.fields:\n   360→                    if request.HasField(field.name):\n   361→                        result[field.name] = getattr(request, field.name)\n   362→                return result\n   363→            else:\n   364→                return {'value': str(request)}\n   365→        except Exception:\n   366→            return {'error': 'Failed to extract request data'}\n   367→    \n   368→    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n   369→        \"\"\"Extract relevant data from response for logging.\"\"\"\n   370→        try:\n   371→            if hasattr(response, '__dict__'):\n   372→                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n   373→            elif hasattr(response, 'DESCRIPTOR'):\n   374→                # Protobuf message\n   375→                result = {}\n   376→                for field in response.DESCRIPTOR.fields:\n   377→                    if response.HasField(field.name):\n   378→                        result[field.name] = getattr(response, field.name)\n   379→                return result\n   380→            else:\n   381→                return {'value': str(response)}\n   382→        except Exception:\n   383→            return {'error': 'Failed to extract response data'}\n   384→    \n   385→    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n   386→        \"\"\"Remove sensitive information from logged data.\"\"\"\n   387→        if not isinstance(data, dict):\n   388→            return data\n   389→        \n   390→        sanitized = {}\n   391→        for key, value in data.items():\n   392→            key_lower = key.lower()\n   393→            \n   394→            # Check if field contains sensitive information\n   395→            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n   396→                sanitized[key] = '***REDACTED***'\n   397→            elif isinstance(value, dict):\n   398→                sanitized[key] = self._sanitize_data(value)\n   399→            elif isinstance(value, (list, tuple)):\n   400→                sanitized[key] = [\n   401→                    self._sanitize_data(item) if isinstance(item, dict) else item\n   402→                    for item in value\n   403→                ]\n   404→            else:\n   405→                sanitized[key] = value\n   406→        \n   407→        return sanitized\n   408→    \n   409→    def _update_timing_metrics(self, method_name: str, duration: float):\n   410→        \"\"\"Update timing metrics for performance tracking.\"\"\"\n   411→        method_stats = self._metrics['method_stats'][method_name]\n   412→        current_avg = method_stats['avg_time']\n   413→        count = method_stats['count']\n   414→        \n   415→        # Update method-specific average\n   416→        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n   417→        \n   418→        # Update overall average\n   419→        total_requests = self._metrics['total_requests']\n   420→        current_overall_avg = self._metrics['avg_response_time']\n   421→        self._metrics['avg_response_time'] = (\n   422→            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n   423→        )\n   424→    \n   425→    def get_metrics(self) -> Dict[str, Any]:\n   426→        \"\"\"Get current performance metrics.\"\"\"\n   427→        return {\n   428→            'interceptor_name': 'LoggingInterceptor',\n   429→            'metrics': self._metrics.copy(),\n   430→            'timestamp': time.time()\n   431→        }\n   432→    \n   433→    def reset_metrics(self):\n   434→        \"\"\"Reset performance metrics.\"\"\"\n   435→        self._metrics = {\n   436→            'total_requests': 0,\n   437→            'successful_requests': 0,\n   438→            'failed_requests': 0,\n   439→            'avg_response_time': 0.0,\n   440→            'method_stats': {}\n   441→        }\n   442→\n   443→\n   444→# Convenience function for creating configured logging interceptor\n   445→def create_logging_interceptor(\n   446→    logger_name: str = \"grpc.logging\",\n   447→    log_level: int = logging.INFO,\n   448→    **kwargs\n   449→) -> LoggingInterceptor:\n   450→    \"\"\"Create a pre-configured logging interceptor.\n   451→    \n   452→    Args:\n   453→        logger_name: Name for the logger\n   454→        log_level: Logging level\n   455→        **kwargs: Additional configuration options\n   456→        \n   457→    Returns:\n   458→        Configured LoggingInterceptor instance\n   459→    \"\"\"\n   460→    logger = logging.getLogger(logger_name)\n   461→    return LoggingInterceptor(\n   462→        logger=logger,\n   463→        log_level=log_level,\n   464→        **kwargs\n   465→    )\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"c5f34df5-7932-48be-af46-c1ca864d0d70","timestamp":"2025-08-20T01:57:55.210Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py","content":"\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n\nThis interceptor provides:\n- Detailed request/response logging\n- Performance metrics tracking\n- Error logging and analysis\n- Request correlation and tracing\n- Structured logging with context\n\"\"\"\nimport json\nimport logging\nimport time\nfrom typing import Any, Callable, Iterator, Optional, Dict\nfrom uuid import uuid4\n\nimport grpc\nimport grpc.aio\nfrom grpc.aio import ServerInterceptor, ServicerContext\n\n\nclass LoggingInterceptor(grpc.aio.ServerInterceptor):\n    \"\"\"Advanced logging interceptor for gRPC services.\n    \n    Features:\n    - Request/response logging with structured data\n    - Performance metrics and timing\n    - Error tracking and analysis\n    - Request correlation IDs\n    - Configurable log levels and filters\n    - Security-aware logging (excludes sensitive data)\n    \"\"\"\n    \n    def __init__(\n        self,\n        logger: Optional[logging.Logger] = None,\n        log_level: int = logging.INFO,\n        log_requests: bool = True,\n        log_responses: bool = True,\n        log_errors: bool = True,\n        log_performance: bool = True,\n        exclude_methods: Optional[list] = None,\n        sensitive_fields: Optional[list] = None\n    ):\n        self.logger = logger or logging.getLogger(__name__)\n        self.log_level = log_level\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_errors = log_errors\n        self.log_performance = log_performance\n        self.exclude_methods = exclude_methods or []\n        self.sensitive_fields = sensitive_fields or [\n            'password', 'token', 'secret', 'key', 'credential', 'auth'\n        ]\n        \n        # Performance metrics\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n    \n    async def intercept_service(\n        self,\n        continuation: Callable[[Any], Any],\n        handler_call_details: Any,\n    ) -> Any:\n        \"\"\"Intercept service calls for logging.\"\"\"\n        \n        # Skip excluded methods\n        method_name = handler_call_details.method\n        if method_name in self.exclude_methods:\n            return await continuation(handler_call_details)\n        \n        # Get the original handler\n        handler = await continuation(handler_call_details)\n        if handler is None:\n            return None\n        \n        # Wrap the handler based on its type; bypass reflection and health entirely\n        if method_name in {\n            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n            '/grpc.health.v1.Health/Check',\n            '/grpc.health.v1.Health/Watch',\n        }:\n            return handler\n        if handler.unary_unary:\n            return grpc.unary_unary_rpc_method_handler(\n                self._wrap_unary_unary(handler.unary_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.unary_stream:\n            return grpc.unary_stream_rpc_method_handler(\n                self._wrap_unary_stream(handler.unary_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_unary:\n            return grpc.stream_unary_rpc_method_handler(\n                self._wrap_stream_unary(handler.stream_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_stream:\n            return grpc.stream_stream_rpc_method_handler(\n                self._wrap_stream_stream(handler.stream_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        else:\n            return handler\n    \n    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-unary RPC method.\"\"\"\n        \n        async def wrapper(request: Any, context: ServicerContext) -> Any:\n            return await self._handle_request(\n                behavior, request, context, method_name, is_streaming=False\n            )\n        \n        return wrapper\n    \n    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n        async def wrapper(request: Any, context: ServicerContext):\n            # For streaming responses, return the iterator directly (do not await)\n            try:\n                return behavior(request, context)\n            finally:\n                # Minimal logging to avoid interfering with stream lifecycle\n                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-unary RPC method.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n            # Request is streaming, response is unary; await the final response\n            try:\n                return await behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext):\n            # For bi-di streaming, return the response iterator directly\n            try:\n                return behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n        return wrapper\n    \n    async def _handle_request(\n        self, \n        behavior: Callable, \n        request: Any, \n        context: ServicerContext, \n        method_name: str,\n        is_streaming: bool = False\n    ) -> Any:\n        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n        \n        # Generate correlation ID\n        correlation_id = str(uuid4())\n        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            pass\n        \n        start_time = time.time()\n        \n        # Update metrics\n        self._metrics['total_requests'] += 1\n        if method_name not in self._metrics['method_stats']:\n            self._metrics['method_stats'][method_name] = {\n                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n            }\n        self._metrics['method_stats'][method_name]['count'] += 1\n        \n        try:\n            # Log request\n            if self.log_requests:\n                self._log_request(method_name, request, correlation_id, context)\n            \n            # Execute the actual method\n            response = await behavior(request, context)\n            \n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update success metrics\n            self._metrics['successful_requests'] += 1\n            self._metrics['method_stats'][method_name]['success'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log response\n            if self.log_responses:\n                self._log_response(method_name, response, correlation_id, duration, context)\n            \n            # Log performance\n            if self.log_performance:\n                self._log_performance(method_name, duration, correlation_id)\n            \n            return response\n            \n        except Exception as e:\n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update error metrics\n            self._metrics['failed_requests'] += 1\n            self._metrics['method_stats'][method_name]['error'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log error\n            if self.log_errors:\n                self._log_error(method_name, e, correlation_id, duration, context)\n            \n            # Re-raise the exception\n            raise\n    \n    async def _handle_streaming_request(\n        self,\n        behavior: Callable,\n        request_data: Any,\n        context: ServicerContext,\n        method_name: str,\n        request_streaming: bool = False,\n        response_streaming: bool = True\n    ) -> Any:\n        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n        # For Phase 3, we're focusing on request/response patterns\n        # Streaming would be implemented in a future phase\n        correlation_id = str(uuid4())\n        # Store correlation ID in invocation metadata instead of setting attributes\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            # If metadata setting fails, continue without it\n            pass\n        \n        self.logger.info(\n            f\"[{correlation_id}] Streaming method {method_name} called \"\n            f\"(streaming implementation pending)\"\n        )\n        \n        return behavior(request_data, context)\n    \n    def _log_request(\n        self, \n        method_name: str, \n        request: Any, \n        correlation_id: str, \n        context: ServicerContext\n    ):\n        \"\"\"Log incoming request with structured data.\"\"\"\n        \n        # Extract client information\n        peer = context.peer() if hasattr(context, 'peer') else 'unknown'\n        \n        # Sanitize request data\n        request_data = self._sanitize_data(self._extract_request_data(request))\n        \n        log_data = {\n            'event': 'grpc_request',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'peer': peer,\n            'request_data': request_data,\n            'timestamp': time.time()\n        }\n        \n        self.logger.log(\n            self.log_level,\n            f\"[{correlation_id}] gRPC Request: {method_name}\",\n            extra={'structured_data': log_data}\n        )\n    \n    def _log_response(\n        self, \n        method_name: str, \n        response: Any, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log outgoing response with structured data.\"\"\"\n        \n        # Sanitize response data\n        response_data = self._sanitize_data(self._extract_response_data(response))\n        \n        log_data = {\n            'event': 'grpc_response',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'duration_ms': round(duration * 1000, 2),\n            'response_data': response_data,\n            'timestamp': time.time()\n        }\n        \n        self.logger.log(\n            self.log_level,\n            f\"[{correlation_id}] gRPC Response: {method_name} ({duration*1000:.2f}ms)\",\n            extra={'structured_data': log_data}\n        )\n    \n    def _log_error(\n        self, \n        method_name: str, \n        error: Exception, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log error with comprehensive context.\"\"\"\n        \n        log_data = {\n            'event': 'grpc_error',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'error_type': type(error).__name__,\n            'error_message': str(error),\n            'duration_ms': round(duration * 1000, 2),\n            'timestamp': time.time()\n        }\n        \n        self.logger.error(\n            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n            extra={'structured_data': log_data},\n            exc_info=True\n        )\n    \n    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n        \"\"\"Log performance metrics.\"\"\"\n        \n        if duration > 1.0:  # Log slow requests (>1 second)\n            self.logger.warning(\n                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n            )\n    \n    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            elif hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    if request.HasField(field.name):\n                        result[field.name] = getattr(request, field.name)\n                return result\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}\n    \n    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            elif hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    if response.HasField(field.name):\n                        result[field.name] = getattr(response, field.name)\n                return result\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}\n    \n    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove sensitive information from logged data.\"\"\"\n        if not isinstance(data, dict):\n            return data\n        \n        sanitized = {}\n        for key, value in data.items():\n            key_lower = key.lower()\n            \n            # Check if field contains sensitive information\n            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n                sanitized[key] = '***REDACTED***'\n            elif isinstance(value, dict):\n                sanitized[key] = self._sanitize_data(value)\n            elif isinstance(value, (list, tuple)):\n                sanitized[key] = [\n                    self._sanitize_data(item) if isinstance(item, dict) else item\n                    for item in value\n                ]\n            else:\n                sanitized[key] = value\n        \n        return sanitized\n    \n    def _update_timing_metrics(self, method_name: str, duration: float):\n        \"\"\"Update timing metrics for performance tracking.\"\"\"\n        method_stats = self._metrics['method_stats'][method_name]\n        current_avg = method_stats['avg_time']\n        count = method_stats['count']\n        \n        # Update method-specific average\n        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n        \n        # Update overall average\n        total_requests = self._metrics['total_requests']\n        current_overall_avg = self._metrics['avg_response_time']\n        self._metrics['avg_response_time'] = (\n            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n        )\n    \n    def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get current performance metrics.\"\"\"\n        return {\n            'interceptor_name': 'LoggingInterceptor',\n            'metrics': self._metrics.copy(),\n            'timestamp': time.time()\n        }\n    \n    def reset_metrics(self):\n        \"\"\"Reset performance metrics.\"\"\"\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n\n\n# Convenience function for creating configured logging interceptor\ndef create_logging_interceptor(\n    logger_name: str = \"grpc.logging\",\n    log_level: int = logging.INFO,\n    **kwargs\n) -> LoggingInterceptor:\n    \"\"\"Create a pre-configured logging interceptor.\n    \n    Args:\n        logger_name: Name for the logger\n        log_level: Logging level\n        **kwargs: Additional configuration options\n        \n    Returns:\n        Configured LoggingInterceptor instance\n    \"\"\"\n    logger = logging.getLogger(logger_name)\n    return LoggingInterceptor(\n        logger=logger,\n        log_level=log_level,\n        **kwargs\n    )","numLines":465,"startLine":1,"totalLines":465}}}
{"parentUuid":"c5f34df5-7932-48be-af46-c1ca864d0d70","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_018uUvAnGSCCWuzzD2AMEnLK","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Tôi thấy có 2 logging interceptor khác nhau. Một trong `src/infrastructure/grpc/interceptors/` (mới được cải thiện) và một trong `src/presentation/grpc/interceptors/` (hiện tại đang được sử dụng). Tôi cần cập nhật interceptor trong presentation layer để sử dụng cải tiến từ infrastructure layer:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":5989,"cache_read_input_tokens":55830,"cache_creation":{"ephemeral_5m_input_tokens":5989,"ephemeral_1h_input_tokens":0},"output_tokens":5,"service_tier":"standard"}},"requestId":"req_011CSJ8fQ5mUV3id1VqB73Jk","type":"assistant","uuid":"b243c547-89f9-482d-8331-a9b713498a06","timestamp":"2025-08-20T01:58:06.438Z"}
{"parentUuid":"b243c547-89f9-482d-8331-a9b713498a06","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_018uUvAnGSCCWuzzD2AMEnLK","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01DV5YipLriZLyyszU9Fw4vH","name":"Edit","input":{"file_path":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py","old_string":"    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            elif hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    if request.HasField(field.name):\n                        result[field.name] = getattr(request, field.name)\n                return result\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}","new_string":"    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(request, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_request_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}\n    \n    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a value for logging.\"\"\"\n        try:\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                import base64\n                encoded = base64.b64encode(value).decode()\n                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n            elif hasattr(value, 'isoformat'):\n                return value.isoformat()\n            elif hasattr(value, 'name'):  # Enum value\n                return value.name\n            else:\n                str_value = str(value)\n                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n        except Exception:\n            return \"<non-serializable>\""}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":5989,"cache_read_input_tokens":55830,"cache_creation":{"ephemeral_5m_input_tokens":5989,"ephemeral_1h_input_tokens":0},"output_tokens":998,"service_tier":"standard"}},"requestId":"req_011CSJ8fQ5mUV3id1VqB73Jk","type":"assistant","uuid":"f875085d-fcba-42d6-83c4-e2603bc54d25","timestamp":"2025-08-20T01:58:14.690Z"}
{"parentUuid":"f875085d-fcba-42d6-83c4-e2603bc54d25","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01DV5YipLriZLyyszU9Fw4vH","type":"tool_result","content":"The file /Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n     1→\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n     2→\n     3→This interceptor provides:\n     4→- Detailed request/response logging\n     5→- Performance metrics tracking\n     6→- Error logging and analysis\n     7→- Request correlation and tracing\n     8→- Structured logging with context\n     9→\"\"\"\n    10→import json\n    11→import logging\n    12→import time\n    13→from typing import Any, Callable, Iterator, Optional, Dict\n    14→from uuid import uuid4\n    15→\n    16→import grpc\n    17→import grpc.aio\n    18→from grpc.aio import ServerInterceptor, ServicerContext\n    19→\n    20→\n    21→class LoggingInterceptor(grpc.aio.ServerInterceptor):\n    22→    \"\"\"Advanced logging interceptor for gRPC services.\n    23→    \n    24→    Features:\n    25→    - Request/response logging with structured data\n    26→    - Performance metrics and timing\n    27→    - Error tracking and analysis\n    28→    - Request correlation IDs\n    29→    - Configurable log levels and filters\n    30→    - Security-aware logging (excludes sensitive data)\n    31→    \"\"\"\n    32→    \n    33→    def __init__(\n    34→        self,\n    35→        logger: Optional[logging.Logger] = None,\n    36→        log_level: int = logging.INFO,\n    37→        log_requests: bool = True,\n    38→        log_responses: bool = True,\n    39→        log_errors: bool = True,\n    40→        log_performance: bool = True,\n    41→        exclude_methods: Optional[list] = None,\n    42→        sensitive_fields: Optional[list] = None\n    43→    ):\n    44→        self.logger = logger or logging.getLogger(__name__)\n    45→        self.log_level = log_level\n    46→        self.log_requests = log_requests\n    47→        self.log_responses = log_responses\n    48→        self.log_errors = log_errors\n    49→        self.log_performance = log_performance\n    50→        self.exclude_methods = exclude_methods or []\n    51→        self.sensitive_fields = sensitive_fields or [\n    52→            'password', 'token', 'secret', 'key', 'credential', 'auth'\n    53→        ]\n    54→        \n    55→        # Performance metrics\n    56→        self._metrics = {\n    57→            'total_requests': 0,\n    58→            'successful_requests': 0,\n    59→            'failed_requests': 0,\n    60→            'avg_response_time': 0.0,\n    61→            'method_stats': {}\n    62→        }\n    63→    \n    64→    async def intercept_service(\n    65→        self,\n    66→        continuation: Callable[[Any], Any],\n    67→        handler_call_details: Any,\n    68→    ) -> Any:\n    69→        \"\"\"Intercept service calls for logging.\"\"\"\n    70→        \n    71→        # Skip excluded methods\n    72→        method_name = handler_call_details.method\n    73→        if method_name in self.exclude_methods:\n    74→            return await continuation(handler_call_details)\n    75→        \n    76→        # Get the original handler\n    77→        handler = await continuation(handler_call_details)\n    78→        if handler is None:\n    79→            return None\n    80→        \n    81→        # Wrap the handler based on its type; bypass reflection and health entirely\n    82→        if method_name in {\n    83→            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n    84→            '/grpc.health.v1.Health/Check',\n    85→            '/grpc.health.v1.Health/Watch',\n    86→        }:\n    87→            return handler\n    88→        if handler.unary_unary:\n    89→            return grpc.unary_unary_rpc_method_handler(\n    90→                self._wrap_unary_unary(handler.unary_unary, method_name),\n    91→                request_deserializer=handler.request_deserializer,\n    92→                response_serializer=handler.response_serializer,\n    93→            )\n    94→        elif handler.unary_stream:\n    95→            return grpc.unary_stream_rpc_method_handler(\n    96→                self._wrap_unary_stream(handler.unary_stream, method_name),\n    97→                request_deserializer=handler.request_deserializer,\n    98→                response_serializer=handler.response_serializer,\n    99→            )\n   100→        elif handler.stream_unary:\n   101→            return grpc.stream_unary_rpc_method_handler(\n   102→                self._wrap_stream_unary(handler.stream_unary, method_name),\n   103→                request_deserializer=handler.request_deserializer,\n   104→                response_serializer=handler.response_serializer,\n   105→            )\n   106→        elif handler.stream_stream:\n   107→            return grpc.stream_stream_rpc_method_handler(\n   108→                self._wrap_stream_stream(handler.stream_stream, method_name),\n   109→                request_deserializer=handler.request_deserializer,\n   110→                response_serializer=handler.response_serializer,\n   111→            )\n   112→        else:\n   113→            return handler\n   114→    \n   115→    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n   116→        \"\"\"Wrap unary-unary RPC method.\"\"\"\n   117→        \n   118→        async def wrapper(request: Any, context: ServicerContext) -> Any:\n   119→            return await self._handle_request(\n   120→                behavior, request, context, method_name, is_streaming=False\n   121→            )\n   122→        \n   123→        return wrapper\n   124→    \n   125→    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n   126→        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n   127→        async def wrapper(request: Any, context: ServicerContext):\n   128→            # For streaming responses, return the iterator directly (do not await)\n   129→            try:\n   130→                return behavior(request, context)\n   131→            finally:\n   132→                # Minimal logging to avoid interfering with stream lifecycle\n   133→                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n   134→        return wrapper\n   135→    \n   136→    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n   137→        \"\"\"Wrap stream-unary RPC method.\"\"\"\n   138→        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n   139→            # Request is streaming, response is unary; await the final response\n   140→            try:\n   141→                return await behavior(request_iterator, context)\n   142→            finally:\n   143→                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n   144→        return wrapper\n   145→    \n   146→    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n   147→        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n   148→        async def wrapper(request_iterator, context: ServicerContext):\n   149→            # For bi-di streaming, return the response iterator directly\n   150→            try:\n   151→                return behavior(request_iterator, context)\n   152→            finally:\n   153→                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n   154→        return wrapper\n   155→    \n   156→    async def _handle_request(\n   157→        self, \n   158→        behavior: Callable, \n   159→        request: Any, \n   160→        context: ServicerContext, \n   161→        method_name: str,\n   162→        is_streaming: bool = False\n   163→    ) -> Any:\n   164→        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n   165→        \n   166→        # Generate correlation ID\n   167→        correlation_id = str(uuid4())\n   168→        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n   169→        try:\n   170→            context.set_trailing_metadata([\n   171→                ('correlation_id', correlation_id),\n   172→                ('request_id', correlation_id)\n   173→            ])\n   174→        except Exception:\n   175→            pass\n   176→        \n   177→        start_time = time.time()\n   178→        \n   179→        # Update metrics\n   180→        self._metrics['total_requests'] += 1\n   181→        if method_name not in self._metrics['method_stats']:\n   182→            self._metrics['method_stats'][method_name] = {\n   183→                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n   184→            }\n   185→        self._metrics['method_stats'][method_name]['count'] += 1\n   186→        \n   187→        try:\n   188→            # Log request\n   189→            if self.log_requests:\n   190→                self._log_request(method_name, request, correlation_id, context)\n   191→            \n   192→            # Execute the actual method\n   193→            response = await behavior(request, context)\n   194→            \n   195→            # Calculate duration\n   196→            duration = time.time() - start_time\n   197→            \n   198→            # Update success metrics\n   199→            self._metrics['successful_requests'] += 1\n   200→            self._metrics['method_stats'][method_name]['success'] += 1\n   201→            self._update_timing_metrics(method_name, duration)\n   202→            \n   203→            # Log response\n   204→            if self.log_responses:\n   205→                self._log_response(method_name, response, correlation_id, duration, context)\n   206→            \n   207→            # Log performance\n   208→            if self.log_performance:\n   209→                self._log_performance(method_name, duration, correlation_id)\n   210→            \n   211→            return response\n   212→            \n   213→        except Exception as e:\n   214→            # Calculate duration\n   215→            duration = time.time() - start_time\n   216→            \n   217→            # Update error metrics\n   218→            self._metrics['failed_requests'] += 1\n   219→            self._metrics['method_stats'][method_name]['error'] += 1\n   220→            self._update_timing_metrics(method_name, duration)\n   221→            \n   222→            # Log error\n   223→            if self.log_errors:\n   224→                self._log_error(method_name, e, correlation_id, duration, context)\n   225→            \n   226→            # Re-raise the exception\n   227→            raise\n   228→    \n   229→    async def _handle_streaming_request(\n   230→        self,\n   231→        behavior: Callable,\n   232→        request_data: Any,\n   233→        context: ServicerContext,\n   234→        method_name: str,\n   235→        request_streaming: bool = False,\n   236→        response_streaming: bool = True\n   237→    ) -> Any:\n   238→        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n   239→        # For Phase 3, we're focusing on request/response patterns\n   240→        # Streaming would be implemented in a future phase\n   241→        correlation_id = str(uuid4())\n   242→        # Store correlation ID in invocation metadata instead of setting attributes\n   243→        try:\n   244→            context.set_trailing_metadata([\n   245→                ('correlation_id', correlation_id),\n   246→                ('request_id', correlation_id)\n   247→            ])\n   248→        except Exception:\n   249→            # If metadata setting fails, continue without it\n   250→            pass\n   251→        \n   252→        self.logger.info(\n   253→            f\"[{correlation_id}] Streaming method {method_name} called \"\n   254→            f\"(streaming implementation pending)\"\n   255→        )\n   256→        \n   257→        return behavior(request_data, context)\n   258→    \n   259→    def _log_request(\n   260→        self, \n   261→        method_name: str, \n   262→        request: Any, \n   263→        correlation_id: str, \n   264→        context: ServicerContext\n   265→    ):\n   266→        \"\"\"Log incoming request with structured data.\"\"\"\n   267→        \n   268→        # Extract client information\n   269→        peer = context.peer() if hasattr(context, 'peer') else 'unknown'\n   270→        \n   271→        # Sanitize request data\n   272→        request_data = self._sanitize_data(self._extract_request_data(request))\n   273→        \n   274→        log_data = {\n   275→            'event': 'grpc_request',\n   276→            'correlation_id': correlation_id,\n   277→            'method': method_name,\n   278→            'peer': peer,\n   279→            'request_data': request_data,\n   280→            'timestamp': time.time()\n   281→        }\n   282→        \n   283→        self.logger.log(\n   284→            self.log_level,\n   285→            f\"[{correlation_id}] gRPC Request: {method_name}\",\n   286→            extra={'structured_data': log_data}\n   287→        )\n   288→    \n   289→    def _log_response(\n   290→        self, \n   291→        method_name: str, \n   292→        response: Any, \n   293→        correlation_id: str, \n   294→        duration: float,\n   295→        context: ServicerContext\n   296→    ):\n   297→        \"\"\"Log outgoing response with structured data.\"\"\"\n   298→        \n   299→        # Sanitize response data\n   300→        response_data = self._sanitize_data(self._extract_response_data(response))\n   301→        \n   302→        log_data = {\n   303→            'event': 'grpc_response',\n   304→            'correlation_id': correlation_id,\n   305→            'method': method_name,\n   306→            'duration_ms': round(duration * 1000, 2),\n   307→            'response_data': response_data,\n   308→            'timestamp': time.time()\n   309→        }\n   310→        \n   311→        self.logger.log(\n   312→            self.log_level,\n   313→            f\"[{correlation_id}] gRPC Response: {method_name} ({duration*1000:.2f}ms)\",\n   314→            extra={'structured_data': log_data}\n   315→        )\n   316→    \n   317→    def _log_error(\n   318→        self, \n   319→        method_name: str, \n   320→        error: Exception, \n   321→        correlation_id: str, \n   322→        duration: float,\n   323→        context: ServicerContext\n   324→    ):\n   325→        \"\"\"Log error with comprehensive context.\"\"\"\n   326→        \n   327→        log_data = {\n   328→            'event': 'grpc_error',\n   329→            'correlation_id': correlation_id,\n   330→            'method': method_name,\n   331→            'error_type': type(error).__name__,\n   332→            'error_message': str(error),\n   333→            'duration_ms': round(duration * 1000, 2),\n   334→            'timestamp': time.time()\n   335→        }\n   336→        \n   337→        self.logger.error(\n   338→            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n   339→            extra={'structured_data': log_data},\n   340→            exc_info=True\n   341→        )\n   342→    \n   343→    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n   344→        \"\"\"Log performance metrics.\"\"\"\n   345→        \n   346→        if duration > 1.0:  # Log slow requests (>1 second)\n   347→            self.logger.warning(\n   348→                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n   349→            )\n   350→    \n   351→    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n   352→        \"\"\"Extract relevant data from request for logging.\"\"\"\n   353→        try:\n   354→            if hasattr(request, 'DESCRIPTOR'):\n   355→                # Protobuf message - extract all fields including defaults\n   356→                result = {}\n   357→                for field in request.DESCRIPTOR.fields:\n   358→                    field_name = field.name\n   359→                    try:\n   360→                        value = getattr(request, field_name, None)\n   361→                        if value is not None:\n   362→                            if hasattr(value, 'DESCRIPTOR'):\n   363→                                # Nested protobuf message\n   364→                                result[field_name] = self._extract_request_data(value)\n   365→                            elif field.label == field.LABEL_REPEATED:\n   366→                                # Repeated field (list)\n   367→                                result[field_name] = [\n   368→                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n   369→                                    for item in value\n   370→                                ]\n   371→                            else:\n   372→                                result[field_name] = self._serialize_value(value)\n   373→                    except Exception as e:\n   374→                        result[field_name] = f\"<error: {str(e)}>\"\n   375→                return result\n   376→            elif hasattr(request, '__dict__'):\n   377→                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n   378→            else:\n   379→                return {'value': str(request)}\n   380→        except Exception:\n   381→            return {'error': 'Failed to extract request data'}\n   382→    \n   383→    def _serialize_value(self, value: Any) -> Any:\n   384→        \"\"\"Serialize a value for logging.\"\"\"\n   385→        try:\n   386→            if isinstance(value, (str, int, float, bool)):\n   387→                return value\n   388→            elif isinstance(value, bytes):\n   389→                import base64\n   390→                encoded = base64.b64encode(value).decode()\n   391→                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n   392→            elif hasattr(value, 'isoformat'):\n   393→                return value.isoformat()\n   394→            elif hasattr(value, 'name'):  # Enum value\n   395→                return value.name\n   396→            else:\n   397→                str_value = str(value)\n   398→                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n   399→        except Exception:\n   400→            return \"<non-serializable>\"\n   401→    \n   402→    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n   403→        \"\"\"Extract relevant data from response for logging.\"\"\"\n   404→        try:\n   405→            if hasattr(response, '__dict__'):\n   406→                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n   407→            elif hasattr(response, 'DESCRIPTOR'):\n   408→                # Protobuf message\n   409→                result = {}\n   410→                for field in response.DESCRIPTOR.fields:\n   411→                    if response.HasField(field.name):\n   412→                        result[field.name] = getattr(response, field.name)\n   413→                return result\n   414→            else:\n   415→                return {'value': str(response)}\n   416→        except Exception:\n   417→            return {'error': 'Failed to extract response data'}\n   418→    \n   419→    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n   420→        \"\"\"Remove sensitive information from logged data.\"\"\"\n   421→        if not isinstance(data, dict):\n   422→            return data\n   423→        \n   424→        sanitized = {}\n   425→        for key, value in data.items():\n   426→            key_lower = key.lower()\n   427→            \n   428→            # Check if field contains sensitive information\n   429→            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n   430→                sanitized[key] = '***REDACTED***'\n   431→            elif isinstance(value, dict):\n   432→                sanitized[key] = self._sanitize_data(value)\n   433→            elif isinstance(value, (list, tuple)):\n   434→                sanitized[key] = [\n   435→                    self._sanitize_data(item) if isinstance(item, dict) else item\n   436→                    for item in value\n   437→                ]\n   438→            else:\n   439→                sanitized[key] = value\n   440→        \n   441→        return sanitized\n   442→    \n   443→    def _update_timing_metrics(self, method_name: str, duration: float):\n   444→        \"\"\"Update timing metrics for performance tracking.\"\"\"\n   445→        method_stats = self._metrics['method_stats'][method_name]\n   446→        current_avg = method_stats['avg_time']\n   447→        count = method_stats['count']\n   448→        \n   449→        # Update method-specific average\n   450→        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n   451→        \n   452→        # Update overall average\n   453→        total_requests = self._metrics['total_requests']\n   454→        current_overall_avg = self._metrics['avg_response_time']\n   455→        self._metrics['avg_response_time'] = (\n   456→            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n   457→        )\n   458→    \n   459→    def get_metrics(self) -> Dict[str, Any]:\n   460→        \"\"\"Get current performance metrics.\"\"\"\n   461→        return {\n   462→            'interceptor_name': 'LoggingInterceptor',\n   463→            'metrics': self._metrics.copy(),\n   464→            'timestamp': time.time()\n   465→        }\n   466→    \n   467→    def reset_metrics(self):\n   468→        \"\"\"Reset performance metrics.\"\"\"\n   469→        self._metrics = {\n   470→            'total_requests': 0,\n   471→            'successful_requests': 0,\n   472→            'failed_requests': 0,\n   473→            'avg_response_time': 0.0,\n   474→            'method_stats': {}\n   475→        }\n   476→\n   477→\n   478→# Convenience function for creating configured logging interceptor\n   479→def create_logging_interceptor(\n   480→    logger_name: str = \"grpc.logging\",\n   481→    log_level: int = logging.INFO,\n   482→    **kwargs\n   483→) -> LoggingInterceptor:\n   484→    \"\"\"Create a pre-configured logging interceptor.\n   485→    \n   486→    Args:\n   487→        logger_name: Name for the logger\n   488→        log_level: Logging level\n   489→        **kwargs: Additional configuration options\n   490→        \n   491→    Returns:\n   492→        Configured LoggingInterceptor instance\n   493→    \"\"\"\n   494→    logger = logging.getLogger(logger_name)\n   495→    return LoggingInterceptor(\n   496→        logger=logger,\n   497→        log_level=log_level,\n   498→        **kwargs\n   499→    )"}]},"uuid":"aab37eae-bd16-48c5-ab0f-91884fd4b1b6","timestamp":"2025-08-20T01:58:33.617Z","toolUseResult":{"filePath":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py","oldString":"\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n\nThis interceptor provides:\n- Detailed request/response logging\n- Performance metrics tracking\n- Error logging and analysis\n- Request correlation and tracing\n- Structured logging with context\n\"\"\"\nimport json\nimport logging\nimport time\nfrom typing import Any, Callable, Iterator, Optional, Dict\nfrom uuid import uuid4\n\nimport grpc\nimport grpc.aio\nfrom grpc.aio import ServerInterceptor, ServicerContext\n\n\nclass LoggingInterceptor(grpc.aio.ServerInterceptor):\n    \"\"\"Advanced logging interceptor for gRPC services.\n    \n    Features:\n    - Request/response logging with structured data\n    - Performance metrics and timing\n    - Error tracking and analysis\n    - Request correlation IDs\n    - Configurable log levels and filters\n    - Security-aware logging (excludes sensitive data)\n    \"\"\"\n    \n    def __init__(\n        self,\n        logger: Optional[logging.Logger] = None,\n        log_level: int = logging.INFO,\n        log_requests: bool = True,\n        log_responses: bool = True,\n        log_errors: bool = True,\n        log_performance: bool = True,\n        exclude_methods: Optional[list] = None,\n        sensitive_fields: Optional[list] = None\n    ):\n        self.logger = logger or logging.getLogger(__name__)\n        self.log_level = log_level\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_errors = log_errors\n        self.log_performance = log_performance\n        self.exclude_methods = exclude_methods or []\n        self.sensitive_fields = sensitive_fields or [\n            'password', 'token', 'secret', 'key', 'credential', 'auth'\n        ]\n        \n        # Performance metrics\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n    \n    async def intercept_service(\n        self,\n        continuation: Callable[[Any], Any],\n        handler_call_details: Any,\n    ) -> Any:\n        \"\"\"Intercept service calls for logging.\"\"\"\n        \n        # Skip excluded methods\n        method_name = handler_call_details.method\n        if method_name in self.exclude_methods:\n            return await continuation(handler_call_details)\n        \n        # Get the original handler\n        handler = await continuation(handler_call_details)\n        if handler is None:\n            return None\n        \n        # Wrap the handler based on its type; bypass reflection and health entirely\n        if method_name in {\n            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n            '/grpc.health.v1.Health/Check',\n            '/grpc.health.v1.Health/Watch',\n        }:\n            return handler\n        if handler.unary_unary:\n            return grpc.unary_unary_rpc_method_handler(\n                self._wrap_unary_unary(handler.unary_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.unary_stream:\n            return grpc.unary_stream_rpc_method_handler(\n                self._wrap_unary_stream(handler.unary_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_unary:\n            return grpc.stream_unary_rpc_method_handler(\n                self._wrap_stream_unary(handler.stream_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_stream:\n            return grpc.stream_stream_rpc_method_handler(\n                self._wrap_stream_stream(handler.stream_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        else:\n            return handler\n    \n    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-unary RPC method.\"\"\"\n        \n        async def wrapper(request: Any, context: ServicerContext) -> Any:\n            return await self._handle_request(\n                behavior, request, context, method_name, is_streaming=False\n            )\n        \n        return wrapper\n    \n    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n        async def wrapper(request: Any, context: ServicerContext):\n            # For streaming responses, return the iterator directly (do not await)\n            try:\n                return behavior(request, context)\n            finally:\n                # Minimal logging to avoid interfering with stream lifecycle\n                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-unary RPC method.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n            # Request is streaming, response is unary; await the final response\n            try:\n                return await behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext):\n            # For bi-di streaming, return the response iterator directly\n            try:\n                return behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n        return wrapper\n    \n    async def _handle_request(\n        self, \n        behavior: Callable, \n        request: Any, \n        context: ServicerContext, \n        method_name: str,\n        is_streaming: bool = False\n    ) -> Any:\n        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n        \n        # Generate correlation ID\n        correlation_id = str(uuid4())\n        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            pass\n        \n        start_time = time.time()\n        \n        # Update metrics\n        self._metrics['total_requests'] += 1\n        if method_name not in self._metrics['method_stats']:\n            self._metrics['method_stats'][method_name] = {\n                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n            }\n        self._metrics['method_stats'][method_name]['count'] += 1\n        \n        try:\n            # Log request\n            if self.log_requests:\n                self._log_request(method_name, request, correlation_id, context)\n            \n            # Execute the actual method\n            response = await behavior(request, context)\n            \n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update success metrics\n            self._metrics['successful_requests'] += 1\n            self._metrics['method_stats'][method_name]['success'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log response\n            if self.log_responses:\n                self._log_response(method_name, response, correlation_id, duration, context)\n            \n            # Log performance\n            if self.log_performance:\n                self._log_performance(method_name, duration, correlation_id)\n            \n            return response\n            \n        except Exception as e:\n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update error metrics\n            self._metrics['failed_requests'] += 1\n            self._metrics['method_stats'][method_name]['error'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log error\n            if self.log_errors:\n                self._log_error(method_name, e, correlation_id, duration, context)\n            \n            # Re-raise the exception\n            raise\n    \n    async def _handle_streaming_request(\n        self,\n        behavior: Callable,\n        request_data: Any,\n        context: ServicerContext,\n        method_name: str,\n        request_streaming: bool = False,\n        response_streaming: bool = True\n    ) -> Any:\n        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n        # For Phase 3, we're focusing on request/response patterns\n        # Streaming would be implemented in a future phase\n        correlation_id = str(uuid4())\n        # Store correlation ID in invocation metadata instead of setting attributes\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            # If metadata setting fails, continue without it\n            pass\n        \n        self.logger.info(\n            f\"[{correlation_id}] Streaming method {method_name} called \"\n            f\"(streaming implementation pending)\"\n        )\n        \n        return behavior(request_data, context)\n    \n    def _log_request(\n        self, \n        method_name: str, \n        request: Any, \n        correlation_id: str, \n        context: ServicerContext\n    ):\n        \"\"\"Log incoming request with structured data.\"\"\"\n        \n        # Extract client information\n        peer = context.peer() if hasattr(context, 'peer') else 'unknown'\n        \n        # Sanitize request data\n        request_data = self._sanitize_data(self._extract_request_data(request))\n        \n        log_data = {\n            'event': 'grpc_request',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'peer': peer,\n            'request_data': request_data,\n            'timestamp': time.time()\n        }\n        \n        self.logger.log(\n            self.log_level,\n            f\"[{correlation_id}] gRPC Request: {method_name}\",\n            extra={'structured_data': log_data}\n        )\n    \n    def _log_response(\n        self, \n        method_name: str, \n        response: Any, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log outgoing response with structured data.\"\"\"\n        \n        # Sanitize response data\n        response_data = self._sanitize_data(self._extract_response_data(response))\n        \n        log_data = {\n            'event': 'grpc_response',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'duration_ms': round(duration * 1000, 2),\n            'response_data': response_data,\n            'timestamp': time.time()\n        }\n        \n        self.logger.log(\n            self.log_level,\n            f\"[{correlation_id}] gRPC Response: {method_name} ({duration*1000:.2f}ms)\",\n            extra={'structured_data': log_data}\n        )\n    \n    def _log_error(\n        self, \n        method_name: str, \n        error: Exception, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log error with comprehensive context.\"\"\"\n        \n        log_data = {\n            'event': 'grpc_error',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'error_type': type(error).__name__,\n            'error_message': str(error),\n            'duration_ms': round(duration * 1000, 2),\n            'timestamp': time.time()\n        }\n        \n        self.logger.error(\n            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n            extra={'structured_data': log_data},\n            exc_info=True\n        )\n    \n    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n        \"\"\"Log performance metrics.\"\"\"\n        \n        if duration > 1.0:  # Log slow requests (>1 second)\n            self.logger.warning(\n                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n            )\n    \n    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            elif hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    if request.HasField(field.name):\n                        result[field.name] = getattr(request, field.name)\n                return result\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}\n    \n    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            elif hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    if response.HasField(field.name):\n                        result[field.name] = getattr(response, field.name)\n                return result\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}\n    \n    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove sensitive information from logged data.\"\"\"\n        if not isinstance(data, dict):\n            return data\n        \n        sanitized = {}\n        for key, value in data.items():\n            key_lower = key.lower()\n            \n            # Check if field contains sensitive information\n            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n                sanitized[key] = '***REDACTED***'\n            elif isinstance(value, dict):\n                sanitized[key] = self._sanitize_data(value)\n            elif isinstance(value, (list, tuple)):\n                sanitized[key] = [\n                    self._sanitize_data(item) if isinstance(item, dict) else item\n                    for item in value\n                ]\n            else:\n                sanitized[key] = value\n        \n        return sanitized\n    \n    def _update_timing_metrics(self, method_name: str, duration: float):\n        \"\"\"Update timing metrics for performance tracking.\"\"\"\n        method_stats = self._metrics['method_stats'][method_name]\n        current_avg = method_stats['avg_time']\n        count = method_stats['count']\n        \n        # Update method-specific average\n        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n        \n        # Update overall average\n        total_requests = self._metrics['total_requests']\n        current_overall_avg = self._metrics['avg_response_time']\n        self._metrics['avg_response_time'] = (\n            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n        )\n    \n    def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get current performance metrics.\"\"\"\n        return {\n            'interceptor_name': 'LoggingInterceptor',\n            'metrics': self._metrics.copy(),\n            'timestamp': time.time()\n        }\n    \n    def reset_metrics(self):\n        \"\"\"Reset performance metrics.\"\"\"\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n\n\n# Convenience function for creating configured logging interceptor\ndef create_logging_interceptor(\n    logger_name: str = \"grpc.logging\",\n    log_level: int = logging.INFO,\n    **kwargs\n) -> LoggingInterceptor:\n    \"\"\"Create a pre-configured logging interceptor.\n    \n    Args:\n        logger_name: Name for the logger\n        log_level: Logging level\n        **kwargs: Additional configuration options\n        \n    Returns:\n        Configured LoggingInterceptor instance\n    \"\"\"\n    logger = logging.getLogger(logger_name)\n    return LoggingInterceptor(\n        logger=logger,\n        log_level=log_level,\n        **kwargs\n    )","newString":"\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n\nThis interceptor provides:\n- Detailed request/response logging\n- Performance metrics tracking\n- Error logging and analysis\n- Request correlation and tracing\n- Structured logging with context\n\"\"\"\nimport json\nimport logging\nimport time\nfrom typing import Any, Callable, Iterator, Optional, Dict\nfrom uuid import uuid4\n\nimport grpc\nimport grpc.aio\nfrom grpc.aio import ServerInterceptor, ServicerContext\n\n\nclass LoggingInterceptor(grpc.aio.ServerInterceptor):\n    \"\"\"Advanced logging interceptor for gRPC services.\n    \n    Features:\n    - Request/response logging with structured data\n    - Performance metrics and timing\n    - Error tracking and analysis\n    - Request correlation IDs\n    - Configurable log levels and filters\n    - Security-aware logging (excludes sensitive data)\n    \"\"\"\n    \n    def __init__(\n        self,\n        logger: Optional[logging.Logger] = None,\n        log_level: int = logging.INFO,\n        log_requests: bool = True,\n        log_responses: bool = True,\n        log_errors: bool = True,\n        log_performance: bool = True,\n        exclude_methods: Optional[list] = None,\n        sensitive_fields: Optional[list] = None\n    ):\n        self.logger = logger or logging.getLogger(__name__)\n        self.log_level = log_level\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_errors = log_errors\n        self.log_performance = log_performance\n        self.exclude_methods = exclude_methods or []\n        self.sensitive_fields = sensitive_fields or [\n            'password', 'token', 'secret', 'key', 'credential', 'auth'\n        ]\n        \n        # Performance metrics\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n    \n    async def intercept_service(\n        self,\n        continuation: Callable[[Any], Any],\n        handler_call_details: Any,\n    ) -> Any:\n        \"\"\"Intercept service calls for logging.\"\"\"\n        \n        # Skip excluded methods\n        method_name = handler_call_details.method\n        if method_name in self.exclude_methods:\n            return await continuation(handler_call_details)\n        \n        # Get the original handler\n        handler = await continuation(handler_call_details)\n        if handler is None:\n            return None\n        \n        # Wrap the handler based on its type; bypass reflection and health entirely\n        if method_name in {\n            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n            '/grpc.health.v1.Health/Check',\n            '/grpc.health.v1.Health/Watch',\n        }:\n            return handler\n        if handler.unary_unary:\n            return grpc.unary_unary_rpc_method_handler(\n                self._wrap_unary_unary(handler.unary_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.unary_stream:\n            return grpc.unary_stream_rpc_method_handler(\n                self._wrap_unary_stream(handler.unary_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_unary:\n            return grpc.stream_unary_rpc_method_handler(\n                self._wrap_stream_unary(handler.stream_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_stream:\n            return grpc.stream_stream_rpc_method_handler(\n                self._wrap_stream_stream(handler.stream_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        else:\n            return handler\n    \n    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-unary RPC method.\"\"\"\n        \n        async def wrapper(request: Any, context: ServicerContext) -> Any:\n            return await self._handle_request(\n                behavior, request, context, method_name, is_streaming=False\n            )\n        \n        return wrapper\n    \n    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n        async def wrapper(request: Any, context: ServicerContext):\n            # For streaming responses, return the iterator directly (do not await)\n            try:\n                return behavior(request, context)\n            finally:\n                # Minimal logging to avoid interfering with stream lifecycle\n                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-unary RPC method.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n            # Request is streaming, response is unary; await the final response\n            try:\n                return await behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext):\n            # For bi-di streaming, return the response iterator directly\n            try:\n                return behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n        return wrapper\n    \n    async def _handle_request(\n        self, \n        behavior: Callable, \n        request: Any, \n        context: ServicerContext, \n        method_name: str,\n        is_streaming: bool = False\n    ) -> Any:\n        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n        \n        # Generate correlation ID\n        correlation_id = str(uuid4())\n        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            pass\n        \n        start_time = time.time()\n        \n        # Update metrics\n        self._metrics['total_requests'] += 1\n        if method_name not in self._metrics['method_stats']:\n            self._metrics['method_stats'][method_name] = {\n                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n            }\n        self._metrics['method_stats'][method_name]['count'] += 1\n        \n        try:\n            # Log request\n            if self.log_requests:\n                self._log_request(method_name, request, correlation_id, context)\n            \n            # Execute the actual method\n            response = await behavior(request, context)\n            \n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update success metrics\n            self._metrics['successful_requests'] += 1\n            self._metrics['method_stats'][method_name]['success'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log response\n            if self.log_responses:\n                self._log_response(method_name, response, correlation_id, duration, context)\n            \n            # Log performance\n            if self.log_performance:\n                self._log_performance(method_name, duration, correlation_id)\n            \n            return response\n            \n        except Exception as e:\n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update error metrics\n            self._metrics['failed_requests'] += 1\n            self._metrics['method_stats'][method_name]['error'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log error\n            if self.log_errors:\n                self._log_error(method_name, e, correlation_id, duration, context)\n            \n            # Re-raise the exception\n            raise\n    \n    async def _handle_streaming_request(\n        self,\n        behavior: Callable,\n        request_data: Any,\n        context: ServicerContext,\n        method_name: str,\n        request_streaming: bool = False,\n        response_streaming: bool = True\n    ) -> Any:\n        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n        # For Phase 3, we're focusing on request/response patterns\n        # Streaming would be implemented in a future phase\n        correlation_id = str(uuid4())\n        # Store correlation ID in invocation metadata instead of setting attributes\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            # If metadata setting fails, continue without it\n            pass\n        \n        self.logger.info(\n            f\"[{correlation_id}] Streaming method {method_name} called \"\n            f\"(streaming implementation pending)\"\n        )\n        \n        return behavior(request_data, context)\n    \n    def _log_request(\n        self, \n        method_name: str, \n        request: Any, \n        correlation_id: str, \n        context: ServicerContext\n    ):\n        \"\"\"Log incoming request with structured data.\"\"\"\n        \n        # Extract client information\n        peer = context.peer() if hasattr(context, 'peer') else 'unknown'\n        \n        # Sanitize request data\n        request_data = self._sanitize_data(self._extract_request_data(request))\n        \n        log_data = {\n            'event': 'grpc_request',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'peer': peer,\n            'request_data': request_data,\n            'timestamp': time.time()\n        }\n        \n        self.logger.log(\n            self.log_level,\n            f\"[{correlation_id}] gRPC Request: {method_name}\",\n            extra={'structured_data': log_data}\n        )\n    \n    def _log_response(\n        self, \n        method_name: str, \n        response: Any, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log outgoing response with structured data.\"\"\"\n        \n        # Sanitize response data\n        response_data = self._sanitize_data(self._extract_response_data(response))\n        \n        log_data = {\n            'event': 'grpc_response',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'duration_ms': round(duration * 1000, 2),\n            'response_data': response_data,\n            'timestamp': time.time()\n        }\n        \n        self.logger.log(\n            self.log_level,\n            f\"[{correlation_id}] gRPC Response: {method_name} ({duration*1000:.2f}ms)\",\n            extra={'structured_data': log_data}\n        )\n    \n    def _log_error(\n        self, \n        method_name: str, \n        error: Exception, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log error with comprehensive context.\"\"\"\n        \n        log_data = {\n            'event': 'grpc_error',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'error_type': type(error).__name__,\n            'error_message': str(error),\n            'duration_ms': round(duration * 1000, 2),\n            'timestamp': time.time()\n        }\n        \n        self.logger.error(\n            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n            extra={'structured_data': log_data},\n            exc_info=True\n        )\n    \n    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n        \"\"\"Log performance metrics.\"\"\"\n        \n        if duration > 1.0:  # Log slow requests (>1 second)\n            self.logger.warning(\n                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n            )\n    \n    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(request, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_request_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}\n    \n    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a value for logging.\"\"\"\n        try:\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                import base64\n                encoded = base64.b64encode(value).decode()\n                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n            elif hasattr(value, 'isoformat'):\n                return value.isoformat()\n            elif hasattr(value, 'name'):  # Enum value\n                return value.name\n            else:\n                str_value = str(value)\n                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n        except Exception:\n            return \"<non-serializable>\"\n    \n    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            elif hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    if response.HasField(field.name):\n                        result[field.name] = getattr(response, field.name)\n                return result\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}\n    \n    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove sensitive information from logged data.\"\"\"\n        if not isinstance(data, dict):\n            return data\n        \n        sanitized = {}\n        for key, value in data.items():\n            key_lower = key.lower()\n            \n            # Check if field contains sensitive information\n            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n                sanitized[key] = '***REDACTED***'\n            elif isinstance(value, dict):\n                sanitized[key] = self._sanitize_data(value)\n            elif isinstance(value, (list, tuple)):\n                sanitized[key] = [\n                    self._sanitize_data(item) if isinstance(item, dict) else item\n                    for item in value\n                ]\n            else:\n                sanitized[key] = value\n        \n        return sanitized\n    \n    def _update_timing_metrics(self, method_name: str, duration: float):\n        \"\"\"Update timing metrics for performance tracking.\"\"\"\n        method_stats = self._metrics['method_stats'][method_name]\n        current_avg = method_stats['avg_time']\n        count = method_stats['count']\n        \n        # Update method-specific average\n        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n        \n        # Update overall average\n        total_requests = self._metrics['total_requests']\n        current_overall_avg = self._metrics['avg_response_time']\n        self._metrics['avg_response_time'] = (\n            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n        )\n    \n    def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get current performance metrics.\"\"\"\n        return {\n            'interceptor_name': 'LoggingInterceptor',\n            'metrics': self._metrics.copy(),\n            'timestamp': time.time()\n        }\n    \n    def reset_metrics(self):\n        \"\"\"Reset performance metrics.\"\"\"\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n\n\n# Convenience function for creating configured logging interceptor\ndef create_logging_interceptor(\n    logger_name: str = \"grpc.logging\",\n    log_level: int = logging.INFO,\n    **kwargs\n) -> LoggingInterceptor:\n    \"\"\"Create a pre-configured logging interceptor.\n    \n    Args:\n        logger_name: Name for the logger\n        log_level: Logging level\n        **kwargs: Additional configuration options\n        \n    Returns:\n        Configured LoggingInterceptor instance\n    \"\"\"\n    logger = logging.getLogger(logger_name)\n    return LoggingInterceptor(\n        logger=logger,\n        log_level=log_level,\n        **kwargs\n    )","originalFile":"\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n\nThis interceptor provides:\n- Detailed request/response logging\n- Performance metrics tracking\n- Error logging and analysis\n- Request correlation and tracing\n- Structured logging with context\n\"\"\"\nimport json\nimport logging\nimport time\nfrom typing import Any, Callable, Iterator, Optional, Dict\nfrom uuid import uuid4\n\nimport grpc\nimport grpc.aio\nfrom grpc.aio import ServerInterceptor, ServicerContext\n\n\nclass LoggingInterceptor(grpc.aio.ServerInterceptor):\n    \"\"\"Advanced logging interceptor for gRPC services.\n    \n    Features:\n    - Request/response logging with structured data\n    - Performance metrics and timing\n    - Error tracking and analysis\n    - Request correlation IDs\n    - Configurable log levels and filters\n    - Security-aware logging (excludes sensitive data)\n    \"\"\"\n    \n    def __init__(\n        self,\n        logger: Optional[logging.Logger] = None,\n        log_level: int = logging.INFO,\n        log_requests: bool = True,\n        log_responses: bool = True,\n        log_errors: bool = True,\n        log_performance: bool = True,\n        exclude_methods: Optional[list] = None,\n        sensitive_fields: Optional[list] = None\n    ):\n        self.logger = logger or logging.getLogger(__name__)\n        self.log_level = log_level\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_errors = log_errors\n        self.log_performance = log_performance\n        self.exclude_methods = exclude_methods or []\n        self.sensitive_fields = sensitive_fields or [\n            'password', 'token', 'secret', 'key', 'credential', 'auth'\n        ]\n        \n        # Performance metrics\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n    \n    async def intercept_service(\n        self,\n        continuation: Callable[[Any], Any],\n        handler_call_details: Any,\n    ) -> Any:\n        \"\"\"Intercept service calls for logging.\"\"\"\n        \n        # Skip excluded methods\n        method_name = handler_call_details.method\n        if method_name in self.exclude_methods:\n            return await continuation(handler_call_details)\n        \n        # Get the original handler\n        handler = await continuation(handler_call_details)\n        if handler is None:\n            return None\n        \n        # Wrap the handler based on its type; bypass reflection and health entirely\n        if method_name in {\n            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n            '/grpc.health.v1.Health/Check',\n            '/grpc.health.v1.Health/Watch',\n        }:\n            return handler\n        if handler.unary_unary:\n            return grpc.unary_unary_rpc_method_handler(\n                self._wrap_unary_unary(handler.unary_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.unary_stream:\n            return grpc.unary_stream_rpc_method_handler(\n                self._wrap_unary_stream(handler.unary_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_unary:\n            return grpc.stream_unary_rpc_method_handler(\n                self._wrap_stream_unary(handler.stream_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_stream:\n            return grpc.stream_stream_rpc_method_handler(\n                self._wrap_stream_stream(handler.stream_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        else:\n            return handler\n    \n    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-unary RPC method.\"\"\"\n        \n        async def wrapper(request: Any, context: ServicerContext) -> Any:\n            return await self._handle_request(\n                behavior, request, context, method_name, is_streaming=False\n            )\n        \n        return wrapper\n    \n    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n        async def wrapper(request: Any, context: ServicerContext):\n            # For streaming responses, return the iterator directly (do not await)\n            try:\n                return behavior(request, context)\n            finally:\n                # Minimal logging to avoid interfering with stream lifecycle\n                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-unary RPC method.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n            # Request is streaming, response is unary; await the final response\n            try:\n                return await behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext):\n            # For bi-di streaming, return the response iterator directly\n            try:\n                return behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n        return wrapper\n    \n    async def _handle_request(\n        self, \n        behavior: Callable, \n        request: Any, \n        context: ServicerContext, \n        method_name: str,\n        is_streaming: bool = False\n    ) -> Any:\n        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n        \n        # Generate correlation ID\n        correlation_id = str(uuid4())\n        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            pass\n        \n        start_time = time.time()\n        \n        # Update metrics\n        self._metrics['total_requests'] += 1\n        if method_name not in self._metrics['method_stats']:\n            self._metrics['method_stats'][method_name] = {\n                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n            }\n        self._metrics['method_stats'][method_name]['count'] += 1\n        \n        try:\n            # Log request\n            if self.log_requests:\n                self._log_request(method_name, request, correlation_id, context)\n            \n            # Execute the actual method\n            response = await behavior(request, context)\n            \n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update success metrics\n            self._metrics['successful_requests'] += 1\n            self._metrics['method_stats'][method_name]['success'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log response\n            if self.log_responses:\n                self._log_response(method_name, response, correlation_id, duration, context)\n            \n            # Log performance\n            if self.log_performance:\n                self._log_performance(method_name, duration, correlation_id)\n            \n            return response\n            \n        except Exception as e:\n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update error metrics\n            self._metrics['failed_requests'] += 1\n            self._metrics['method_stats'][method_name]['error'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log error\n            if self.log_errors:\n                self._log_error(method_name, e, correlation_id, duration, context)\n            \n            # Re-raise the exception\n            raise\n    \n    async def _handle_streaming_request(\n        self,\n        behavior: Callable,\n        request_data: Any,\n        context: ServicerContext,\n        method_name: str,\n        request_streaming: bool = False,\n        response_streaming: bool = True\n    ) -> Any:\n        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n        # For Phase 3, we're focusing on request/response patterns\n        # Streaming would be implemented in a future phase\n        correlation_id = str(uuid4())\n        # Store correlation ID in invocation metadata instead of setting attributes\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            # If metadata setting fails, continue without it\n            pass\n        \n        self.logger.info(\n            f\"[{correlation_id}] Streaming method {method_name} called \"\n            f\"(streaming implementation pending)\"\n        )\n        \n        return behavior(request_data, context)\n    \n    def _log_request(\n        self, \n        method_name: str, \n        request: Any, \n        correlation_id: str, \n        context: ServicerContext\n    ):\n        \"\"\"Log incoming request with structured data.\"\"\"\n        \n        # Extract client information\n        peer = context.peer() if hasattr(context, 'peer') else 'unknown'\n        \n        # Sanitize request data\n        request_data = self._sanitize_data(self._extract_request_data(request))\n        \n        log_data = {\n            'event': 'grpc_request',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'peer': peer,\n            'request_data': request_data,\n            'timestamp': time.time()\n        }\n        \n        self.logger.log(\n            self.log_level,\n            f\"[{correlation_id}] gRPC Request: {method_name}\",\n            extra={'structured_data': log_data}\n        )\n    \n    def _log_response(\n        self, \n        method_name: str, \n        response: Any, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log outgoing response with structured data.\"\"\"\n        \n        # Sanitize response data\n        response_data = self._sanitize_data(self._extract_response_data(response))\n        \n        log_data = {\n            'event': 'grpc_response',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'duration_ms': round(duration * 1000, 2),\n            'response_data': response_data,\n            'timestamp': time.time()\n        }\n        \n        self.logger.log(\n            self.log_level,\n            f\"[{correlation_id}] gRPC Response: {method_name} ({duration*1000:.2f}ms)\",\n            extra={'structured_data': log_data}\n        )\n    \n    def _log_error(\n        self, \n        method_name: str, \n        error: Exception, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log error with comprehensive context.\"\"\"\n        \n        log_data = {\n            'event': 'grpc_error',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'error_type': type(error).__name__,\n            'error_message': str(error),\n            'duration_ms': round(duration * 1000, 2),\n            'timestamp': time.time()\n        }\n        \n        self.logger.error(\n            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n            extra={'structured_data': log_data},\n            exc_info=True\n        )\n    \n    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n        \"\"\"Log performance metrics.\"\"\"\n        \n        if duration > 1.0:  # Log slow requests (>1 second)\n            self.logger.warning(\n                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n            )\n    \n    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            elif hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    if request.HasField(field.name):\n                        result[field.name] = getattr(request, field.name)\n                return result\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}\n    \n    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            elif hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    if response.HasField(field.name):\n                        result[field.name] = getattr(response, field.name)\n                return result\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}\n    \n    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove sensitive information from logged data.\"\"\"\n        if not isinstance(data, dict):\n            return data\n        \n        sanitized = {}\n        for key, value in data.items():\n            key_lower = key.lower()\n            \n            # Check if field contains sensitive information\n            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n                sanitized[key] = '***REDACTED***'\n            elif isinstance(value, dict):\n                sanitized[key] = self._sanitize_data(value)\n            elif isinstance(value, (list, tuple)):\n                sanitized[key] = [\n                    self._sanitize_data(item) if isinstance(item, dict) else item\n                    for item in value\n                ]\n            else:\n                sanitized[key] = value\n        \n        return sanitized\n    \n    def _update_timing_metrics(self, method_name: str, duration: float):\n        \"\"\"Update timing metrics for performance tracking.\"\"\"\n        method_stats = self._metrics['method_stats'][method_name]\n        current_avg = method_stats['avg_time']\n        count = method_stats['count']\n        \n        # Update method-specific average\n        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n        \n        # Update overall average\n        total_requests = self._metrics['total_requests']\n        current_overall_avg = self._metrics['avg_response_time']\n        self._metrics['avg_response_time'] = (\n            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n        )\n    \n    def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get current performance metrics.\"\"\"\n        return {\n            'interceptor_name': 'LoggingInterceptor',\n            'metrics': self._metrics.copy(),\n            'timestamp': time.time()\n        }\n    \n    def reset_metrics(self):\n        \"\"\"Reset performance metrics.\"\"\"\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n\n\n# Convenience function for creating configured logging interceptor\ndef create_logging_interceptor(\n    logger_name: str = \"grpc.logging\",\n    log_level: int = logging.INFO,\n    **kwargs\n) -> LoggingInterceptor:\n    \"\"\"Create a pre-configured logging interceptor.\n    \n    Args:\n        logger_name: Name for the logger\n        log_level: Logging level\n        **kwargs: Additional configuration options\n        \n    Returns:\n        Configured LoggingInterceptor instance\n    \"\"\"\n    logger = logging.getLogger(logger_name)\n    return LoggingInterceptor(\n        logger=logger,\n        log_level=log_level,\n        **kwargs\n    )","structuredPatch":[{"oldStart":351,"oldLines":20,"newStart":351,"newLines":54,"lines":["     def _extract_request_data(self, request: Any) -> Dict[str, Any]:","         \"\"\"Extract relevant data from request for logging.\"\"\"","         try:","-            if hasattr(request, '__dict__'):","-                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}","-            elif hasattr(request, 'DESCRIPTOR'):","-                # Protobuf message","+            if hasattr(request, 'DESCRIPTOR'):","+                # Protobuf message - extract all fields including defaults","                 result = {}","                 for field in request.DESCRIPTOR.fields:","-                    if request.HasField(field.name):","-                        result[field.name] = getattr(request, field.name)","+                    field_name = field.name","+                    try:","+                        value = getattr(request, field_name, None)","+                        if value is not None:","+                            if hasattr(value, 'DESCRIPTOR'):","+                                # Nested protobuf message","+                                result[field_name] = self._extract_request_data(value)","+                            elif field.label == field.LABEL_REPEATED:","+                                # Repeated field (list)","+                                result[field_name] = [","+                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)","+                                    for item in value","+                                ]","+                            else:","+                                result[field_name] = self._serialize_value(value)","+                    except Exception as e:","+                        result[field_name] = f\"<error: {str(e)}>\"","                 return result","+            elif hasattr(request, '__dict__'):","+                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}","             else:","                 return {'value': str(request)}","         except Exception:","             return {'error': 'Failed to extract request data'}","     ","+    def _serialize_value(self, value: Any) -> Any:","+        \"\"\"Serialize a value for logging.\"\"\"","+        try:","+            if isinstance(value, (str, int, float, bool)):","+                return value","+            elif isinstance(value, bytes):","+                import base64","+                encoded = base64.b64encode(value).decode()","+                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"","+            elif hasattr(value, 'isoformat'):","+                return value.isoformat()","+            elif hasattr(value, 'name'):  # Enum value","+                return value.name","+            else:","+                str_value = str(value)","+                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value","+        except Exception:","+            return \"<non-serializable>\"","+    ","     def _extract_response_data(self, response: Any) -> Dict[str, Any]:","         \"\"\"Extract relevant data from response for logging.\"\"\"","         try:"]}],"userModified":false,"replaceAll":false}}
{"parentUuid":"aab37eae-bd16-48c5-ab0f-91884fd4b1b6","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01Nzb6x3vwWv3aNuF1KyNDy8","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Bây giờ tôi cần cập nhật `_extract_response_data` để cũng có thể extract đầy đủ response data:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":7352,"cache_read_input_tokens":61819,"cache_creation":{"ephemeral_5m_input_tokens":7352,"ephemeral_1h_input_tokens":0},"output_tokens":3,"service_tier":"standard"}},"requestId":"req_011CSJ8huF1KQPR499t897hd","type":"assistant","uuid":"a936120c-f2e8-4664-a8fe-ed6481d53ba7","timestamp":"2025-08-20T01:58:39.078Z"}
{"parentUuid":"a936120c-f2e8-4664-a8fe-ed6481d53ba7","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01Nzb6x3vwWv3aNuF1KyNDy8","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01UVVUuFW48erRUZN5F1bdsv","name":"Edit","input":{"file_path":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py","old_string":"    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            elif hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    if response.HasField(field.name):\n                        result[field.name] = getattr(response, field.name)\n                return result\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}","new_string":"    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(response, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_response_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":7352,"cache_read_input_tokens":61819,"cache_creation":{"ephemeral_5m_input_tokens":7352,"ephemeral_1h_input_tokens":0},"output_tokens":723,"service_tier":"standard"}},"requestId":"req_011CSJ8huF1KQPR499t897hd","type":"assistant","uuid":"2f98186b-d465-4034-8d2d-f482c16a17e5","timestamp":"2025-08-20T01:58:47.313Z"}
{"parentUuid":"2f98186b-d465-4034-8d2d-f482c16a17e5","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01UVVUuFW48erRUZN5F1bdsv","type":"tool_result","content":"The file /Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   398→                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n   399→        except Exception:\n   400→            return \"<non-serializable>\"\n   401→    \n   402→    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n   403→        \"\"\"Extract relevant data from response for logging.\"\"\"\n   404→        try:\n   405→            if hasattr(response, 'DESCRIPTOR'):\n   406→                # Protobuf message - extract all fields including defaults\n   407→                result = {}\n   408→                for field in response.DESCRIPTOR.fields:\n   409→                    field_name = field.name\n   410→                    try:\n   411→                        value = getattr(response, field_name, None)\n   412→                        if value is not None:\n   413→                            if hasattr(value, 'DESCRIPTOR'):\n   414→                                # Nested protobuf message\n   415→                                result[field_name] = self._extract_response_data(value)\n   416→                            elif field.label == field.LABEL_REPEATED:\n   417→                                # Repeated field (list)\n   418→                                result[field_name] = [\n   419→                                    self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n   420→                                    for item in value\n   421→                                ]\n   422→                            else:\n   423→                                result[field_name] = self._serialize_value(value)\n   424→                    except Exception as e:\n   425→                        result[field_name] = f\"<error: {str(e)}>\"\n   426→                return result\n   427→            elif hasattr(response, '__dict__'):\n   428→                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n   429→            else:\n   430→                return {'value': str(response)}\n   431→        except Exception:\n   432→            return {'error': 'Failed to extract response data'}\n   433→    \n   434→    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n   435→        \"\"\"Remove sensitive information from logged data.\"\"\"\n   436→        if not isinstance(data, dict):"}]},"uuid":"c53fdbe5-391f-4ccc-b56b-4f4f970b1260","timestamp":"2025-08-20T01:58:58.555Z","toolUseResult":{"filePath":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py","oldString":"    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            elif hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    if response.HasField(field.name):\n                        result[field.name] = getattr(response, field.name)\n                return result\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}","newString":"    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(response, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_response_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}","originalFile":"\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n\nThis interceptor provides:\n- Detailed request/response logging\n- Performance metrics tracking\n- Error logging and analysis\n- Request correlation and tracing\n- Structured logging with context\n\"\"\"\nimport json\nimport logging\nimport time\nfrom typing import Any, Callable, Iterator, Optional, Dict\nfrom uuid import uuid4\n\nimport grpc\nimport grpc.aio\nfrom grpc.aio import ServerInterceptor, ServicerContext\n\n\nclass LoggingInterceptor(grpc.aio.ServerInterceptor):\n    \"\"\"Advanced logging interceptor for gRPC services.\n    \n    Features:\n    - Request/response logging with structured data\n    - Performance metrics and timing\n    - Error tracking and analysis\n    - Request correlation IDs\n    - Configurable log levels and filters\n    - Security-aware logging (excludes sensitive data)\n    \"\"\"\n    \n    def __init__(\n        self,\n        logger: Optional[logging.Logger] = None,\n        log_level: int = logging.INFO,\n        log_requests: bool = True,\n        log_responses: bool = True,\n        log_errors: bool = True,\n        log_performance: bool = True,\n        exclude_methods: Optional[list] = None,\n        sensitive_fields: Optional[list] = None\n    ):\n        self.logger = logger or logging.getLogger(__name__)\n        self.log_level = log_level\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_errors = log_errors\n        self.log_performance = log_performance\n        self.exclude_methods = exclude_methods or []\n        self.sensitive_fields = sensitive_fields or [\n            'password', 'token', 'secret', 'key', 'credential', 'auth'\n        ]\n        \n        # Performance metrics\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n    \n    async def intercept_service(\n        self,\n        continuation: Callable[[Any], Any],\n        handler_call_details: Any,\n    ) -> Any:\n        \"\"\"Intercept service calls for logging.\"\"\"\n        \n        # Skip excluded methods\n        method_name = handler_call_details.method\n        if method_name in self.exclude_methods:\n            return await continuation(handler_call_details)\n        \n        # Get the original handler\n        handler = await continuation(handler_call_details)\n        if handler is None:\n            return None\n        \n        # Wrap the handler based on its type; bypass reflection and health entirely\n        if method_name in {\n            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n            '/grpc.health.v1.Health/Check',\n            '/grpc.health.v1.Health/Watch',\n        }:\n            return handler\n        if handler.unary_unary:\n            return grpc.unary_unary_rpc_method_handler(\n                self._wrap_unary_unary(handler.unary_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.unary_stream:\n            return grpc.unary_stream_rpc_method_handler(\n                self._wrap_unary_stream(handler.unary_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_unary:\n            return grpc.stream_unary_rpc_method_handler(\n                self._wrap_stream_unary(handler.stream_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_stream:\n            return grpc.stream_stream_rpc_method_handler(\n                self._wrap_stream_stream(handler.stream_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        else:\n            return handler\n    \n    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-unary RPC method.\"\"\"\n        \n        async def wrapper(request: Any, context: ServicerContext) -> Any:\n            return await self._handle_request(\n                behavior, request, context, method_name, is_streaming=False\n            )\n        \n        return wrapper\n    \n    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n        async def wrapper(request: Any, context: ServicerContext):\n            # For streaming responses, return the iterator directly (do not await)\n            try:\n                return behavior(request, context)\n            finally:\n                # Minimal logging to avoid interfering with stream lifecycle\n                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-unary RPC method.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n            # Request is streaming, response is unary; await the final response\n            try:\n                return await behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext):\n            # For bi-di streaming, return the response iterator directly\n            try:\n                return behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n        return wrapper\n    \n    async def _handle_request(\n        self, \n        behavior: Callable, \n        request: Any, \n        context: ServicerContext, \n        method_name: str,\n        is_streaming: bool = False\n    ) -> Any:\n        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n        \n        # Generate correlation ID\n        correlation_id = str(uuid4())\n        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            pass\n        \n        start_time = time.time()\n        \n        # Update metrics\n        self._metrics['total_requests'] += 1\n        if method_name not in self._metrics['method_stats']:\n            self._metrics['method_stats'][method_name] = {\n                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n            }\n        self._metrics['method_stats'][method_name]['count'] += 1\n        \n        try:\n            # Log request\n            if self.log_requests:\n                self._log_request(method_name, request, correlation_id, context)\n            \n            # Execute the actual method\n            response = await behavior(request, context)\n            \n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update success metrics\n            self._metrics['successful_requests'] += 1\n            self._metrics['method_stats'][method_name]['success'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log response\n            if self.log_responses:\n                self._log_response(method_name, response, correlation_id, duration, context)\n            \n            # Log performance\n            if self.log_performance:\n                self._log_performance(method_name, duration, correlation_id)\n            \n            return response\n            \n        except Exception as e:\n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update error metrics\n            self._metrics['failed_requests'] += 1\n            self._metrics['method_stats'][method_name]['error'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log error\n            if self.log_errors:\n                self._log_error(method_name, e, correlation_id, duration, context)\n            \n            # Re-raise the exception\n            raise\n    \n    async def _handle_streaming_request(\n        self,\n        behavior: Callable,\n        request_data: Any,\n        context: ServicerContext,\n        method_name: str,\n        request_streaming: bool = False,\n        response_streaming: bool = True\n    ) -> Any:\n        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n        # For Phase 3, we're focusing on request/response patterns\n        # Streaming would be implemented in a future phase\n        correlation_id = str(uuid4())\n        # Store correlation ID in invocation metadata instead of setting attributes\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            # If metadata setting fails, continue without it\n            pass\n        \n        self.logger.info(\n            f\"[{correlation_id}] Streaming method {method_name} called \"\n            f\"(streaming implementation pending)\"\n        )\n        \n        return behavior(request_data, context)\n    \n    def _log_request(\n        self, \n        method_name: str, \n        request: Any, \n        correlation_id: str, \n        context: ServicerContext\n    ):\n        \"\"\"Log incoming request with structured data.\"\"\"\n        \n        # Extract client information\n        peer = context.peer() if hasattr(context, 'peer') else 'unknown'\n        \n        # Sanitize request data\n        request_data = self._sanitize_data(self._extract_request_data(request))\n        \n        log_data = {\n            'event': 'grpc_request',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'peer': peer,\n            'request_data': request_data,\n            'timestamp': time.time()\n        }\n        \n        self.logger.log(\n            self.log_level,\n            f\"[{correlation_id}] gRPC Request: {method_name}\",\n            extra={'structured_data': log_data}\n        )\n    \n    def _log_response(\n        self, \n        method_name: str, \n        response: Any, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log outgoing response with structured data.\"\"\"\n        \n        # Sanitize response data\n        response_data = self._sanitize_data(self._extract_response_data(response))\n        \n        log_data = {\n            'event': 'grpc_response',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'duration_ms': round(duration * 1000, 2),\n            'response_data': response_data,\n            'timestamp': time.time()\n        }\n        \n        self.logger.log(\n            self.log_level,\n            f\"[{correlation_id}] gRPC Response: {method_name} ({duration*1000:.2f}ms)\",\n            extra={'structured_data': log_data}\n        )\n    \n    def _log_error(\n        self, \n        method_name: str, \n        error: Exception, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log error with comprehensive context.\"\"\"\n        \n        log_data = {\n            'event': 'grpc_error',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'error_type': type(error).__name__,\n            'error_message': str(error),\n            'duration_ms': round(duration * 1000, 2),\n            'timestamp': time.time()\n        }\n        \n        self.logger.error(\n            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n            extra={'structured_data': log_data},\n            exc_info=True\n        )\n    \n    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n        \"\"\"Log performance metrics.\"\"\"\n        \n        if duration > 1.0:  # Log slow requests (>1 second)\n            self.logger.warning(\n                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n            )\n    \n    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(request, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_request_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}\n    \n    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a value for logging.\"\"\"\n        try:\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                import base64\n                encoded = base64.b64encode(value).decode()\n                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n            elif hasattr(value, 'isoformat'):\n                return value.isoformat()\n            elif hasattr(value, 'name'):  # Enum value\n                return value.name\n            else:\n                str_value = str(value)\n                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n        except Exception:\n            return \"<non-serializable>\"\n    \n    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            elif hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    if response.HasField(field.name):\n                        result[field.name] = getattr(response, field.name)\n                return result\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}\n    \n    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove sensitive information from logged data.\"\"\"\n        if not isinstance(data, dict):\n            return data\n        \n        sanitized = {}\n        for key, value in data.items():\n            key_lower = key.lower()\n            \n            # Check if field contains sensitive information\n            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n                sanitized[key] = '***REDACTED***'\n            elif isinstance(value, dict):\n                sanitized[key] = self._sanitize_data(value)\n            elif isinstance(value, (list, tuple)):\n                sanitized[key] = [\n                    self._sanitize_data(item) if isinstance(item, dict) else item\n                    for item in value\n                ]\n            else:\n                sanitized[key] = value\n        \n        return sanitized\n    \n    def _update_timing_metrics(self, method_name: str, duration: float):\n        \"\"\"Update timing metrics for performance tracking.\"\"\"\n        method_stats = self._metrics['method_stats'][method_name]\n        current_avg = method_stats['avg_time']\n        count = method_stats['count']\n        \n        # Update method-specific average\n        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n        \n        # Update overall average\n        total_requests = self._metrics['total_requests']\n        current_overall_avg = self._metrics['avg_response_time']\n        self._metrics['avg_response_time'] = (\n            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n        )\n    \n    def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get current performance metrics.\"\"\"\n        return {\n            'interceptor_name': 'LoggingInterceptor',\n            'metrics': self._metrics.copy(),\n            'timestamp': time.time()\n        }\n    \n    def reset_metrics(self):\n        \"\"\"Reset performance metrics.\"\"\"\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n\n\n# Convenience function for creating configured logging interceptor\ndef create_logging_interceptor(\n    logger_name: str = \"grpc.logging\",\n    log_level: int = logging.INFO,\n    **kwargs\n) -> LoggingInterceptor:\n    \"\"\"Create a pre-configured logging interceptor.\n    \n    Args:\n        logger_name: Name for the logger\n        log_level: Logging level\n        **kwargs: Additional configuration options\n        \n    Returns:\n        Configured LoggingInterceptor instance\n    \"\"\"\n    logger = logging.getLogger(logger_name)\n    return LoggingInterceptor(\n        logger=logger,\n        log_level=log_level,\n        **kwargs\n    )","structuredPatch":[{"oldStart":402,"oldLines":15,"newStart":402,"newLines":30,"lines":["     def _extract_response_data(self, response: Any) -> Dict[str, Any]:","         \"\"\"Extract relevant data from response for logging.\"\"\"","         try:","-            if hasattr(response, '__dict__'):","-                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}","-            elif hasattr(response, 'DESCRIPTOR'):","-                # Protobuf message","+            if hasattr(response, 'DESCRIPTOR'):","+                # Protobuf message - extract all fields including defaults","                 result = {}","                 for field in response.DESCRIPTOR.fields:","-                    if response.HasField(field.name):","-                        result[field.name] = getattr(response, field.name)","+                    field_name = field.name","+                    try:","+                        value = getattr(response, field_name, None)","+                        if value is not None:","+                            if hasattr(value, 'DESCRIPTOR'):","+                                # Nested protobuf message","+                                result[field_name] = self._extract_response_data(value)","+                            elif field.label == field.LABEL_REPEATED:","+                                # Repeated field (list)","+                                result[field_name] = [","+                                    self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)","+                                    for item in value","+                                ]","+                            else:","+                                result[field_name] = self._serialize_value(value)","+                    except Exception as e:","+                        result[field_name] = f\"<error: {str(e)}>\"","                 return result","+            elif hasattr(response, '__dict__'):","+                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}","             else:","                 return {'value': str(response)}","         except Exception:"]}],"userModified":false,"replaceAll":false}}
{"parentUuid":"c53fdbe5-391f-4ccc-b56b-4f4f970b1260","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01CWb8AeqEK9ERKfEHhaLCZm","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01Y4V6kAuEfmfusZMhuCZvwQ","name":"TodoWrite","input":{"todos":[{"id":"1","content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"completed","priority":"high"},{"id":"2","content":"Tạo interceptor gRPC để log request/response","status":"completed","priority":"high"},{"id":"3","content":"Thêm logging vào các servicer chính","status":"completed","priority":"high"},{"id":"4","content":"Cấu hình log format và level phù hợp","status":"in_progress","priority":"medium"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":1381,"cache_read_input_tokens":69171,"cache_creation":{"ephemeral_5m_input_tokens":1381,"ephemeral_1h_input_tokens":0},"output_tokens":283,"service_tier":"standard"}},"requestId":"req_011CSJ8jjr1zncqNKip91NBk","type":"assistant","uuid":"38a12cf9-d0a0-4d51-9663-3088a051ef66","timestamp":"2025-08-20T01:59:07.715Z"}
{"parentUuid":"38a12cf9-d0a0-4d51-9663-3088a051ef66","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Y4V6kAuEfmfusZMhuCZvwQ","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"f044f4ac-0d8f-40cd-a162-b714fc38d0e8","timestamp":"2025-08-20T01:59:07.767Z","toolUseResult":{"oldTodos":[{"content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"completed","priority":"high","id":"1"},{"content":"Tạo interceptor gRPC để log request/response","status":"completed","priority":"high","id":"2"},{"content":"Thêm logging vào các servicer chính","status":"in_progress","priority":"high","id":"3"},{"content":"Cấu hình log format và level phù hợp","status":"pending","priority":"medium","id":"4"}],"newTodos":[{"content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"completed","priority":"high","id":"1"},{"content":"Tạo interceptor gRPC để log request/response","status":"completed","priority":"high","id":"2"},{"content":"Thêm logging vào các servicer chính","status":"completed","priority":"high","id":"3"},{"content":"Cấu hình log format và level phù hợp","status":"in_progress","priority":"medium","id":"4"}]}}
{"parentUuid":"f044f4ac-0d8f-40cd-a162-b714fc38d0e8","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_018aoV97GcCP8i9TTW9itXoq","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Bây giờ tôi cần cấu hình logging format và level. Tôi sẽ cập nhật main.py để có cấu hình logging chi tiết hơn:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":503,"cache_read_input_tokens":70552,"cache_creation":{"ephemeral_5m_input_tokens":503,"ephemeral_1h_input_tokens":0},"output_tokens":2,"service_tier":"standard"}},"requestId":"req_011CSJ8kQJ4EBw1KmM4qwpeP","type":"assistant","uuid":"1ede3f21-f229-4ac8-ac8d-4db4c977d986","timestamp":"2025-08-20T01:59:13.154Z"}
{"parentUuid":"1ede3f21-f229-4ac8-ac8d-4db4c977d986","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_018aoV97GcCP8i9TTW9itXoq","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_015CdTtZ5rHHJ57SxkBEp6jh","name":"Edit","input":{"file_path":"/Users/lee/Documents/src/bep-oms-broker/main.py","old_string":"async def main():\n    # Configure logging with detailed format\n    logging.basicConfig(\n        level=logging.DEBUG,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.StreamHandler(),\n            logging.FileHandler('grpc_server.log'),\n            logging.FileHandler('server_output.log')\n        ]\n    )\n    \n    # Set specific loggers to appropriate levels\n    logging.getLogger('sqlalchemy.engine').setLevel(logging.WARNING)  # Reduce SQL noise","new_string":"async def main():\n    # Configure structured logging with detailed format\n    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'\n    \n    # Create formatters\n    detailed_formatter = logging.Formatter(log_format)\n    json_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n    \n    # Configure root logger\n    root_logger = logging.getLogger()\n    root_logger.setLevel(logging.DEBUG)\n    \n    # Clear existing handlers\n    for handler in root_logger.handlers[:]:\n        root_logger.removeHandler(handler)\n    \n    # Console handler with structured format\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(logging.INFO)\n    console_handler.setFormatter(detailed_formatter)\n    root_logger.addHandler(console_handler)\n    \n    # File handler for all logs\n    file_handler = logging.FileHandler('grpc_server.log')\n    file_handler.setLevel(logging.DEBUG)\n    file_handler.setFormatter(detailed_formatter)\n    root_logger.addHandler(file_handler)\n    \n    # Separate file for request/response logs\n    request_response_handler = logging.FileHandler('grpc_requests.log')\n    request_response_handler.setLevel(logging.INFO)\n    request_response_handler.setFormatter(json_formatter)\n    \n    # Configure specific loggers\n    grpc_logger = logging.getLogger('grpc.logging')\n    grpc_logger.setLevel(logging.INFO)\n    grpc_logger.addHandler(request_response_handler)\n    grpc_logger.propagate = False  # Don't duplicate to root logger\n    \n    # Error logs\n    error_handler = logging.FileHandler('grpc_errors.log')\n    error_handler.setLevel(logging.ERROR)\n    error_handler.setFormatter(detailed_formatter)\n    root_logger.addHandler(error_handler)\n    \n    # Set specific loggers to appropriate levels\n    logging.getLogger('sqlalchemy.engine').setLevel(logging.WARNING)  # Reduce SQL noise\n    logging.getLogger('sqlalchemy.pool').setLevel(logging.WARNING)\n    logging.getLogger('asyncio').setLevel(logging.WARNING)\n    logging.getLogger('grpc._server').setLevel(logging.WARNING)\n    \n    # Enable detailed logging for our application\n    logging.getLogger('src').setLevel(logging.DEBUG)\n    \n    logging.info(\"Enhanced logging configuration complete\")\n    logging.info(\"Logs will be written to:\")\n    logging.info(\"  - grpc_server.log: All application logs\") \n    logging.info(\"  - grpc_requests.log: Request/response logs\")\n    logging.info(\"  - grpc_errors.log: Error logs only\")"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":503,"cache_read_input_tokens":70552,"cache_creation":{"ephemeral_5m_input_tokens":503,"ephemeral_1h_input_tokens":0},"output_tokens":999,"service_tier":"standard"}},"requestId":"req_011CSJ8kQJ4EBw1KmM4qwpeP","type":"assistant","uuid":"2dff53bf-44ed-4f50-9620-7a5170e0a879","timestamp":"2025-08-20T01:59:22.201Z"}
{"parentUuid":"2dff53bf-44ed-4f50-9620-7a5170e0a879","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_015CdTtZ5rHHJ57SxkBEp6jh","type":"tool_result","content":"The file /Users/lee/Documents/src/bep-oms-broker/main.py has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n    24→    ConfirmPasswordResetUseCase,\n    25→)\n    26→from src.infrastructure.persistence.async_database import get_async_session\n    27→\n    28→async def main():\n    29→    # Configure structured logging with detailed format\n    30→    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'\n    31→    \n    32→    # Create formatters\n    33→    detailed_formatter = logging.Formatter(log_format)\n    34→    json_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n    35→    \n    36→    # Configure root logger\n    37→    root_logger = logging.getLogger()\n    38→    root_logger.setLevel(logging.DEBUG)\n    39→    \n    40→    # Clear existing handlers\n    41→    for handler in root_logger.handlers[:]:\n    42→        root_logger.removeHandler(handler)\n    43→    \n    44→    # Console handler with structured format\n    45→    console_handler = logging.StreamHandler(sys.stdout)\n    46→    console_handler.setLevel(logging.INFO)\n    47→    console_handler.setFormatter(detailed_formatter)\n    48→    root_logger.addHandler(console_handler)\n    49→    \n    50→    # File handler for all logs\n    51→    file_handler = logging.FileHandler('grpc_server.log')\n    52→    file_handler.setLevel(logging.DEBUG)\n    53→    file_handler.setFormatter(detailed_formatter)\n    54→    root_logger.addHandler(file_handler)\n    55→    \n    56→    # Separate file for request/response logs\n    57→    request_response_handler = logging.FileHandler('grpc_requests.log')\n    58→    request_response_handler.setLevel(logging.INFO)\n    59→    request_response_handler.setFormatter(json_formatter)\n    60→    \n    61→    # Configure specific loggers\n    62→    grpc_logger = logging.getLogger('grpc.logging')\n    63→    grpc_logger.setLevel(logging.INFO)\n    64→    grpc_logger.addHandler(request_response_handler)\n    65→    grpc_logger.propagate = False  # Don't duplicate to root logger\n    66→    \n    67→    # Error logs\n    68→    error_handler = logging.FileHandler('grpc_errors.log')\n    69→    error_handler.setLevel(logging.ERROR)\n    70→    error_handler.setFormatter(detailed_formatter)\n    71→    root_logger.addHandler(error_handler)\n    72→    \n    73→    # Set specific loggers to appropriate levels\n    74→    logging.getLogger('sqlalchemy.engine').setLevel(logging.WARNING)  # Reduce SQL noise\n    75→    logging.getLogger('sqlalchemy.pool').setLevel(logging.WARNING)\n    76→    logging.getLogger('asyncio').setLevel(logging.WARNING)\n    77→    logging.getLogger('grpc._server').setLevel(logging.WARNING)\n    78→    \n    79→    # Enable detailed logging for our application\n    80→    logging.getLogger('src').setLevel(logging.DEBUG)\n    81→    \n    82→    logging.info(\"Enhanced logging configuration complete\")\n    83→    logging.info(\"Logs will be written to:\")\n    84→    logging.info(\"  - grpc_server.log: All application logs\") \n    85→    logging.info(\"  - grpc_requests.log: Request/response logs\")\n    86→    logging.info(\"  - grpc_errors.log: Error logs only\")\n    87→    \n    88→    # Initialize DI container\n    89→    container = Container()\n    90→    await container.init_resources()"}]},"uuid":"49673556-5ba7-49c9-be81-0c98c116b1ff","timestamp":"2025-08-20T01:59:41.773Z","toolUseResult":{"filePath":"/Users/lee/Documents/src/bep-oms-broker/main.py","oldString":"async def main():\n    # Configure logging with detailed format\n    logging.basicConfig(\n        level=logging.DEBUG,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.StreamHandler(),\n            logging.FileHandler('grpc_server.log'),\n            logging.FileHandler('server_output.log')\n        ]\n    )\n    \n    # Set specific loggers to appropriate levels\n    logging.getLogger('sqlalchemy.engine').setLevel(logging.WARNING)  # Reduce SQL noise","newString":"async def main():\n    # Configure structured logging with detailed format\n    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'\n    \n    # Create formatters\n    detailed_formatter = logging.Formatter(log_format)\n    json_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n    \n    # Configure root logger\n    root_logger = logging.getLogger()\n    root_logger.setLevel(logging.DEBUG)\n    \n    # Clear existing handlers\n    for handler in root_logger.handlers[:]:\n        root_logger.removeHandler(handler)\n    \n    # Console handler with structured format\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(logging.INFO)\n    console_handler.setFormatter(detailed_formatter)\n    root_logger.addHandler(console_handler)\n    \n    # File handler for all logs\n    file_handler = logging.FileHandler('grpc_server.log')\n    file_handler.setLevel(logging.DEBUG)\n    file_handler.setFormatter(detailed_formatter)\n    root_logger.addHandler(file_handler)\n    \n    # Separate file for request/response logs\n    request_response_handler = logging.FileHandler('grpc_requests.log')\n    request_response_handler.setLevel(logging.INFO)\n    request_response_handler.setFormatter(json_formatter)\n    \n    # Configure specific loggers\n    grpc_logger = logging.getLogger('grpc.logging')\n    grpc_logger.setLevel(logging.INFO)\n    grpc_logger.addHandler(request_response_handler)\n    grpc_logger.propagate = False  # Don't duplicate to root logger\n    \n    # Error logs\n    error_handler = logging.FileHandler('grpc_errors.log')\n    error_handler.setLevel(logging.ERROR)\n    error_handler.setFormatter(detailed_formatter)\n    root_logger.addHandler(error_handler)\n    \n    # Set specific loggers to appropriate levels\n    logging.getLogger('sqlalchemy.engine').setLevel(logging.WARNING)  # Reduce SQL noise\n    logging.getLogger('sqlalchemy.pool').setLevel(logging.WARNING)\n    logging.getLogger('asyncio').setLevel(logging.WARNING)\n    logging.getLogger('grpc._server').setLevel(logging.WARNING)\n    \n    # Enable detailed logging for our application\n    logging.getLogger('src').setLevel(logging.DEBUG)\n    \n    logging.info(\"Enhanced logging configuration complete\")\n    logging.info(\"Logs will be written to:\")\n    logging.info(\"  - grpc_server.log: All application logs\") \n    logging.info(\"  - grpc_requests.log: Request/response logs\")\n    logging.info(\"  - grpc_errors.log: Error logs only\")","originalFile":"import grpc\nimport logging\nimport sys\nimport os\nimport asyncio\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), 'src')))\n\nfrom src.infrastructure.di.container import Container\nfrom src.presentation.grpc.server_config import EnhancedGrpcServerConfig\nfrom src.application.services.order_application_service import OrderApplicationService\nfrom src.presentation.grpc.servicers.order_servicer import OrderServicer\nfrom src.infrastructure.config.aws_config import AWSConfig\nfrom src.infrastructure.adapters.email.aws_ses_service import AWSSESService\nfrom src.infrastructure.persistence.repositories.otp_repository import OTPRepository as OTPRepositoryImpl\nfrom src.application.use_cases.otp.send_otp_email_use_case import SendOTPEmailUseCase\nfrom src.application.use_cases.otp.verify_otp_use_case import VerifyOTPUseCase\nfrom src.application.use_cases.otp.verify_registration_use_case import VerifyRegistrationUseCase\nfrom src.application.use_cases.otp.change_password_use_cases import (\n    RequestPasswordResetUseCase,\n    ConfirmPasswordResetUseCase,\n)\nfrom src.infrastructure.persistence.async_database import get_async_session\n\nasync def main():\n    # Configure logging with detailed format\n    logging.basicConfig(\n        level=logging.DEBUG,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.StreamHandler(),\n            logging.FileHandler('grpc_server.log'),\n            logging.FileHandler('server_output.log')\n        ]\n    )\n    \n    # Set specific loggers to appropriate levels\n    logging.getLogger('sqlalchemy.engine').setLevel(logging.WARNING)  # Reduce SQL noise\n    \n    # Initialize DI container\n    container = Container()\n    await container.init_resources()\n    container.wire(modules=[\"src.presentation.grpc.servicers\", \"src.application.services\"])\n\n    # Create enhanced server with authentication enabled\n    server_config = EnhancedGrpcServerConfig(\n        port=55051,\n        max_workers=10,\n        enable_reflection=True,\n        enable_health_checks=True,\n        enable_metrics=True,\n        enable_authentication=True,\n        enable_rate_limiting=False,\n        enable_detailed_logging=True,\n        jwt_secret_key=os.getenv(\"TOKEN_ENCRYPTION_KEY\", \"super-secret-key\")\n    )\n    \n    # Helper to resolve possibly awaitable providers\n    import inspect\n    async def _resolve(maybe):\n        return await maybe if inspect.isawaitable(maybe) else maybe\n\n    # Configure server dependencies first\n    # Prepare OTP dependencies (async session)\n    send_otp_uc = None\n    verify_otp_uc = None\n    verify_registration_uc = None\n    request_change_password_uc = None\n    confirm_change_password_uc = None\n    try:\n        aws_config = AWSConfig()\n        ses_service = AWSSESService(aws_config)\n        # Pass session factory callable to repository to avoid closing issues\n        otp_repo = OTPRepositoryImpl(get_async_session)\n        send_otp_uc = SendOTPEmailUseCase(ses_service, otp_repo)\n        verify_otp_uc = VerifyOTPUseCase(otp_repo)\n        # Resolve providers to concrete instances to avoid passing awaitables\n        user_repo = await _resolve(container.user_repository())\n        password_hasher = await _resolve(container.password_hasher())\n\n        verify_registration_uc = VerifyRegistrationUseCase(\n            user_repository=user_repo,\n            verify_otp_use_case=verify_otp_uc,\n            otp_repository=otp_repo,\n        )\n        request_change_password_uc = RequestPasswordResetUseCase(\n            user_repository=user_repo,\n            send_otp_email_use_case=send_otp_uc,\n        )\n        confirm_change_password_uc = ConfirmPasswordResetUseCase(\n            otp_repository=otp_repo,\n            verify_otp_use_case=verify_otp_uc,\n            user_repository=user_repo,\n            password_hasher=password_hasher,\n        )\n        logging.info(\"OTP email feature enabled (AWS SES configured)\")\n    except Exception as e:\n        logging.warning(f\"OTP email feature disabled: {e}\")\n\n    user_app = await _resolve(container.user_application_service())\n    broker_app = await _resolve(container.broker_account_application_service())\n    market_data_app = await _resolve(container.market_data_application_service())\n    order_app = await _resolve(container.order_application_service())\n    \n    # Portfolio dependencies\n    get_portfolio_uc = await _resolve(container.get_portfolio_use_case())\n    portfolio_domain_mapper = await _resolve(container.portfolio_domain_mapper())\n\n    server_config.configure_dependencies(\n        user_application_service=user_app,\n        broker_account_application_service=broker_app,\n        market_data_application_service=market_data_app,\n        order_application_service=order_app,\n        use_cases={\n            # User use cases for consistent fallback paths across flows\n            'register_user': await _resolve(container.register_user_use_case()),\n            'update_user_profile': await _resolve(container.update_user_profile_use_case()),\n            'deactivate_user': await _resolve(container.deactivate_user_use_case()),\n            'login_user': await _resolve(container.login_user_use_case()),\n            'logout_user': await _resolve(container.logout_user_use_case()),\n            # OTP flows\n            'send_otp_email': send_otp_uc,\n            'verify_otp': verify_otp_uc,\n            'verify_registration': verify_registration_uc,\n            'request_password_reset': request_change_password_uc,\n            'confirm_password_reset': confirm_change_password_uc,\n        },\n        repositories={}\n    )\n    \n    # Set portfolio dependencies manually\n    server_config.get_portfolio_use_case = get_portfolio_uc\n    server_config.portfolio_domain_mapper = portfolio_domain_mapper\n    \n    # Configure the server\n    server_config.server = server_config.configure_server()\n    \n    # Start enhanced server\n    print(\"Starting enhanced gRPC server on port 55051...\")\n    print(\"Features enabled: Enhanced Servicers, Interceptors, Health Checks, Metrics\")\n    logging.info(\"Enhanced gRPC server starting with Phase 3 improvements\")\n    \n    await server_config.start_server()\n    await server_config.server.wait_for_termination()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())","structuredPatch":[{"oldStart":26,"oldLines":20,"newStart":26,"newLines":65,"lines":[" from src.infrastructure.persistence.async_database import get_async_session"," "," async def main():","-    # Configure logging with detailed format","-    logging.basicConfig(","-        level=logging.DEBUG,","-        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',","-        handlers=[","-            logging.StreamHandler(),","-            logging.FileHandler('grpc_server.log'),","-            logging.FileHandler('server_output.log')","-        ]","-    )","+    # Configure structured logging with detailed format","+    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'","     ","+    # Create formatters","+    detailed_formatter = logging.Formatter(log_format)","+    json_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')","+    ","+    # Configure root logger","+    root_logger = logging.getLogger()","+    root_logger.setLevel(logging.DEBUG)","+    ","+    # Clear existing handlers","+    for handler in root_logger.handlers[:]:","+        root_logger.removeHandler(handler)","+    ","+    # Console handler with structured format","+    console_handler = logging.StreamHandler(sys.stdout)","+    console_handler.setLevel(logging.INFO)","+    console_handler.setFormatter(detailed_formatter)","+    root_logger.addHandler(console_handler)","+    ","+    # File handler for all logs","+    file_handler = logging.FileHandler('grpc_server.log')","+    file_handler.setLevel(logging.DEBUG)","+    file_handler.setFormatter(detailed_formatter)","+    root_logger.addHandler(file_handler)","+    ","+    # Separate file for request/response logs","+    request_response_handler = logging.FileHandler('grpc_requests.log')","+    request_response_handler.setLevel(logging.INFO)","+    request_response_handler.setFormatter(json_formatter)","+    ","+    # Configure specific loggers","+    grpc_logger = logging.getLogger('grpc.logging')","+    grpc_logger.setLevel(logging.INFO)","+    grpc_logger.addHandler(request_response_handler)","+    grpc_logger.propagate = False  # Don't duplicate to root logger","+    ","+    # Error logs","+    error_handler = logging.FileHandler('grpc_errors.log')","+    error_handler.setLevel(logging.ERROR)","+    error_handler.setFormatter(detailed_formatter)","+    root_logger.addHandler(error_handler)","+    ","     # Set specific loggers to appropriate levels","     logging.getLogger('sqlalchemy.engine').setLevel(logging.WARNING)  # Reduce SQL noise","+    logging.getLogger('sqlalchemy.pool').setLevel(logging.WARNING)","+    logging.getLogger('asyncio').setLevel(logging.WARNING)","+    logging.getLogger('grpc._server').setLevel(logging.WARNING)","     ","+    # Enable detailed logging for our application","+    logging.getLogger('src').setLevel(logging.DEBUG)","+    ","+    logging.info(\"Enhanced logging configuration complete\")","+    logging.info(\"Logs will be written to:\")","+    logging.info(\"  - grpc_server.log: All application logs\") ","+    logging.info(\"  - grpc_requests.log: Request/response logs\")","+    logging.info(\"  - grpc_errors.log: Error logs only\")","+    ","     # Initialize DI container","     container = Container()","     await container.init_resources()"]}],"userModified":false,"replaceAll":false}}
{"parentUuid":"49673556-5ba7-49c9-be81-0c98c116b1ff","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01FDeDhjRdF1JUcasnpamWjh","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Tôi cũng cần tạo một structured JSON formatter để log dữ liệu có cấu trúc:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":2019,"cache_read_input_tokens":71055,"cache_creation":{"ephemeral_5m_input_tokens":2019,"ephemeral_1h_input_tokens":0},"output_tokens":2,"service_tier":"standard"}},"requestId":"req_011CSJ8nvt83qyWamGRCM9AM","type":"assistant","uuid":"a09398c8-c9f5-46f8-97b0-5e38854986f8","timestamp":"2025-08-20T01:59:48.246Z"}
{"parentUuid":"a09398c8-c9f5-46f8-97b0-5e38854986f8","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01FDeDhjRdF1JUcasnpamWjh","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_017RzTg86y85xBgEKic48MLh","name":"Edit","input":{"file_path":"/Users/lee/Documents/src/bep-oms-broker/main.py","old_string":"import grpc\nimport logging\nimport sys\nimport os\nimport asyncio\nfrom dotenv import load_dotenv\n\nload_dotenv()","new_string":"import grpc\nimport logging\nimport sys\nimport os\nimport asyncio\nimport json\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\nclass StructuredJSONFormatter(logging.Formatter):\n    \"\"\"JSON formatter for structured logging with extra data.\"\"\"\n    \n    def format(self, record):\n        log_obj = {\n            'timestamp': self.formatTime(record),\n            'level': record.levelname,\n            'logger': record.name,\n            'message': record.getMessage(),\n            'module': record.module,\n            'function': record.funcName,\n            'line': record.lineno\n        }\n        \n        # Add structured data if present\n        if hasattr(record, 'structured_data'):\n            log_obj.update(record.structured_data)\n        \n        # Add exception info if present\n        if record.exc_info:\n            log_obj['exception'] = {\n                'type': record.exc_info[0].__name__,\n                'message': str(record.exc_info[1]),\n                'traceback': self.formatException(record.exc_info)\n            }\n        \n        return json.dumps(log_obj, ensure_ascii=False, default=str)"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":2019,"cache_read_input_tokens":71055,"cache_creation":{"ephemeral_5m_input_tokens":2019,"ephemeral_1h_input_tokens":0},"output_tokens":462,"service_tier":"standard"}},"requestId":"req_011CSJ8nvt83qyWamGRCM9AM","type":"assistant","uuid":"5a87d28d-3266-40d1-af19-80d993c878a5","timestamp":"2025-08-20T01:59:52.128Z"}
{"parentUuid":"5a87d28d-3266-40d1-af19-80d993c878a5","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_017RzTg86y85xBgEKic48MLh","type":"tool_result","content":"The file /Users/lee/Documents/src/bep-oms-broker/main.py has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n     1→import grpc\n     2→import logging\n     3→import sys\n     4→import os\n     5→import asyncio\n     6→import json\n     7→from dotenv import load_dotenv\n     8→\n     9→load_dotenv()\n    10→\n    11→\n    12→class StructuredJSONFormatter(logging.Formatter):\n    13→    \"\"\"JSON formatter for structured logging with extra data.\"\"\"\n    14→    \n    15→    def format(self, record):\n    16→        log_obj = {\n    17→            'timestamp': self.formatTime(record),\n    18→            'level': record.levelname,\n    19→            'logger': record.name,\n    20→            'message': record.getMessage(),\n    21→            'module': record.module,\n    22→            'function': record.funcName,\n    23→            'line': record.lineno\n    24→        }\n    25→        \n    26→        # Add structured data if present\n    27→        if hasattr(record, 'structured_data'):\n    28→            log_obj.update(record.structured_data)\n    29→        \n    30→        # Add exception info if present\n    31→        if record.exc_info:\n    32→            log_obj['exception'] = {\n    33→                'type': record.exc_info[0].__name__,\n    34→                'message': str(record.exc_info[1]),\n    35→                'traceback': self.formatException(record.exc_info)\n    36→            }\n    37→        \n    38→        return json.dumps(log_obj, ensure_ascii=False, default=str)\n    39→\n    40→sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), 'src')))\n    41→\n    42→from src.infrastructure.di.container import Container"}]},"uuid":"651b9ebf-939e-42c7-b9d2-b87cf9fb6479","timestamp":"2025-08-20T02:00:11.324Z","toolUseResult":{"filePath":"/Users/lee/Documents/src/bep-oms-broker/main.py","oldString":"import grpc\nimport logging\nimport sys\nimport os\nimport asyncio\nfrom dotenv import load_dotenv\n\nload_dotenv()","newString":"import grpc\nimport logging\nimport sys\nimport os\nimport asyncio\nimport json\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\nclass StructuredJSONFormatter(logging.Formatter):\n    \"\"\"JSON formatter for structured logging with extra data.\"\"\"\n    \n    def format(self, record):\n        log_obj = {\n            'timestamp': self.formatTime(record),\n            'level': record.levelname,\n            'logger': record.name,\n            'message': record.getMessage(),\n            'module': record.module,\n            'function': record.funcName,\n            'line': record.lineno\n        }\n        \n        # Add structured data if present\n        if hasattr(record, 'structured_data'):\n            log_obj.update(record.structured_data)\n        \n        # Add exception info if present\n        if record.exc_info:\n            log_obj['exception'] = {\n                'type': record.exc_info[0].__name__,\n                'message': str(record.exc_info[1]),\n                'traceback': self.formatException(record.exc_info)\n            }\n        \n        return json.dumps(log_obj, ensure_ascii=False, default=str)","originalFile":"import grpc\nimport logging\nimport sys\nimport os\nimport asyncio\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), 'src')))\n\nfrom src.infrastructure.di.container import Container\nfrom src.presentation.grpc.server_config import EnhancedGrpcServerConfig\nfrom src.application.services.order_application_service import OrderApplicationService\nfrom src.presentation.grpc.servicers.order_servicer import OrderServicer\nfrom src.infrastructure.config.aws_config import AWSConfig\nfrom src.infrastructure.adapters.email.aws_ses_service import AWSSESService\nfrom src.infrastructure.persistence.repositories.otp_repository import OTPRepository as OTPRepositoryImpl\nfrom src.application.use_cases.otp.send_otp_email_use_case import SendOTPEmailUseCase\nfrom src.application.use_cases.otp.verify_otp_use_case import VerifyOTPUseCase\nfrom src.application.use_cases.otp.verify_registration_use_case import VerifyRegistrationUseCase\nfrom src.application.use_cases.otp.change_password_use_cases import (\n    RequestPasswordResetUseCase,\n    ConfirmPasswordResetUseCase,\n)\nfrom src.infrastructure.persistence.async_database import get_async_session\n\nasync def main():\n    # Configure structured logging with detailed format\n    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'\n    \n    # Create formatters\n    detailed_formatter = logging.Formatter(log_format)\n    json_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n    \n    # Configure root logger\n    root_logger = logging.getLogger()\n    root_logger.setLevel(logging.DEBUG)\n    \n    # Clear existing handlers\n    for handler in root_logger.handlers[:]:\n        root_logger.removeHandler(handler)\n    \n    # Console handler with structured format\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(logging.INFO)\n    console_handler.setFormatter(detailed_formatter)\n    root_logger.addHandler(console_handler)\n    \n    # File handler for all logs\n    file_handler = logging.FileHandler('grpc_server.log')\n    file_handler.setLevel(logging.DEBUG)\n    file_handler.setFormatter(detailed_formatter)\n    root_logger.addHandler(file_handler)\n    \n    # Separate file for request/response logs\n    request_response_handler = logging.FileHandler('grpc_requests.log')\n    request_response_handler.setLevel(logging.INFO)\n    request_response_handler.setFormatter(json_formatter)\n    \n    # Configure specific loggers\n    grpc_logger = logging.getLogger('grpc.logging')\n    grpc_logger.setLevel(logging.INFO)\n    grpc_logger.addHandler(request_response_handler)\n    grpc_logger.propagate = False  # Don't duplicate to root logger\n    \n    # Error logs\n    error_handler = logging.FileHandler('grpc_errors.log')\n    error_handler.setLevel(logging.ERROR)\n    error_handler.setFormatter(detailed_formatter)\n    root_logger.addHandler(error_handler)\n    \n    # Set specific loggers to appropriate levels\n    logging.getLogger('sqlalchemy.engine').setLevel(logging.WARNING)  # Reduce SQL noise\n    logging.getLogger('sqlalchemy.pool').setLevel(logging.WARNING)\n    logging.getLogger('asyncio').setLevel(logging.WARNING)\n    logging.getLogger('grpc._server').setLevel(logging.WARNING)\n    \n    # Enable detailed logging for our application\n    logging.getLogger('src').setLevel(logging.DEBUG)\n    \n    logging.info(\"Enhanced logging configuration complete\")\n    logging.info(\"Logs will be written to:\")\n    logging.info(\"  - grpc_server.log: All application logs\") \n    logging.info(\"  - grpc_requests.log: Request/response logs\")\n    logging.info(\"  - grpc_errors.log: Error logs only\")\n    \n    # Initialize DI container\n    container = Container()\n    await container.init_resources()\n    container.wire(modules=[\"src.presentation.grpc.servicers\", \"src.application.services\"])\n\n    # Create enhanced server with authentication enabled\n    server_config = EnhancedGrpcServerConfig(\n        port=55051,\n        max_workers=10,\n        enable_reflection=True,\n        enable_health_checks=True,\n        enable_metrics=True,\n        enable_authentication=True,\n        enable_rate_limiting=False,\n        enable_detailed_logging=True,\n        jwt_secret_key=os.getenv(\"TOKEN_ENCRYPTION_KEY\", \"super-secret-key\")\n    )\n    \n    # Helper to resolve possibly awaitable providers\n    import inspect\n    async def _resolve(maybe):\n        return await maybe if inspect.isawaitable(maybe) else maybe\n\n    # Configure server dependencies first\n    # Prepare OTP dependencies (async session)\n    send_otp_uc = None\n    verify_otp_uc = None\n    verify_registration_uc = None\n    request_change_password_uc = None\n    confirm_change_password_uc = None\n    try:\n        aws_config = AWSConfig()\n        ses_service = AWSSESService(aws_config)\n        # Pass session factory callable to repository to avoid closing issues\n        otp_repo = OTPRepositoryImpl(get_async_session)\n        send_otp_uc = SendOTPEmailUseCase(ses_service, otp_repo)\n        verify_otp_uc = VerifyOTPUseCase(otp_repo)\n        # Resolve providers to concrete instances to avoid passing awaitables\n        user_repo = await _resolve(container.user_repository())\n        password_hasher = await _resolve(container.password_hasher())\n\n        verify_registration_uc = VerifyRegistrationUseCase(\n            user_repository=user_repo,\n            verify_otp_use_case=verify_otp_uc,\n            otp_repository=otp_repo,\n        )\n        request_change_password_uc = RequestPasswordResetUseCase(\n            user_repository=user_repo,\n            send_otp_email_use_case=send_otp_uc,\n        )\n        confirm_change_password_uc = ConfirmPasswordResetUseCase(\n            otp_repository=otp_repo,\n            verify_otp_use_case=verify_otp_uc,\n            user_repository=user_repo,\n            password_hasher=password_hasher,\n        )\n        logging.info(\"OTP email feature enabled (AWS SES configured)\")\n    except Exception as e:\n        logging.warning(f\"OTP email feature disabled: {e}\")\n\n    user_app = await _resolve(container.user_application_service())\n    broker_app = await _resolve(container.broker_account_application_service())\n    market_data_app = await _resolve(container.market_data_application_service())\n    order_app = await _resolve(container.order_application_service())\n    \n    # Portfolio dependencies\n    get_portfolio_uc = await _resolve(container.get_portfolio_use_case())\n    portfolio_domain_mapper = await _resolve(container.portfolio_domain_mapper())\n\n    server_config.configure_dependencies(\n        user_application_service=user_app,\n        broker_account_application_service=broker_app,\n        market_data_application_service=market_data_app,\n        order_application_service=order_app,\n        use_cases={\n            # User use cases for consistent fallback paths across flows\n            'register_user': await _resolve(container.register_user_use_case()),\n            'update_user_profile': await _resolve(container.update_user_profile_use_case()),\n            'deactivate_user': await _resolve(container.deactivate_user_use_case()),\n            'login_user': await _resolve(container.login_user_use_case()),\n            'logout_user': await _resolve(container.logout_user_use_case()),\n            # OTP flows\n            'send_otp_email': send_otp_uc,\n            'verify_otp': verify_otp_uc,\n            'verify_registration': verify_registration_uc,\n            'request_password_reset': request_change_password_uc,\n            'confirm_password_reset': confirm_change_password_uc,\n        },\n        repositories={}\n    )\n    \n    # Set portfolio dependencies manually\n    server_config.get_portfolio_use_case = get_portfolio_uc\n    server_config.portfolio_domain_mapper = portfolio_domain_mapper\n    \n    # Configure the server\n    server_config.server = server_config.configure_server()\n    \n    # Start enhanced server\n    print(\"Starting enhanced gRPC server on port 55051...\")\n    print(\"Features enabled: Enhanced Servicers, Interceptors, Health Checks, Metrics\")\n    logging.info(\"Enhanced gRPC server starting with Phase 3 improvements\")\n    \n    await server_config.start_server()\n    await server_config.server.wait_for_termination()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())","structuredPatch":[{"oldStart":3,"oldLines":10,"newStart":3,"newLines":40,"lines":[" import sys"," import os"," import asyncio","+import json"," from dotenv import load_dotenv"," "," load_dotenv()"," ","+","+class StructuredJSONFormatter(logging.Formatter):","+    \"\"\"JSON formatter for structured logging with extra data.\"\"\"","+    ","+    def format(self, record):","+        log_obj = {","+            'timestamp': self.formatTime(record),","+            'level': record.levelname,","+            'logger': record.name,","+            'message': record.getMessage(),","+            'module': record.module,","+            'function': record.funcName,","+            'line': record.lineno","+        }","+        ","+        # Add structured data if present","+        if hasattr(record, 'structured_data'):","+            log_obj.update(record.structured_data)","+        ","+        # Add exception info if present","+        if record.exc_info:","+            log_obj['exception'] = {","+                'type': record.exc_info[0].__name__,","+                'message': str(record.exc_info[1]),","+                'traceback': self.formatException(record.exc_info)","+            }","+        ","+        return json.dumps(log_obj, ensure_ascii=False, default=str)","+"," sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), 'src')))"," "," from src.infrastructure.di.container import Container"]}],"userModified":false,"replaceAll":false}}
{"parentUuid":"651b9ebf-939e-42c7-b9d2-b87cf9fb6479","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01E1Bdqku7LV5L9YwkDDuLEN","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Bây giờ tôi cần cập nhật cấu hình logging để sử dụng StructuredJSONFormatter:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":989,"cache_read_input_tokens":73074,"cache_creation":{"ephemeral_5m_input_tokens":989,"ephemeral_1h_input_tokens":0},"output_tokens":2,"service_tier":"standard"}},"requestId":"req_011CSJ8q6xAFqKBtDmjL1Qw7","type":"assistant","uuid":"722f2f3e-be63-491e-8107-784f8e665eed","timestamp":"2025-08-20T02:00:16.596Z"}
{"parentUuid":"722f2f3e-be63-491e-8107-784f8e665eed","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01E1Bdqku7LV5L9YwkDDuLEN","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01RzNEgSuc2BmAEiUgvMMkbH","name":"Edit","input":{"file_path":"/Users/lee/Documents/src/bep-oms-broker/main.py","old_string":"    # Create formatters\n    detailed_formatter = logging.Formatter(log_format)\n    json_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')","new_string":"    # Create formatters\n    detailed_formatter = logging.Formatter(log_format)\n    json_formatter = StructuredJSONFormatter()"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":989,"cache_read_input_tokens":73074,"cache_creation":{"ephemeral_5m_input_tokens":989,"ephemeral_1h_input_tokens":0},"output_tokens":224,"service_tier":"standard"}},"requestId":"req_011CSJ8q6xAFqKBtDmjL1Qw7","type":"assistant","uuid":"6b9edcab-506e-4735-84fa-c703e523f82c","timestamp":"2025-08-20T02:00:17.875Z"}
{"parentUuid":"6b9edcab-506e-4735-84fa-c703e523f82c","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01RzNEgSuc2BmAEiUgvMMkbH","type":"tool_result","content":"The file /Users/lee/Documents/src/bep-oms-broker/main.py has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n    58→async def main():\n    59→    # Configure structured logging with detailed format\n    60→    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'\n    61→    \n    62→    # Create formatters\n    63→    detailed_formatter = logging.Formatter(log_format)\n    64→    json_formatter = StructuredJSONFormatter()\n    65→    \n    66→    # Configure root logger\n    67→    root_logger = logging.getLogger()\n    68→    root_logger.setLevel(logging.DEBUG)"}]},"uuid":"520eb9b4-0ce6-45e3-a44d-b62ad2802c05","timestamp":"2025-08-20T02:00:39.318Z","toolUseResult":{"filePath":"/Users/lee/Documents/src/bep-oms-broker/main.py","oldString":"    # Create formatters\n    detailed_formatter = logging.Formatter(log_format)\n    json_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')","newString":"    # Create formatters\n    detailed_formatter = logging.Formatter(log_format)\n    json_formatter = StructuredJSONFormatter()","originalFile":"import grpc\nimport logging\nimport sys\nimport os\nimport asyncio\nimport json\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\nclass StructuredJSONFormatter(logging.Formatter):\n    \"\"\"JSON formatter for structured logging with extra data.\"\"\"\n    \n    def format(self, record):\n        log_obj = {\n            'timestamp': self.formatTime(record),\n            'level': record.levelname,\n            'logger': record.name,\n            'message': record.getMessage(),\n            'module': record.module,\n            'function': record.funcName,\n            'line': record.lineno\n        }\n        \n        # Add structured data if present\n        if hasattr(record, 'structured_data'):\n            log_obj.update(record.structured_data)\n        \n        # Add exception info if present\n        if record.exc_info:\n            log_obj['exception'] = {\n                'type': record.exc_info[0].__name__,\n                'message': str(record.exc_info[1]),\n                'traceback': self.formatException(record.exc_info)\n            }\n        \n        return json.dumps(log_obj, ensure_ascii=False, default=str)\n\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), 'src')))\n\nfrom src.infrastructure.di.container import Container\nfrom src.presentation.grpc.server_config import EnhancedGrpcServerConfig\nfrom src.application.services.order_application_service import OrderApplicationService\nfrom src.presentation.grpc.servicers.order_servicer import OrderServicer\nfrom src.infrastructure.config.aws_config import AWSConfig\nfrom src.infrastructure.adapters.email.aws_ses_service import AWSSESService\nfrom src.infrastructure.persistence.repositories.otp_repository import OTPRepository as OTPRepositoryImpl\nfrom src.application.use_cases.otp.send_otp_email_use_case import SendOTPEmailUseCase\nfrom src.application.use_cases.otp.verify_otp_use_case import VerifyOTPUseCase\nfrom src.application.use_cases.otp.verify_registration_use_case import VerifyRegistrationUseCase\nfrom src.application.use_cases.otp.change_password_use_cases import (\n    RequestPasswordResetUseCase,\n    ConfirmPasswordResetUseCase,\n)\nfrom src.infrastructure.persistence.async_database import get_async_session\n\nasync def main():\n    # Configure structured logging with detailed format\n    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'\n    \n    # Create formatters\n    detailed_formatter = logging.Formatter(log_format)\n    json_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n    \n    # Configure root logger\n    root_logger = logging.getLogger()\n    root_logger.setLevel(logging.DEBUG)\n    \n    # Clear existing handlers\n    for handler in root_logger.handlers[:]:\n        root_logger.removeHandler(handler)\n    \n    # Console handler with structured format\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(logging.INFO)\n    console_handler.setFormatter(detailed_formatter)\n    root_logger.addHandler(console_handler)\n    \n    # File handler for all logs\n    file_handler = logging.FileHandler('grpc_server.log')\n    file_handler.setLevel(logging.DEBUG)\n    file_handler.setFormatter(detailed_formatter)\n    root_logger.addHandler(file_handler)\n    \n    # Separate file for request/response logs\n    request_response_handler = logging.FileHandler('grpc_requests.log')\n    request_response_handler.setLevel(logging.INFO)\n    request_response_handler.setFormatter(json_formatter)\n    \n    # Configure specific loggers\n    grpc_logger = logging.getLogger('grpc.logging')\n    grpc_logger.setLevel(logging.INFO)\n    grpc_logger.addHandler(request_response_handler)\n    grpc_logger.propagate = False  # Don't duplicate to root logger\n    \n    # Error logs\n    error_handler = logging.FileHandler('grpc_errors.log')\n    error_handler.setLevel(logging.ERROR)\n    error_handler.setFormatter(detailed_formatter)\n    root_logger.addHandler(error_handler)\n    \n    # Set specific loggers to appropriate levels\n    logging.getLogger('sqlalchemy.engine').setLevel(logging.WARNING)  # Reduce SQL noise\n    logging.getLogger('sqlalchemy.pool').setLevel(logging.WARNING)\n    logging.getLogger('asyncio').setLevel(logging.WARNING)\n    logging.getLogger('grpc._server').setLevel(logging.WARNING)\n    \n    # Enable detailed logging for our application\n    logging.getLogger('src').setLevel(logging.DEBUG)\n    \n    logging.info(\"Enhanced logging configuration complete\")\n    logging.info(\"Logs will be written to:\")\n    logging.info(\"  - grpc_server.log: All application logs\") \n    logging.info(\"  - grpc_requests.log: Request/response logs\")\n    logging.info(\"  - grpc_errors.log: Error logs only\")\n    \n    # Initialize DI container\n    container = Container()\n    await container.init_resources()\n    container.wire(modules=[\"src.presentation.grpc.servicers\", \"src.application.services\"])\n\n    # Create enhanced server with authentication enabled\n    server_config = EnhancedGrpcServerConfig(\n        port=55051,\n        max_workers=10,\n        enable_reflection=True,\n        enable_health_checks=True,\n        enable_metrics=True,\n        enable_authentication=True,\n        enable_rate_limiting=False,\n        enable_detailed_logging=True,\n        jwt_secret_key=os.getenv(\"TOKEN_ENCRYPTION_KEY\", \"super-secret-key\")\n    )\n    \n    # Helper to resolve possibly awaitable providers\n    import inspect\n    async def _resolve(maybe):\n        return await maybe if inspect.isawaitable(maybe) else maybe\n\n    # Configure server dependencies first\n    # Prepare OTP dependencies (async session)\n    send_otp_uc = None\n    verify_otp_uc = None\n    verify_registration_uc = None\n    request_change_password_uc = None\n    confirm_change_password_uc = None\n    try:\n        aws_config = AWSConfig()\n        ses_service = AWSSESService(aws_config)\n        # Pass session factory callable to repository to avoid closing issues\n        otp_repo = OTPRepositoryImpl(get_async_session)\n        send_otp_uc = SendOTPEmailUseCase(ses_service, otp_repo)\n        verify_otp_uc = VerifyOTPUseCase(otp_repo)\n        # Resolve providers to concrete instances to avoid passing awaitables\n        user_repo = await _resolve(container.user_repository())\n        password_hasher = await _resolve(container.password_hasher())\n\n        verify_registration_uc = VerifyRegistrationUseCase(\n            user_repository=user_repo,\n            verify_otp_use_case=verify_otp_uc,\n            otp_repository=otp_repo,\n        )\n        request_change_password_uc = RequestPasswordResetUseCase(\n            user_repository=user_repo,\n            send_otp_email_use_case=send_otp_uc,\n        )\n        confirm_change_password_uc = ConfirmPasswordResetUseCase(\n            otp_repository=otp_repo,\n            verify_otp_use_case=verify_otp_uc,\n            user_repository=user_repo,\n            password_hasher=password_hasher,\n        )\n        logging.info(\"OTP email feature enabled (AWS SES configured)\")\n    except Exception as e:\n        logging.warning(f\"OTP email feature disabled: {e}\")\n\n    user_app = await _resolve(container.user_application_service())\n    broker_app = await _resolve(container.broker_account_application_service())\n    market_data_app = await _resolve(container.market_data_application_service())\n    order_app = await _resolve(container.order_application_service())\n    \n    # Portfolio dependencies\n    get_portfolio_uc = await _resolve(container.get_portfolio_use_case())\n    portfolio_domain_mapper = await _resolve(container.portfolio_domain_mapper())\n\n    server_config.configure_dependencies(\n        user_application_service=user_app,\n        broker_account_application_service=broker_app,\n        market_data_application_service=market_data_app,\n        order_application_service=order_app,\n        use_cases={\n            # User use cases for consistent fallback paths across flows\n            'register_user': await _resolve(container.register_user_use_case()),\n            'update_user_profile': await _resolve(container.update_user_profile_use_case()),\n            'deactivate_user': await _resolve(container.deactivate_user_use_case()),\n            'login_user': await _resolve(container.login_user_use_case()),\n            'logout_user': await _resolve(container.logout_user_use_case()),\n            # OTP flows\n            'send_otp_email': send_otp_uc,\n            'verify_otp': verify_otp_uc,\n            'verify_registration': verify_registration_uc,\n            'request_password_reset': request_change_password_uc,\n            'confirm_password_reset': confirm_change_password_uc,\n        },\n        repositories={}\n    )\n    \n    # Set portfolio dependencies manually\n    server_config.get_portfolio_use_case = get_portfolio_uc\n    server_config.portfolio_domain_mapper = portfolio_domain_mapper\n    \n    # Configure the server\n    server_config.server = server_config.configure_server()\n    \n    # Start enhanced server\n    print(\"Starting enhanced gRPC server on port 55051...\")\n    print(\"Features enabled: Enhanced Servicers, Interceptors, Health Checks, Metrics\")\n    logging.info(\"Enhanced gRPC server starting with Phase 3 improvements\")\n    \n    await server_config.start_server()\n    await server_config.server.wait_for_termination()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())","structuredPatch":[{"oldStart":61,"oldLines":7,"newStart":61,"newLines":7,"lines":["     ","     # Create formatters","     detailed_formatter = logging.Formatter(log_format)","-    json_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')","+    json_formatter = StructuredJSONFormatter()","     ","     # Configure root logger","     root_logger = logging.getLogger()"]}],"userModified":false,"replaceAll":false}}
{"parentUuid":"520eb9b4-0ce6-45e3-a44d-b62ad2802c05","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01Gma8EYbT4tBBhaiT6JRYxb","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01XeWoBPeU9LaYBPUowhPyfr","name":"TodoWrite","input":{"todos":[{"id":"1","content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"completed","priority":"high"},{"id":"2","content":"Tạo interceptor gRPC để log request/response","status":"completed","priority":"high"},{"id":"3","content":"Thêm logging vào các servicer chính","status":"completed","priority":"high"},{"id":"4","content":"Cấu hình log format và level phù hợp","status":"completed","priority":"medium"},{"id":"5","content":"Kiểm tra và test cấu hình logging","status":"pending","priority":"low"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":437,"cache_read_input_tokens":74063,"cache_creation":{"ephemeral_5m_input_tokens":437,"ephemeral_1h_input_tokens":0},"output_tokens":335,"service_tier":"standard"}},"requestId":"req_011CSJ8sAkRC5e15nAxnypbm","type":"assistant","uuid":"4efedbe8-ee71-4ef6-bd75-bcc377038c33","timestamp":"2025-08-20T02:00:48.119Z"}
{"parentUuid":"4efedbe8-ee71-4ef6-bd75-bcc377038c33","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01XeWoBPeU9LaYBPUowhPyfr","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"a2a35501-45bc-401d-9cb1-3aaed47df57a","timestamp":"2025-08-20T02:00:48.167Z","toolUseResult":{"oldTodos":[{"content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"completed","priority":"high","id":"1"},{"content":"Tạo interceptor gRPC để log request/response","status":"completed","priority":"high","id":"2"},{"content":"Thêm logging vào các servicer chính","status":"completed","priority":"high","id":"3"},{"content":"Cấu hình log format và level phù hợp","status":"in_progress","priority":"medium","id":"4"}],"newTodos":[{"content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"completed","priority":"high","id":"1"},{"content":"Tạo interceptor gRPC để log request/response","status":"completed","priority":"high","id":"2"},{"content":"Thêm logging vào các servicer chính","status":"completed","priority":"high","id":"3"},{"content":"Cấu hình log format và level phù hợp","status":"completed","priority":"medium","id":"4"},{"content":"Kiểm tra và test cấu hình logging","status":"pending","priority":"low","id":"5"}]}}
{"parentUuid":"a2a35501-45bc-401d-9cb1-3aaed47df57a","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01TTkqGpWSG7cSMZT7dvvqbJ","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Bây giờ tôi sẽ kiểm tra type checking và formatting để đảm bảo code chạy tốt:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":581,"cache_read_input_tokens":74500,"cache_creation":{"ephemeral_5m_input_tokens":581,"ephemeral_1h_input_tokens":0},"output_tokens":2,"service_tier":"standard"}},"requestId":"req_011CSJ8sobxTeR7RDr4QzrX5","type":"assistant","uuid":"14aa09ce-5a70-4081-9e2f-426b3c9c36a2","timestamp":"2025-08-20T02:00:53.133Z"}
{"parentUuid":"14aa09ce-5a70-4081-9e2f-426b3c9c36a2","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01TTkqGpWSG7cSMZT7dvvqbJ","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01VpwNkyqsMHjCq2vnpM67pr","name":"Bash","input":{"command":"poetry run ruff check src/presentation/grpc/interceptors/logging_interceptor.py","description":"Check ruff linting for logging interceptor"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":581,"cache_read_input_tokens":74500,"cache_creation":{"ephemeral_5m_input_tokens":581,"ephemeral_1h_input_tokens":0},"output_tokens":152,"service_tier":"standard"}},"requestId":"req_011CSJ8sobxTeR7RDr4QzrX5","type":"assistant","uuid":"a5b12104-5711-4de1-87ca-d4c054dbd368","timestamp":"2025-08-20T02:00:54.096Z"}
{"parentUuid":"a5b12104-5711-4de1-87ca-d4c054dbd368","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"type":"tool_result","content":"The user doesn't want to proceed with this tool use. The tool use was rejected (eg. if it was a file edit, the new_string was NOT written to the file). STOP what you are doing and wait for the user to tell you how to proceed.","is_error":true,"tool_use_id":"toolu_01VpwNkyqsMHjCq2vnpM67pr"}]},"uuid":"e9f50629-f7e2-4455-94c3-f8314f716faf","timestamp":"2025-08-20T02:01:45.350Z","toolUseResult":"Error: The user doesn't want to proceed with this tool use. The tool use was rejected (eg. if it was a file edit, the new_string was NOT written to the file). STOP what you are doing and wait for the user to tell you how to proceed."}
{"parentUuid":"e9f50629-f7e2-4455-94c3-f8314f716faf","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"type":"text","text":"[Request interrupted by user for tool use]"}]},"uuid":"fd24a8ce-6353-498a-82f4-62e1c04a3fa8","timestamp":"2025-08-20T02:01:45.380Z"}
{"parentUuid":"fd24a8ce-6353-498a-82f4-62e1c04a3fa8","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":"2025-08-20 09:01:15,192 - src.presentation.grpc.server_config - INFO - _log_server_status:464 - Server Status: {'server_address': '0.0.0.0:55051', 'max_workers': 10, 'features': {'authentication': True, 'rate_limiting': False, 'detailed_logging': True, 'health_checks': True, 'metrics': True, 'reflection': True}, 'services': ['UserService', 'BrokerAccountService', 'MarketDataService', 'OrderService', 'HealthService']}\n2025-08-20 09:01:22,677 - grpc.requests - INFO - _log_request:283 - [364a6929-2b05-4230-bbbe-bcd6a86a26e9] gRPC Request: /order.OrderService/CreateOrder\n2025-08-20 09:01:22,680 - src.presentation.grpc.servicers.order_servicer - INFO - _request_context:61 - [ca67759c-76f6-4388-b1bc-0380cb67736f] Starting CreateOrder\n2025-08-20 09:01:22,682 - src.presentation.grpc.servicers.order_servicer - WARNING - CreateOrder:320 - [ca67759c-76f6-4388-b1bc-0380cb67736f] Validation error in CreateOrder: MISSING_SYMBOL: Symbol is required\n2025-08-20 09:01:22,683 - src.presentation.grpc.servicers.order_servicer - INFO - _request_context:73 - [ca67759c-76f6-4388-b1bc-0380cb67736f] Completed CreateOrder in 0.003s\n2025-08-20 09:01:22,684 - grpc.requests - INFO - _log_response:311 - [364a6929-2b05-4230-bbbe-bcd6a86a26e9] gRPC Response: /order.OrderService/CreateOrder (6.37ms)\nToi da kiem tra nhung van khong thay log request, response. Toi chi muon don gian luc nhan request thì ghi log request duoi dang json string. truoc luc tra ve ket qua cho client thì ghi log response duoi dang json string"},"uuid":"225cf430-116f-40c2-86a9-262d7d4ec7a7","timestamp":"2025-08-20T02:03:04.216Z"}
{"parentUuid":"225cf430-116f-40c2-86a9-262d7d4ec7a7","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01EnyfR3oMhT9dvb8VFhiK3o","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Tôi hiểu rồi. Bạn muốn có logging đơn giản hơn - chỉ log request dưới dạng JSON string khi nhận request và log response dưới dạng JSON string trước khi trả về client. Hãy tôi sửa lại interceptor để đơn giản hơn:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":1329,"cache_read_input_tokens":75081,"cache_creation":{"ephemeral_5m_input_tokens":1329,"ephemeral_1h_input_tokens":0},"output_tokens":5,"service_tier":"standard"}},"requestId":"req_011CSJ93pN8XcsEYRUBjsded","type":"assistant","uuid":"ae4ab72f-bd20-4161-8d77-daf00d55c3ef","timestamp":"2025-08-20T02:03:10.487Z"}
{"parentUuid":"ae4ab72f-bd20-4161-8d77-daf00d55c3ef","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01EnyfR3oMhT9dvb8VFhiK3o","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01FeFSyVtBtmLr7GAgbgMtBU","name":"Edit","input":{"file_path":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py","old_string":"    def _log_request(\n        self, \n        method_name: str, \n        request: Any, \n        correlation_id: str, \n        context: ServicerContext\n    ):\n        \"\"\"Log incoming request with structured data.\"\"\"\n        \n        # Extract client information\n        peer = context.peer() if hasattr(context, 'peer') else 'unknown'\n        \n        # Sanitize request data\n        request_data = self._sanitize_data(self._extract_request_data(request))\n        \n        log_data = {\n            'event': 'grpc_request',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'peer': peer,\n            'request_data': request_data,\n            'timestamp': time.time()\n        }\n        \n        self.logger.log(\n            self.log_level,\n            f\"[{correlation_id}] gRPC Request: {method_name}\",\n            extra={'structured_data': log_data}\n        )","new_string":"    def _log_request(\n        self, \n        method_name: str, \n        request: Any, \n        correlation_id: str, \n        context: ServicerContext\n    ):\n        \"\"\"Log incoming request as JSON string.\"\"\"\n        try:\n            # Extract and sanitize request data\n            request_data = self._sanitize_data(self._extract_request_data(request))\n            \n            # Create simple log object\n            log_obj = {\n                'type': 'REQUEST',\n                'method': method_name,\n                'correlation_id': correlation_id,\n                'timestamp': time.time(),\n                'data': request_data\n            }\n            \n            # Log as JSON string\n            self.logger.info(f\"REQUEST: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log request for {method_name}: {e}\")"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":1329,"cache_read_input_tokens":75081,"cache_creation":{"ephemeral_5m_input_tokens":1329,"ephemeral_1h_input_tokens":0},"output_tokens":692,"service_tier":"standard"}},"requestId":"req_011CSJ93pN8XcsEYRUBjsded","type":"assistant","uuid":"b4fa66af-7d95-4627-bf22-f274796dab07","timestamp":"2025-08-20T02:03:16.401Z"}
{"parentUuid":"b4fa66af-7d95-4627-bf22-f274796dab07","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01FeFSyVtBtmLr7GAgbgMtBU","type":"tool_result","content":"The file /Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n     1→\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n     2→\n     3→This interceptor provides:\n     4→- Detailed request/response logging\n     5→- Performance metrics tracking\n     6→- Error logging and analysis\n     7→- Request correlation and tracing\n     8→- Structured logging with context\n     9→\"\"\"\n    10→import json\n    11→import logging\n    12→import time\n    13→from typing import Any, Callable, Iterator, Optional, Dict\n    14→from uuid import uuid4\n    15→\n    16→import grpc\n    17→import grpc.aio\n    18→from grpc.aio import ServerInterceptor, ServicerContext\n    19→\n    20→\n    21→class LoggingInterceptor(grpc.aio.ServerInterceptor):\n    22→    \"\"\"Advanced logging interceptor for gRPC services.\n    23→    \n    24→    Features:\n    25→    - Request/response logging with structured data\n    26→    - Performance metrics and timing\n    27→    - Error tracking and analysis\n    28→    - Request correlation IDs\n    29→    - Configurable log levels and filters\n    30→    - Security-aware logging (excludes sensitive data)\n    31→    \"\"\"\n    32→    \n    33→    def __init__(\n    34→        self,\n    35→        logger: Optional[logging.Logger] = None,\n    36→        log_level: int = logging.INFO,\n    37→        log_requests: bool = True,\n    38→        log_responses: bool = True,\n    39→        log_errors: bool = True,\n    40→        log_performance: bool = True,\n    41→        exclude_methods: Optional[list] = None,\n    42→        sensitive_fields: Optional[list] = None\n    43→    ):\n    44→        self.logger = logger or logging.getLogger(__name__)\n    45→        self.log_level = log_level\n    46→        self.log_requests = log_requests\n    47→        self.log_responses = log_responses\n    48→        self.log_errors = log_errors\n    49→        self.log_performance = log_performance\n    50→        self.exclude_methods = exclude_methods or []\n    51→        self.sensitive_fields = sensitive_fields or [\n    52→            'password', 'token', 'secret', 'key', 'credential', 'auth'\n    53→        ]\n    54→        \n    55→        # Performance metrics\n    56→        self._metrics = {\n    57→            'total_requests': 0,\n    58→            'successful_requests': 0,\n    59→            'failed_requests': 0,\n    60→            'avg_response_time': 0.0,\n    61→            'method_stats': {}\n    62→        }\n    63→    \n    64→    async def intercept_service(\n    65→        self,\n    66→        continuation: Callable[[Any], Any],\n    67→        handler_call_details: Any,\n    68→    ) -> Any:\n    69→        \"\"\"Intercept service calls for logging.\"\"\"\n    70→        \n    71→        # Skip excluded methods\n    72→        method_name = handler_call_details.method\n    73→        if method_name in self.exclude_methods:\n    74→            return await continuation(handler_call_details)\n    75→        \n    76→        # Get the original handler\n    77→        handler = await continuation(handler_call_details)\n    78→        if handler is None:\n    79→            return None\n    80→        \n    81→        # Wrap the handler based on its type; bypass reflection and health entirely\n    82→        if method_name in {\n    83→            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n    84→            '/grpc.health.v1.Health/Check',\n    85→            '/grpc.health.v1.Health/Watch',\n    86→        }:\n    87→            return handler\n    88→        if handler.unary_unary:\n    89→            return grpc.unary_unary_rpc_method_handler(\n    90→                self._wrap_unary_unary(handler.unary_unary, method_name),\n    91→                request_deserializer=handler.request_deserializer,\n    92→                response_serializer=handler.response_serializer,\n    93→            )\n    94→        elif handler.unary_stream:\n    95→            return grpc.unary_stream_rpc_method_handler(\n    96→                self._wrap_unary_stream(handler.unary_stream, method_name),\n    97→                request_deserializer=handler.request_deserializer,\n    98→                response_serializer=handler.response_serializer,\n    99→            )\n   100→        elif handler.stream_unary:\n   101→            return grpc.stream_unary_rpc_method_handler(\n   102→                self._wrap_stream_unary(handler.stream_unary, method_name),\n   103→                request_deserializer=handler.request_deserializer,\n   104→                response_serializer=handler.response_serializer,\n   105→            )\n   106→        elif handler.stream_stream:\n   107→            return grpc.stream_stream_rpc_method_handler(\n   108→                self._wrap_stream_stream(handler.stream_stream, method_name),\n   109→                request_deserializer=handler.request_deserializer,\n   110→                response_serializer=handler.response_serializer,\n   111→            )\n   112→        else:\n   113→            return handler\n   114→    \n   115→    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n   116→        \"\"\"Wrap unary-unary RPC method.\"\"\"\n   117→        \n   118→        async def wrapper(request: Any, context: ServicerContext) -> Any:\n   119→            return await self._handle_request(\n   120→                behavior, request, context, method_name, is_streaming=False\n   121→            )\n   122→        \n   123→        return wrapper\n   124→    \n   125→    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n   126→        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n   127→        async def wrapper(request: Any, context: ServicerContext):\n   128→            # For streaming responses, return the iterator directly (do not await)\n   129→            try:\n   130→                return behavior(request, context)\n   131→            finally:\n   132→                # Minimal logging to avoid interfering with stream lifecycle\n   133→                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n   134→        return wrapper\n   135→    \n   136→    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n   137→        \"\"\"Wrap stream-unary RPC method.\"\"\"\n   138→        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n   139→            # Request is streaming, response is unary; await the final response\n   140→            try:\n   141→                return await behavior(request_iterator, context)\n   142→            finally:\n   143→                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n   144→        return wrapper\n   145→    \n   146→    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n   147→        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n   148→        async def wrapper(request_iterator, context: ServicerContext):\n   149→            # For bi-di streaming, return the response iterator directly\n   150→            try:\n   151→                return behavior(request_iterator, context)\n   152→            finally:\n   153→                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n   154→        return wrapper\n   155→    \n   156→    async def _handle_request(\n   157→        self, \n   158→        behavior: Callable, \n   159→        request: Any, \n   160→        context: ServicerContext, \n   161→        method_name: str,\n   162→        is_streaming: bool = False\n   163→    ) -> Any:\n   164→        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n   165→        \n   166→        # Generate correlation ID\n   167→        correlation_id = str(uuid4())\n   168→        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n   169→        try:\n   170→            context.set_trailing_metadata([\n   171→                ('correlation_id', correlation_id),\n   172→                ('request_id', correlation_id)\n   173→            ])\n   174→        except Exception:\n   175→            pass\n   176→        \n   177→        start_time = time.time()\n   178→        \n   179→        # Update metrics\n   180→        self._metrics['total_requests'] += 1\n   181→        if method_name not in self._metrics['method_stats']:\n   182→            self._metrics['method_stats'][method_name] = {\n   183→                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n   184→            }\n   185→        self._metrics['method_stats'][method_name]['count'] += 1\n   186→        \n   187→        try:\n   188→            # Log request\n   189→            if self.log_requests:\n   190→                self._log_request(method_name, request, correlation_id, context)\n   191→            \n   192→            # Execute the actual method\n   193→            response = await behavior(request, context)\n   194→            \n   195→            # Calculate duration\n   196→            duration = time.time() - start_time\n   197→            \n   198→            # Update success metrics\n   199→            self._metrics['successful_requests'] += 1\n   200→            self._metrics['method_stats'][method_name]['success'] += 1\n   201→            self._update_timing_metrics(method_name, duration)\n   202→            \n   203→            # Log response\n   204→            if self.log_responses:\n   205→                self._log_response(method_name, response, correlation_id, duration, context)\n   206→            \n   207→            # Log performance\n   208→            if self.log_performance:\n   209→                self._log_performance(method_name, duration, correlation_id)\n   210→            \n   211→            return response\n   212→            \n   213→        except Exception as e:\n   214→            # Calculate duration\n   215→            duration = time.time() - start_time\n   216→            \n   217→            # Update error metrics\n   218→            self._metrics['failed_requests'] += 1\n   219→            self._metrics['method_stats'][method_name]['error'] += 1\n   220→            self._update_timing_metrics(method_name, duration)\n   221→            \n   222→            # Log error\n   223→            if self.log_errors:\n   224→                self._log_error(method_name, e, correlation_id, duration, context)\n   225→            \n   226→            # Re-raise the exception\n   227→            raise\n   228→    \n   229→    async def _handle_streaming_request(\n   230→        self,\n   231→        behavior: Callable,\n   232→        request_data: Any,\n   233→        context: ServicerContext,\n   234→        method_name: str,\n   235→        request_streaming: bool = False,\n   236→        response_streaming: bool = True\n   237→    ) -> Any:\n   238→        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n   239→        # For Phase 3, we're focusing on request/response patterns\n   240→        # Streaming would be implemented in a future phase\n   241→        correlation_id = str(uuid4())\n   242→        # Store correlation ID in invocation metadata instead of setting attributes\n   243→        try:\n   244→            context.set_trailing_metadata([\n   245→                ('correlation_id', correlation_id),\n   246→                ('request_id', correlation_id)\n   247→            ])\n   248→        except Exception:\n   249→            # If metadata setting fails, continue without it\n   250→            pass\n   251→        \n   252→        self.logger.info(\n   253→            f\"[{correlation_id}] Streaming method {method_name} called \"\n   254→            f\"(streaming implementation pending)\"\n   255→        )\n   256→        \n   257→        return behavior(request_data, context)\n   258→    \n   259→    def _log_request(\n   260→        self, \n   261→        method_name: str, \n   262→        request: Any, \n   263→        correlation_id: str, \n   264→        context: ServicerContext\n   265→    ):\n   266→        \"\"\"Log incoming request as JSON string.\"\"\"\n   267→        try:\n   268→            # Extract and sanitize request data\n   269→            request_data = self._sanitize_data(self._extract_request_data(request))\n   270→            \n   271→            # Create simple log object\n   272→            log_obj = {\n   273→                'type': 'REQUEST',\n   274→                'method': method_name,\n   275→                'correlation_id': correlation_id,\n   276→                'timestamp': time.time(),\n   277→                'data': request_data\n   278→            }\n   279→            \n   280→            # Log as JSON string\n   281→            self.logger.info(f\"REQUEST: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n   282→            \n   283→        except Exception as e:\n   284→            self.logger.error(f\"Failed to log request for {method_name}: {e}\")\n   285→    \n   286→    def _log_response(\n   287→        self, \n   288→        method_name: str, \n   289→        response: Any, \n   290→        correlation_id: str, \n   291→        duration: float,\n   292→        context: ServicerContext\n   293→    ):\n   294→        \"\"\"Log outgoing response with structured data.\"\"\"\n   295→        \n   296→        # Sanitize response data\n   297→        response_data = self._sanitize_data(self._extract_response_data(response))\n   298→        \n   299→        log_data = {\n   300→            'event': 'grpc_response',\n   301→            'correlation_id': correlation_id,\n   302→            'method': method_name,\n   303→            'duration_ms': round(duration * 1000, 2),\n   304→            'response_data': response_data,\n   305→            'timestamp': time.time()\n   306→        }\n   307→        \n   308→        self.logger.log(\n   309→            self.log_level,\n   310→            f\"[{correlation_id}] gRPC Response: {method_name} ({duration*1000:.2f}ms)\",\n   311→            extra={'structured_data': log_data}\n   312→        )\n   313→    \n   314→    def _log_error(\n   315→        self, \n   316→        method_name: str, \n   317→        error: Exception, \n   318→        correlation_id: str, \n   319→        duration: float,\n   320→        context: ServicerContext\n   321→    ):\n   322→        \"\"\"Log error with comprehensive context.\"\"\"\n   323→        \n   324→        log_data = {\n   325→            'event': 'grpc_error',\n   326→            'correlation_id': correlation_id,\n   327→            'method': method_name,\n   328→            'error_type': type(error).__name__,\n   329→            'error_message': str(error),\n   330→            'duration_ms': round(duration * 1000, 2),\n   331→            'timestamp': time.time()\n   332→        }\n   333→        \n   334→        self.logger.error(\n   335→            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n   336→            extra={'structured_data': log_data},\n   337→            exc_info=True\n   338→        )\n   339→    \n   340→    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n   341→        \"\"\"Log performance metrics.\"\"\"\n   342→        \n   343→        if duration > 1.0:  # Log slow requests (>1 second)\n   344→            self.logger.warning(\n   345→                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n   346→            )\n   347→    \n   348→    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n   349→        \"\"\"Extract relevant data from request for logging.\"\"\"\n   350→        try:\n   351→            if hasattr(request, 'DESCRIPTOR'):\n   352→                # Protobuf message - extract all fields including defaults\n   353→                result = {}\n   354→                for field in request.DESCRIPTOR.fields:\n   355→                    field_name = field.name\n   356→                    try:\n   357→                        value = getattr(request, field_name, None)\n   358→                        if value is not None:\n   359→                            if hasattr(value, 'DESCRIPTOR'):\n   360→                                # Nested protobuf message\n   361→                                result[field_name] = self._extract_request_data(value)\n   362→                            elif field.label == field.LABEL_REPEATED:\n   363→                                # Repeated field (list)\n   364→                                result[field_name] = [\n   365→                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n   366→                                    for item in value\n   367→                                ]\n   368→                            else:\n   369→                                result[field_name] = self._serialize_value(value)\n   370→                    except Exception as e:\n   371→                        result[field_name] = f\"<error: {str(e)}>\"\n   372→                return result\n   373→            elif hasattr(request, '__dict__'):\n   374→                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n   375→            else:\n   376→                return {'value': str(request)}\n   377→        except Exception:\n   378→            return {'error': 'Failed to extract request data'}\n   379→    \n   380→    def _serialize_value(self, value: Any) -> Any:\n   381→        \"\"\"Serialize a value for logging.\"\"\"\n   382→        try:\n   383→            if isinstance(value, (str, int, float, bool)):\n   384→                return value\n   385→            elif isinstance(value, bytes):\n   386→                import base64\n   387→                encoded = base64.b64encode(value).decode()\n   388→                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n   389→            elif hasattr(value, 'isoformat'):\n   390→                return value.isoformat()\n   391→            elif hasattr(value, 'name'):  # Enum value\n   392→                return value.name\n   393→            else:\n   394→                str_value = str(value)\n   395→                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n   396→        except Exception:\n   397→            return \"<non-serializable>\"\n   398→    \n   399→    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n   400→        \"\"\"Extract relevant data from response for logging.\"\"\"\n   401→        try:\n   402→            if hasattr(response, 'DESCRIPTOR'):\n   403→                # Protobuf message - extract all fields including defaults\n   404→                result = {}\n   405→                for field in response.DESCRIPTOR.fields:\n   406→                    field_name = field.name\n   407→                    try:\n   408→                        value = getattr(response, field_name, None)\n   409→                        if value is not None:\n   410→                            if hasattr(value, 'DESCRIPTOR'):\n   411→                                # Nested protobuf message\n   412→                                result[field_name] = self._extract_response_data(value)\n   413→                            elif field.label == field.LABEL_REPEATED:\n   414→                                # Repeated field (list)\n   415→                                result[field_name] = [\n   416→                                    self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n   417→                                    for item in value\n   418→                                ]\n   419→                            else:\n   420→                                result[field_name] = self._serialize_value(value)\n   421→                    except Exception as e:\n   422→                        result[field_name] = f\"<error: {str(e)}>\"\n   423→                return result\n   424→            elif hasattr(response, '__dict__'):\n   425→                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n   426→            else:\n   427→                return {'value': str(response)}\n   428→        except Exception:\n   429→            return {'error': 'Failed to extract response data'}\n   430→    \n   431→    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n   432→        \"\"\"Remove sensitive information from logged data.\"\"\"\n   433→        if not isinstance(data, dict):\n   434→            return data\n   435→        \n   436→        sanitized = {}\n   437→        for key, value in data.items():\n   438→            key_lower = key.lower()\n   439→            \n   440→            # Check if field contains sensitive information\n   441→            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n   442→                sanitized[key] = '***REDACTED***'\n   443→            elif isinstance(value, dict):\n   444→                sanitized[key] = self._sanitize_data(value)\n   445→            elif isinstance(value, (list, tuple)):\n   446→                sanitized[key] = [\n   447→                    self._sanitize_data(item) if isinstance(item, dict) else item\n   448→                    for item in value\n   449→                ]\n   450→            else:\n   451→                sanitized[key] = value\n   452→        \n   453→        return sanitized\n   454→    \n   455→    def _update_timing_metrics(self, method_name: str, duration: float):\n   456→        \"\"\"Update timing metrics for performance tracking.\"\"\"\n   457→        method_stats = self._metrics['method_stats'][method_name]\n   458→        current_avg = method_stats['avg_time']\n   459→        count = method_stats['count']\n   460→        \n   461→        # Update method-specific average\n   462→        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n   463→        \n   464→        # Update overall average\n   465→        total_requests = self._metrics['total_requests']\n   466→        current_overall_avg = self._metrics['avg_response_time']\n   467→        self._metrics['avg_response_time'] = (\n   468→            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n   469→        )\n   470→    \n   471→    def get_metrics(self) -> Dict[str, Any]:\n   472→        \"\"\"Get current performance metrics.\"\"\"\n   473→        return {\n   474→            'interceptor_name': 'LoggingInterceptor',\n   475→            'metrics': self._metrics.copy(),\n   476→            'timestamp': time.time()\n   477→        }\n   478→    \n   479→    def reset_metrics(self):\n   480→        \"\"\"Reset performance metrics.\"\"\"\n   481→        self._metrics = {\n   482→            'total_requests': 0,\n   483→            'successful_requests': 0,\n   484→            'failed_requests': 0,\n   485→            'avg_response_time': 0.0,\n   486→            'method_stats': {}\n   487→        }\n   488→\n   489→\n   490→# Convenience function for creating configured logging interceptor\n   491→def create_logging_interceptor(\n   492→    logger_name: str = \"grpc.logging\",\n   493→    log_level: int = logging.INFO,\n   494→    **kwargs\n   495→) -> LoggingInterceptor:\n   496→    \"\"\"Create a pre-configured logging interceptor.\n   497→    \n   498→    Args:\n   499→        logger_name: Name for the logger\n   500→        log_level: Logging level\n   501→        **kwargs: Additional configuration options\n   502→        \n   503→    Returns:\n   504→        Configured LoggingInterceptor instance\n   505→    \"\"\"\n   506→    logger = logging.getLogger(logger_name)\n   507→    return LoggingInterceptor(\n   508→        logger=logger,\n   509→        log_level=log_level,\n   510→        **kwargs\n   511→    )"}]},"uuid":"31b781ca-e2fe-4784-8de3-c2e8af66f4ef","timestamp":"2025-08-20T02:03:27.742Z","toolUseResult":{"filePath":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py","oldString":"\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n\nThis interceptor provides:\n- Detailed request/response logging\n- Performance metrics tracking\n- Error logging and analysis\n- Request correlation and tracing\n- Structured logging with context\n\"\"\"\nimport json\nimport logging\nimport time\nfrom typing import Any, Callable, Iterator, Optional, Dict\nfrom uuid import uuid4\n\nimport grpc\nimport grpc.aio\nfrom grpc.aio import ServerInterceptor, ServicerContext\n\n\nclass LoggingInterceptor(grpc.aio.ServerInterceptor):\n    \"\"\"Advanced logging interceptor for gRPC services.\n    \n    Features:\n    - Request/response logging with structured data\n    - Performance metrics and timing\n    - Error tracking and analysis\n    - Request correlation IDs\n    - Configurable log levels and filters\n    - Security-aware logging (excludes sensitive data)\n    \"\"\"\n    \n    def __init__(\n        self,\n        logger: Optional[logging.Logger] = None,\n        log_level: int = logging.INFO,\n        log_requests: bool = True,\n        log_responses: bool = True,\n        log_errors: bool = True,\n        log_performance: bool = True,\n        exclude_methods: Optional[list] = None,\n        sensitive_fields: Optional[list] = None\n    ):\n        self.logger = logger or logging.getLogger(__name__)\n        self.log_level = log_level\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_errors = log_errors\n        self.log_performance = log_performance\n        self.exclude_methods = exclude_methods or []\n        self.sensitive_fields = sensitive_fields or [\n            'password', 'token', 'secret', 'key', 'credential', 'auth'\n        ]\n        \n        # Performance metrics\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n    \n    async def intercept_service(\n        self,\n        continuation: Callable[[Any], Any],\n        handler_call_details: Any,\n    ) -> Any:\n        \"\"\"Intercept service calls for logging.\"\"\"\n        \n        # Skip excluded methods\n        method_name = handler_call_details.method\n        if method_name in self.exclude_methods:\n            return await continuation(handler_call_details)\n        \n        # Get the original handler\n        handler = await continuation(handler_call_details)\n        if handler is None:\n            return None\n        \n        # Wrap the handler based on its type; bypass reflection and health entirely\n        if method_name in {\n            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n            '/grpc.health.v1.Health/Check',\n            '/grpc.health.v1.Health/Watch',\n        }:\n            return handler\n        if handler.unary_unary:\n            return grpc.unary_unary_rpc_method_handler(\n                self._wrap_unary_unary(handler.unary_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.unary_stream:\n            return grpc.unary_stream_rpc_method_handler(\n                self._wrap_unary_stream(handler.unary_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_unary:\n            return grpc.stream_unary_rpc_method_handler(\n                self._wrap_stream_unary(handler.stream_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_stream:\n            return grpc.stream_stream_rpc_method_handler(\n                self._wrap_stream_stream(handler.stream_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        else:\n            return handler\n    \n    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-unary RPC method.\"\"\"\n        \n        async def wrapper(request: Any, context: ServicerContext) -> Any:\n            return await self._handle_request(\n                behavior, request, context, method_name, is_streaming=False\n            )\n        \n        return wrapper\n    \n    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n        async def wrapper(request: Any, context: ServicerContext):\n            # For streaming responses, return the iterator directly (do not await)\n            try:\n                return behavior(request, context)\n            finally:\n                # Minimal logging to avoid interfering with stream lifecycle\n                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-unary RPC method.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n            # Request is streaming, response is unary; await the final response\n            try:\n                return await behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext):\n            # For bi-di streaming, return the response iterator directly\n            try:\n                return behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n        return wrapper\n    \n    async def _handle_request(\n        self, \n        behavior: Callable, \n        request: Any, \n        context: ServicerContext, \n        method_name: str,\n        is_streaming: bool = False\n    ) -> Any:\n        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n        \n        # Generate correlation ID\n        correlation_id = str(uuid4())\n        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            pass\n        \n        start_time = time.time()\n        \n        # Update metrics\n        self._metrics['total_requests'] += 1\n        if method_name not in self._metrics['method_stats']:\n            self._metrics['method_stats'][method_name] = {\n                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n            }\n        self._metrics['method_stats'][method_name]['count'] += 1\n        \n        try:\n            # Log request\n            if self.log_requests:\n                self._log_request(method_name, request, correlation_id, context)\n            \n            # Execute the actual method\n            response = await behavior(request, context)\n            \n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update success metrics\n            self._metrics['successful_requests'] += 1\n            self._metrics['method_stats'][method_name]['success'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log response\n            if self.log_responses:\n                self._log_response(method_name, response, correlation_id, duration, context)\n            \n            # Log performance\n            if self.log_performance:\n                self._log_performance(method_name, duration, correlation_id)\n            \n            return response\n            \n        except Exception as e:\n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update error metrics\n            self._metrics['failed_requests'] += 1\n            self._metrics['method_stats'][method_name]['error'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log error\n            if self.log_errors:\n                self._log_error(method_name, e, correlation_id, duration, context)\n            \n            # Re-raise the exception\n            raise\n    \n    async def _handle_streaming_request(\n        self,\n        behavior: Callable,\n        request_data: Any,\n        context: ServicerContext,\n        method_name: str,\n        request_streaming: bool = False,\n        response_streaming: bool = True\n    ) -> Any:\n        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n        # For Phase 3, we're focusing on request/response patterns\n        # Streaming would be implemented in a future phase\n        correlation_id = str(uuid4())\n        # Store correlation ID in invocation metadata instead of setting attributes\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            # If metadata setting fails, continue without it\n            pass\n        \n        self.logger.info(\n            f\"[{correlation_id}] Streaming method {method_name} called \"\n            f\"(streaming implementation pending)\"\n        )\n        \n        return behavior(request_data, context)\n    \n    def _log_request(\n        self, \n        method_name: str, \n        request: Any, \n        correlation_id: str, \n        context: ServicerContext\n    ):\n        \"\"\"Log incoming request with structured data.\"\"\"\n        \n        # Extract client information\n        peer = context.peer() if hasattr(context, 'peer') else 'unknown'\n        \n        # Sanitize request data\n        request_data = self._sanitize_data(self._extract_request_data(request))\n        \n        log_data = {\n            'event': 'grpc_request',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'peer': peer,\n            'request_data': request_data,\n            'timestamp': time.time()\n        }\n        \n        self.logger.log(\n            self.log_level,\n            f\"[{correlation_id}] gRPC Request: {method_name}\",\n            extra={'structured_data': log_data}\n        )\n    \n    def _log_response(\n        self, \n        method_name: str, \n        response: Any, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log outgoing response with structured data.\"\"\"\n        \n        # Sanitize response data\n        response_data = self._sanitize_data(self._extract_response_data(response))\n        \n        log_data = {\n            'event': 'grpc_response',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'duration_ms': round(duration * 1000, 2),\n            'response_data': response_data,\n            'timestamp': time.time()\n        }\n        \n        self.logger.log(\n            self.log_level,\n            f\"[{correlation_id}] gRPC Response: {method_name} ({duration*1000:.2f}ms)\",\n            extra={'structured_data': log_data}\n        )\n    \n    def _log_error(\n        self, \n        method_name: str, \n        error: Exception, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log error with comprehensive context.\"\"\"\n        \n        log_data = {\n            'event': 'grpc_error',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'error_type': type(error).__name__,\n            'error_message': str(error),\n            'duration_ms': round(duration * 1000, 2),\n            'timestamp': time.time()\n        }\n        \n        self.logger.error(\n            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n            extra={'structured_data': log_data},\n            exc_info=True\n        )\n    \n    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n        \"\"\"Log performance metrics.\"\"\"\n        \n        if duration > 1.0:  # Log slow requests (>1 second)\n            self.logger.warning(\n                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n            )\n    \n    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(request, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_request_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}\n    \n    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a value for logging.\"\"\"\n        try:\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                import base64\n                encoded = base64.b64encode(value).decode()\n                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n            elif hasattr(value, 'isoformat'):\n                return value.isoformat()\n            elif hasattr(value, 'name'):  # Enum value\n                return value.name\n            else:\n                str_value = str(value)\n                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n        except Exception:\n            return \"<non-serializable>\"\n    \n    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(response, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_response_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}\n    \n    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove sensitive information from logged data.\"\"\"\n        if not isinstance(data, dict):\n            return data\n        \n        sanitized = {}\n        for key, value in data.items():\n            key_lower = key.lower()\n            \n            # Check if field contains sensitive information\n            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n                sanitized[key] = '***REDACTED***'\n            elif isinstance(value, dict):\n                sanitized[key] = self._sanitize_data(value)\n            elif isinstance(value, (list, tuple)):\n                sanitized[key] = [\n                    self._sanitize_data(item) if isinstance(item, dict) else item\n                    for item in value\n                ]\n            else:\n                sanitized[key] = value\n        \n        return sanitized\n    \n    def _update_timing_metrics(self, method_name: str, duration: float):\n        \"\"\"Update timing metrics for performance tracking.\"\"\"\n        method_stats = self._metrics['method_stats'][method_name]\n        current_avg = method_stats['avg_time']\n        count = method_stats['count']\n        \n        # Update method-specific average\n        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n        \n        # Update overall average\n        total_requests = self._metrics['total_requests']\n        current_overall_avg = self._metrics['avg_response_time']\n        self._metrics['avg_response_time'] = (\n            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n        )\n    \n    def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get current performance metrics.\"\"\"\n        return {\n            'interceptor_name': 'LoggingInterceptor',\n            'metrics': self._metrics.copy(),\n            'timestamp': time.time()\n        }\n    \n    def reset_metrics(self):\n        \"\"\"Reset performance metrics.\"\"\"\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n\n\n# Convenience function for creating configured logging interceptor\ndef create_logging_interceptor(\n    logger_name: str = \"grpc.logging\",\n    log_level: int = logging.INFO,\n    **kwargs\n) -> LoggingInterceptor:\n    \"\"\"Create a pre-configured logging interceptor.\n    \n    Args:\n        logger_name: Name for the logger\n        log_level: Logging level\n        **kwargs: Additional configuration options\n        \n    Returns:\n        Configured LoggingInterceptor instance\n    \"\"\"\n    logger = logging.getLogger(logger_name)\n    return LoggingInterceptor(\n        logger=logger,\n        log_level=log_level,\n        **kwargs\n    )","newString":"\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n\nThis interceptor provides:\n- Detailed request/response logging\n- Performance metrics tracking\n- Error logging and analysis\n- Request correlation and tracing\n- Structured logging with context\n\"\"\"\nimport json\nimport logging\nimport time\nfrom typing import Any, Callable, Iterator, Optional, Dict\nfrom uuid import uuid4\n\nimport grpc\nimport grpc.aio\nfrom grpc.aio import ServerInterceptor, ServicerContext\n\n\nclass LoggingInterceptor(grpc.aio.ServerInterceptor):\n    \"\"\"Advanced logging interceptor for gRPC services.\n    \n    Features:\n    - Request/response logging with structured data\n    - Performance metrics and timing\n    - Error tracking and analysis\n    - Request correlation IDs\n    - Configurable log levels and filters\n    - Security-aware logging (excludes sensitive data)\n    \"\"\"\n    \n    def __init__(\n        self,\n        logger: Optional[logging.Logger] = None,\n        log_level: int = logging.INFO,\n        log_requests: bool = True,\n        log_responses: bool = True,\n        log_errors: bool = True,\n        log_performance: bool = True,\n        exclude_methods: Optional[list] = None,\n        sensitive_fields: Optional[list] = None\n    ):\n        self.logger = logger or logging.getLogger(__name__)\n        self.log_level = log_level\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_errors = log_errors\n        self.log_performance = log_performance\n        self.exclude_methods = exclude_methods or []\n        self.sensitive_fields = sensitive_fields or [\n            'password', 'token', 'secret', 'key', 'credential', 'auth'\n        ]\n        \n        # Performance metrics\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n    \n    async def intercept_service(\n        self,\n        continuation: Callable[[Any], Any],\n        handler_call_details: Any,\n    ) -> Any:\n        \"\"\"Intercept service calls for logging.\"\"\"\n        \n        # Skip excluded methods\n        method_name = handler_call_details.method\n        if method_name in self.exclude_methods:\n            return await continuation(handler_call_details)\n        \n        # Get the original handler\n        handler = await continuation(handler_call_details)\n        if handler is None:\n            return None\n        \n        # Wrap the handler based on its type; bypass reflection and health entirely\n        if method_name in {\n            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n            '/grpc.health.v1.Health/Check',\n            '/grpc.health.v1.Health/Watch',\n        }:\n            return handler\n        if handler.unary_unary:\n            return grpc.unary_unary_rpc_method_handler(\n                self._wrap_unary_unary(handler.unary_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.unary_stream:\n            return grpc.unary_stream_rpc_method_handler(\n                self._wrap_unary_stream(handler.unary_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_unary:\n            return grpc.stream_unary_rpc_method_handler(\n                self._wrap_stream_unary(handler.stream_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_stream:\n            return grpc.stream_stream_rpc_method_handler(\n                self._wrap_stream_stream(handler.stream_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        else:\n            return handler\n    \n    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-unary RPC method.\"\"\"\n        \n        async def wrapper(request: Any, context: ServicerContext) -> Any:\n            return await self._handle_request(\n                behavior, request, context, method_name, is_streaming=False\n            )\n        \n        return wrapper\n    \n    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n        async def wrapper(request: Any, context: ServicerContext):\n            # For streaming responses, return the iterator directly (do not await)\n            try:\n                return behavior(request, context)\n            finally:\n                # Minimal logging to avoid interfering with stream lifecycle\n                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-unary RPC method.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n            # Request is streaming, response is unary; await the final response\n            try:\n                return await behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext):\n            # For bi-di streaming, return the response iterator directly\n            try:\n                return behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n        return wrapper\n    \n    async def _handle_request(\n        self, \n        behavior: Callable, \n        request: Any, \n        context: ServicerContext, \n        method_name: str,\n        is_streaming: bool = False\n    ) -> Any:\n        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n        \n        # Generate correlation ID\n        correlation_id = str(uuid4())\n        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            pass\n        \n        start_time = time.time()\n        \n        # Update metrics\n        self._metrics['total_requests'] += 1\n        if method_name not in self._metrics['method_stats']:\n            self._metrics['method_stats'][method_name] = {\n                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n            }\n        self._metrics['method_stats'][method_name]['count'] += 1\n        \n        try:\n            # Log request\n            if self.log_requests:\n                self._log_request(method_name, request, correlation_id, context)\n            \n            # Execute the actual method\n            response = await behavior(request, context)\n            \n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update success metrics\n            self._metrics['successful_requests'] += 1\n            self._metrics['method_stats'][method_name]['success'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log response\n            if self.log_responses:\n                self._log_response(method_name, response, correlation_id, duration, context)\n            \n            # Log performance\n            if self.log_performance:\n                self._log_performance(method_name, duration, correlation_id)\n            \n            return response\n            \n        except Exception as e:\n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update error metrics\n            self._metrics['failed_requests'] += 1\n            self._metrics['method_stats'][method_name]['error'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log error\n            if self.log_errors:\n                self._log_error(method_name, e, correlation_id, duration, context)\n            \n            # Re-raise the exception\n            raise\n    \n    async def _handle_streaming_request(\n        self,\n        behavior: Callable,\n        request_data: Any,\n        context: ServicerContext,\n        method_name: str,\n        request_streaming: bool = False,\n        response_streaming: bool = True\n    ) -> Any:\n        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n        # For Phase 3, we're focusing on request/response patterns\n        # Streaming would be implemented in a future phase\n        correlation_id = str(uuid4())\n        # Store correlation ID in invocation metadata instead of setting attributes\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            # If metadata setting fails, continue without it\n            pass\n        \n        self.logger.info(\n            f\"[{correlation_id}] Streaming method {method_name} called \"\n            f\"(streaming implementation pending)\"\n        )\n        \n        return behavior(request_data, context)\n    \n    def _log_request(\n        self, \n        method_name: str, \n        request: Any, \n        correlation_id: str, \n        context: ServicerContext\n    ):\n        \"\"\"Log incoming request as JSON string.\"\"\"\n        try:\n            # Extract and sanitize request data\n            request_data = self._sanitize_data(self._extract_request_data(request))\n            \n            # Create simple log object\n            log_obj = {\n                'type': 'REQUEST',\n                'method': method_name,\n                'correlation_id': correlation_id,\n                'timestamp': time.time(),\n                'data': request_data\n            }\n            \n            # Log as JSON string\n            self.logger.info(f\"REQUEST: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log request for {method_name}: {e}\")\n    \n    def _log_response(\n        self, \n        method_name: str, \n        response: Any, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log outgoing response with structured data.\"\"\"\n        \n        # Sanitize response data\n        response_data = self._sanitize_data(self._extract_response_data(response))\n        \n        log_data = {\n            'event': 'grpc_response',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'duration_ms': round(duration * 1000, 2),\n            'response_data': response_data,\n            'timestamp': time.time()\n        }\n        \n        self.logger.log(\n            self.log_level,\n            f\"[{correlation_id}] gRPC Response: {method_name} ({duration*1000:.2f}ms)\",\n            extra={'structured_data': log_data}\n        )\n    \n    def _log_error(\n        self, \n        method_name: str, \n        error: Exception, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log error with comprehensive context.\"\"\"\n        \n        log_data = {\n            'event': 'grpc_error',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'error_type': type(error).__name__,\n            'error_message': str(error),\n            'duration_ms': round(duration * 1000, 2),\n            'timestamp': time.time()\n        }\n        \n        self.logger.error(\n            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n            extra={'structured_data': log_data},\n            exc_info=True\n        )\n    \n    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n        \"\"\"Log performance metrics.\"\"\"\n        \n        if duration > 1.0:  # Log slow requests (>1 second)\n            self.logger.warning(\n                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n            )\n    \n    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(request, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_request_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}\n    \n    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a value for logging.\"\"\"\n        try:\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                import base64\n                encoded = base64.b64encode(value).decode()\n                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n            elif hasattr(value, 'isoformat'):\n                return value.isoformat()\n            elif hasattr(value, 'name'):  # Enum value\n                return value.name\n            else:\n                str_value = str(value)\n                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n        except Exception:\n            return \"<non-serializable>\"\n    \n    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(response, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_response_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}\n    \n    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove sensitive information from logged data.\"\"\"\n        if not isinstance(data, dict):\n            return data\n        \n        sanitized = {}\n        for key, value in data.items():\n            key_lower = key.lower()\n            \n            # Check if field contains sensitive information\n            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n                sanitized[key] = '***REDACTED***'\n            elif isinstance(value, dict):\n                sanitized[key] = self._sanitize_data(value)\n            elif isinstance(value, (list, tuple)):\n                sanitized[key] = [\n                    self._sanitize_data(item) if isinstance(item, dict) else item\n                    for item in value\n                ]\n            else:\n                sanitized[key] = value\n        \n        return sanitized\n    \n    def _update_timing_metrics(self, method_name: str, duration: float):\n        \"\"\"Update timing metrics for performance tracking.\"\"\"\n        method_stats = self._metrics['method_stats'][method_name]\n        current_avg = method_stats['avg_time']\n        count = method_stats['count']\n        \n        # Update method-specific average\n        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n        \n        # Update overall average\n        total_requests = self._metrics['total_requests']\n        current_overall_avg = self._metrics['avg_response_time']\n        self._metrics['avg_response_time'] = (\n            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n        )\n    \n    def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get current performance metrics.\"\"\"\n        return {\n            'interceptor_name': 'LoggingInterceptor',\n            'metrics': self._metrics.copy(),\n            'timestamp': time.time()\n        }\n    \n    def reset_metrics(self):\n        \"\"\"Reset performance metrics.\"\"\"\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n\n\n# Convenience function for creating configured logging interceptor\ndef create_logging_interceptor(\n    logger_name: str = \"grpc.logging\",\n    log_level: int = logging.INFO,\n    **kwargs\n) -> LoggingInterceptor:\n    \"\"\"Create a pre-configured logging interceptor.\n    \n    Args:\n        logger_name: Name for the logger\n        log_level: Logging level\n        **kwargs: Additional configuration options\n        \n    Returns:\n        Configured LoggingInterceptor instance\n    \"\"\"\n    logger = logging.getLogger(logger_name)\n    return LoggingInterceptor(\n        logger=logger,\n        log_level=log_level,\n        **kwargs\n    )","originalFile":"\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n\nThis interceptor provides:\n- Detailed request/response logging\n- Performance metrics tracking\n- Error logging and analysis\n- Request correlation and tracing\n- Structured logging with context\n\"\"\"\nimport json\nimport logging\nimport time\nfrom typing import Any, Callable, Iterator, Optional, Dict\nfrom uuid import uuid4\n\nimport grpc\nimport grpc.aio\nfrom grpc.aio import ServerInterceptor, ServicerContext\n\n\nclass LoggingInterceptor(grpc.aio.ServerInterceptor):\n    \"\"\"Advanced logging interceptor for gRPC services.\n    \n    Features:\n    - Request/response logging with structured data\n    - Performance metrics and timing\n    - Error tracking and analysis\n    - Request correlation IDs\n    - Configurable log levels and filters\n    - Security-aware logging (excludes sensitive data)\n    \"\"\"\n    \n    def __init__(\n        self,\n        logger: Optional[logging.Logger] = None,\n        log_level: int = logging.INFO,\n        log_requests: bool = True,\n        log_responses: bool = True,\n        log_errors: bool = True,\n        log_performance: bool = True,\n        exclude_methods: Optional[list] = None,\n        sensitive_fields: Optional[list] = None\n    ):\n        self.logger = logger or logging.getLogger(__name__)\n        self.log_level = log_level\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_errors = log_errors\n        self.log_performance = log_performance\n        self.exclude_methods = exclude_methods or []\n        self.sensitive_fields = sensitive_fields or [\n            'password', 'token', 'secret', 'key', 'credential', 'auth'\n        ]\n        \n        # Performance metrics\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n    \n    async def intercept_service(\n        self,\n        continuation: Callable[[Any], Any],\n        handler_call_details: Any,\n    ) -> Any:\n        \"\"\"Intercept service calls for logging.\"\"\"\n        \n        # Skip excluded methods\n        method_name = handler_call_details.method\n        if method_name in self.exclude_methods:\n            return await continuation(handler_call_details)\n        \n        # Get the original handler\n        handler = await continuation(handler_call_details)\n        if handler is None:\n            return None\n        \n        # Wrap the handler based on its type; bypass reflection and health entirely\n        if method_name in {\n            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n            '/grpc.health.v1.Health/Check',\n            '/grpc.health.v1.Health/Watch',\n        }:\n            return handler\n        if handler.unary_unary:\n            return grpc.unary_unary_rpc_method_handler(\n                self._wrap_unary_unary(handler.unary_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.unary_stream:\n            return grpc.unary_stream_rpc_method_handler(\n                self._wrap_unary_stream(handler.unary_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_unary:\n            return grpc.stream_unary_rpc_method_handler(\n                self._wrap_stream_unary(handler.stream_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_stream:\n            return grpc.stream_stream_rpc_method_handler(\n                self._wrap_stream_stream(handler.stream_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        else:\n            return handler\n    \n    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-unary RPC method.\"\"\"\n        \n        async def wrapper(request: Any, context: ServicerContext) -> Any:\n            return await self._handle_request(\n                behavior, request, context, method_name, is_streaming=False\n            )\n        \n        return wrapper\n    \n    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n        async def wrapper(request: Any, context: ServicerContext):\n            # For streaming responses, return the iterator directly (do not await)\n            try:\n                return behavior(request, context)\n            finally:\n                # Minimal logging to avoid interfering with stream lifecycle\n                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-unary RPC method.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n            # Request is streaming, response is unary; await the final response\n            try:\n                return await behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext):\n            # For bi-di streaming, return the response iterator directly\n            try:\n                return behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n        return wrapper\n    \n    async def _handle_request(\n        self, \n        behavior: Callable, \n        request: Any, \n        context: ServicerContext, \n        method_name: str,\n        is_streaming: bool = False\n    ) -> Any:\n        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n        \n        # Generate correlation ID\n        correlation_id = str(uuid4())\n        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            pass\n        \n        start_time = time.time()\n        \n        # Update metrics\n        self._metrics['total_requests'] += 1\n        if method_name not in self._metrics['method_stats']:\n            self._metrics['method_stats'][method_name] = {\n                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n            }\n        self._metrics['method_stats'][method_name]['count'] += 1\n        \n        try:\n            # Log request\n            if self.log_requests:\n                self._log_request(method_name, request, correlation_id, context)\n            \n            # Execute the actual method\n            response = await behavior(request, context)\n            \n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update success metrics\n            self._metrics['successful_requests'] += 1\n            self._metrics['method_stats'][method_name]['success'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log response\n            if self.log_responses:\n                self._log_response(method_name, response, correlation_id, duration, context)\n            \n            # Log performance\n            if self.log_performance:\n                self._log_performance(method_name, duration, correlation_id)\n            \n            return response\n            \n        except Exception as e:\n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update error metrics\n            self._metrics['failed_requests'] += 1\n            self._metrics['method_stats'][method_name]['error'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log error\n            if self.log_errors:\n                self._log_error(method_name, e, correlation_id, duration, context)\n            \n            # Re-raise the exception\n            raise\n    \n    async def _handle_streaming_request(\n        self,\n        behavior: Callable,\n        request_data: Any,\n        context: ServicerContext,\n        method_name: str,\n        request_streaming: bool = False,\n        response_streaming: bool = True\n    ) -> Any:\n        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n        # For Phase 3, we're focusing on request/response patterns\n        # Streaming would be implemented in a future phase\n        correlation_id = str(uuid4())\n        # Store correlation ID in invocation metadata instead of setting attributes\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            # If metadata setting fails, continue without it\n            pass\n        \n        self.logger.info(\n            f\"[{correlation_id}] Streaming method {method_name} called \"\n            f\"(streaming implementation pending)\"\n        )\n        \n        return behavior(request_data, context)\n    \n    def _log_request(\n        self, \n        method_name: str, \n        request: Any, \n        correlation_id: str, \n        context: ServicerContext\n    ):\n        \"\"\"Log incoming request with structured data.\"\"\"\n        \n        # Extract client information\n        peer = context.peer() if hasattr(context, 'peer') else 'unknown'\n        \n        # Sanitize request data\n        request_data = self._sanitize_data(self._extract_request_data(request))\n        \n        log_data = {\n            'event': 'grpc_request',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'peer': peer,\n            'request_data': request_data,\n            'timestamp': time.time()\n        }\n        \n        self.logger.log(\n            self.log_level,\n            f\"[{correlation_id}] gRPC Request: {method_name}\",\n            extra={'structured_data': log_data}\n        )\n    \n    def _log_response(\n        self, \n        method_name: str, \n        response: Any, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log outgoing response with structured data.\"\"\"\n        \n        # Sanitize response data\n        response_data = self._sanitize_data(self._extract_response_data(response))\n        \n        log_data = {\n            'event': 'grpc_response',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'duration_ms': round(duration * 1000, 2),\n            'response_data': response_data,\n            'timestamp': time.time()\n        }\n        \n        self.logger.log(\n            self.log_level,\n            f\"[{correlation_id}] gRPC Response: {method_name} ({duration*1000:.2f}ms)\",\n            extra={'structured_data': log_data}\n        )\n    \n    def _log_error(\n        self, \n        method_name: str, \n        error: Exception, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log error with comprehensive context.\"\"\"\n        \n        log_data = {\n            'event': 'grpc_error',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'error_type': type(error).__name__,\n            'error_message': str(error),\n            'duration_ms': round(duration * 1000, 2),\n            'timestamp': time.time()\n        }\n        \n        self.logger.error(\n            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n            extra={'structured_data': log_data},\n            exc_info=True\n        )\n    \n    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n        \"\"\"Log performance metrics.\"\"\"\n        \n        if duration > 1.0:  # Log slow requests (>1 second)\n            self.logger.warning(\n                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n            )\n    \n    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(request, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_request_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}\n    \n    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a value for logging.\"\"\"\n        try:\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                import base64\n                encoded = base64.b64encode(value).decode()\n                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n            elif hasattr(value, 'isoformat'):\n                return value.isoformat()\n            elif hasattr(value, 'name'):  # Enum value\n                return value.name\n            else:\n                str_value = str(value)\n                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n        except Exception:\n            return \"<non-serializable>\"\n    \n    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(response, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_response_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}\n    \n    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove sensitive information from logged data.\"\"\"\n        if not isinstance(data, dict):\n            return data\n        \n        sanitized = {}\n        for key, value in data.items():\n            key_lower = key.lower()\n            \n            # Check if field contains sensitive information\n            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n                sanitized[key] = '***REDACTED***'\n            elif isinstance(value, dict):\n                sanitized[key] = self._sanitize_data(value)\n            elif isinstance(value, (list, tuple)):\n                sanitized[key] = [\n                    self._sanitize_data(item) if isinstance(item, dict) else item\n                    for item in value\n                ]\n            else:\n                sanitized[key] = value\n        \n        return sanitized\n    \n    def _update_timing_metrics(self, method_name: str, duration: float):\n        \"\"\"Update timing metrics for performance tracking.\"\"\"\n        method_stats = self._metrics['method_stats'][method_name]\n        current_avg = method_stats['avg_time']\n        count = method_stats['count']\n        \n        # Update method-specific average\n        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n        \n        # Update overall average\n        total_requests = self._metrics['total_requests']\n        current_overall_avg = self._metrics['avg_response_time']\n        self._metrics['avg_response_time'] = (\n            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n        )\n    \n    def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get current performance metrics.\"\"\"\n        return {\n            'interceptor_name': 'LoggingInterceptor',\n            'metrics': self._metrics.copy(),\n            'timestamp': time.time()\n        }\n    \n    def reset_metrics(self):\n        \"\"\"Reset performance metrics.\"\"\"\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n\n\n# Convenience function for creating configured logging interceptor\ndef create_logging_interceptor(\n    logger_name: str = \"grpc.logging\",\n    log_level: int = logging.INFO,\n    **kwargs\n) -> LoggingInterceptor:\n    \"\"\"Create a pre-configured logging interceptor.\n    \n    Args:\n        logger_name: Name for the logger\n        log_level: Logging level\n        **kwargs: Additional configuration options\n        \n    Returns:\n        Configured LoggingInterceptor instance\n    \"\"\"\n    logger = logging.getLogger(logger_name)\n    return LoggingInterceptor(\n        logger=logger,\n        log_level=log_level,\n        **kwargs\n    )","structuredPatch":[{"oldStart":263,"oldLines":28,"newStart":263,"newLines":25,"lines":["         correlation_id: str, ","         context: ServicerContext","     ):","-        \"\"\"Log incoming request with structured data.\"\"\"","-        ","-        # Extract client information","-        peer = context.peer() if hasattr(context, 'peer') else 'unknown'","-        ","-        # Sanitize request data","-        request_data = self._sanitize_data(self._extract_request_data(request))","-        ","-        log_data = {","-            'event': 'grpc_request',","-            'correlation_id': correlation_id,","-            'method': method_name,","-            'peer': peer,","-            'request_data': request_data,","-            'timestamp': time.time()","-        }","-        ","-        self.logger.log(","-            self.log_level,","-            f\"[{correlation_id}] gRPC Request: {method_name}\",","-            extra={'structured_data': log_data}","-        )","+        \"\"\"Log incoming request as JSON string.\"\"\"","+        try:","+            # Extract and sanitize request data","+            request_data = self._sanitize_data(self._extract_request_data(request))","+            ","+            # Create simple log object","+            log_obj = {","+                'type': 'REQUEST',","+                'method': method_name,","+                'correlation_id': correlation_id,","+                'timestamp': time.time(),","+                'data': request_data","+            }","+            ","+            # Log as JSON string","+            self.logger.info(f\"REQUEST: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")","+            ","+        except Exception as e:","+            self.logger.error(f\"Failed to log request for {method_name}: {e}\")","     ","     def _log_response(","         self, "]}],"userModified":false,"replaceAll":false}}
{"parentUuid":"31b781ca-e2fe-4784-8de3-c2e8af66f4ef","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01HNhLY6yaEeS4isovkmKArN","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Bây giờ tôi sẽ cập nhật `_log_response` để cũng log response dưới dạng JSON string đơn giản:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":7256,"cache_read_input_tokens":76410,"cache_creation":{"ephemeral_5m_input_tokens":7256,"ephemeral_1h_input_tokens":0},"output_tokens":3,"service_tier":"standard"}},"requestId":"req_011CSJ95asKfhdrLLiRdF19K","type":"assistant","uuid":"231abf34-18ac-4b03-b6ff-104755a0b74d","timestamp":"2025-08-20T02:03:33.768Z"}
{"parentUuid":"231abf34-18ac-4b03-b6ff-104755a0b74d","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01HNhLY6yaEeS4isovkmKArN","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01Nhf2bjfuVY89A39BECBW1P","name":"Edit","input":{"file_path":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py","old_string":"    def _log_response(\n        self, \n        method_name: str, \n        response: Any, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log outgoing response with structured data.\"\"\"\n        \n        # Sanitize response data\n        response_data = self._sanitize_data(self._extract_response_data(response))\n        \n        log_data = {\n            'event': 'grpc_response',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'duration_ms': round(duration * 1000, 2),\n            'response_data': response_data,\n            'timestamp': time.time()\n        }\n        \n        self.logger.log(\n            self.log_level,\n            f\"[{correlation_id}] gRPC Response: {method_name} ({duration*1000:.2f}ms)\",\n            extra={'structured_data': log_data}\n        )","new_string":"    def _log_response(\n        self, \n        method_name: str, \n        response: Any, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log outgoing response as JSON string.\"\"\"\n        try:\n            # Extract and sanitize response data\n            response_data = self._sanitize_data(self._extract_response_data(response))\n            \n            # Create simple log object\n            log_obj = {\n                'type': 'RESPONSE',\n                'method': method_name,\n                'correlation_id': correlation_id,\n                'duration_ms': round(duration * 1000, 2),\n                'timestamp': time.time(),\n                'data': response_data\n            }\n            \n            # Log as JSON string\n            self.logger.info(f\"RESPONSE: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log response for {method_name}: {e}\")"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":7256,"cache_read_input_tokens":76410,"cache_creation":{"ephemeral_5m_input_tokens":7256,"ephemeral_1h_input_tokens":0},"output_tokens":667,"service_tier":"standard"}},"requestId":"req_011CSJ95asKfhdrLLiRdF19K","type":"assistant","uuid":"a167b8c1-9ec6-4e73-9c4c-9896bcb8c7ec","timestamp":"2025-08-20T02:03:39.708Z"}
{"parentUuid":"a167b8c1-9ec6-4e73-9c4c-9896bcb8c7ec","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Nhf2bjfuVY89A39BECBW1P","type":"tool_result","content":"The file /Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n     1→\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n     2→\n     3→This interceptor provides:\n     4→- Detailed request/response logging\n     5→- Performance metrics tracking\n     6→- Error logging and analysis\n     7→- Request correlation and tracing\n     8→- Structured logging with context\n     9→\"\"\"\n    10→import json\n    11→import logging\n    12→import time\n    13→from typing import Any, Callable, Iterator, Optional, Dict\n    14→from uuid import uuid4\n    15→\n    16→import grpc\n    17→import grpc.aio\n    18→from grpc.aio import ServerInterceptor, ServicerContext\n    19→\n    20→\n    21→class LoggingInterceptor(grpc.aio.ServerInterceptor):\n    22→    \"\"\"Advanced logging interceptor for gRPC services.\n    23→    \n    24→    Features:\n    25→    - Request/response logging with structured data\n    26→    - Performance metrics and timing\n    27→    - Error tracking and analysis\n    28→    - Request correlation IDs\n    29→    - Configurable log levels and filters\n    30→    - Security-aware logging (excludes sensitive data)\n    31→    \"\"\"\n    32→    \n    33→    def __init__(\n    34→        self,\n    35→        logger: Optional[logging.Logger] = None,\n    36→        log_level: int = logging.INFO,\n    37→        log_requests: bool = True,\n    38→        log_responses: bool = True,\n    39→        log_errors: bool = True,\n    40→        log_performance: bool = True,\n    41→        exclude_methods: Optional[list] = None,\n    42→        sensitive_fields: Optional[list] = None\n    43→    ):\n    44→        self.logger = logger or logging.getLogger(__name__)\n    45→        self.log_level = log_level\n    46→        self.log_requests = log_requests\n    47→        self.log_responses = log_responses\n    48→        self.log_errors = log_errors\n    49→        self.log_performance = log_performance\n    50→        self.exclude_methods = exclude_methods or []\n    51→        self.sensitive_fields = sensitive_fields or [\n    52→            'password', 'token', 'secret', 'key', 'credential', 'auth'\n    53→        ]\n    54→        \n    55→        # Performance metrics\n    56→        self._metrics = {\n    57→            'total_requests': 0,\n    58→            'successful_requests': 0,\n    59→            'failed_requests': 0,\n    60→            'avg_response_time': 0.0,\n    61→            'method_stats': {}\n    62→        }\n    63→    \n    64→    async def intercept_service(\n    65→        self,\n    66→        continuation: Callable[[Any], Any],\n    67→        handler_call_details: Any,\n    68→    ) -> Any:\n    69→        \"\"\"Intercept service calls for logging.\"\"\"\n    70→        \n    71→        # Skip excluded methods\n    72→        method_name = handler_call_details.method\n    73→        if method_name in self.exclude_methods:\n    74→            return await continuation(handler_call_details)\n    75→        \n    76→        # Get the original handler\n    77→        handler = await continuation(handler_call_details)\n    78→        if handler is None:\n    79→            return None\n    80→        \n    81→        # Wrap the handler based on its type; bypass reflection and health entirely\n    82→        if method_name in {\n    83→            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n    84→            '/grpc.health.v1.Health/Check',\n    85→            '/grpc.health.v1.Health/Watch',\n    86→        }:\n    87→            return handler\n    88→        if handler.unary_unary:\n    89→            return grpc.unary_unary_rpc_method_handler(\n    90→                self._wrap_unary_unary(handler.unary_unary, method_name),\n    91→                request_deserializer=handler.request_deserializer,\n    92→                response_serializer=handler.response_serializer,\n    93→            )\n    94→        elif handler.unary_stream:\n    95→            return grpc.unary_stream_rpc_method_handler(\n    96→                self._wrap_unary_stream(handler.unary_stream, method_name),\n    97→                request_deserializer=handler.request_deserializer,\n    98→                response_serializer=handler.response_serializer,\n    99→            )\n   100→        elif handler.stream_unary:\n   101→            return grpc.stream_unary_rpc_method_handler(\n   102→                self._wrap_stream_unary(handler.stream_unary, method_name),\n   103→                request_deserializer=handler.request_deserializer,\n   104→                response_serializer=handler.response_serializer,\n   105→            )\n   106→        elif handler.stream_stream:\n   107→            return grpc.stream_stream_rpc_method_handler(\n   108→                self._wrap_stream_stream(handler.stream_stream, method_name),\n   109→                request_deserializer=handler.request_deserializer,\n   110→                response_serializer=handler.response_serializer,\n   111→            )\n   112→        else:\n   113→            return handler\n   114→    \n   115→    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n   116→        \"\"\"Wrap unary-unary RPC method.\"\"\"\n   117→        \n   118→        async def wrapper(request: Any, context: ServicerContext) -> Any:\n   119→            return await self._handle_request(\n   120→                behavior, request, context, method_name, is_streaming=False\n   121→            )\n   122→        \n   123→        return wrapper\n   124→    \n   125→    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n   126→        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n   127→        async def wrapper(request: Any, context: ServicerContext):\n   128→            # For streaming responses, return the iterator directly (do not await)\n   129→            try:\n   130→                return behavior(request, context)\n   131→            finally:\n   132→                # Minimal logging to avoid interfering with stream lifecycle\n   133→                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n   134→        return wrapper\n   135→    \n   136→    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n   137→        \"\"\"Wrap stream-unary RPC method.\"\"\"\n   138→        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n   139→            # Request is streaming, response is unary; await the final response\n   140→            try:\n   141→                return await behavior(request_iterator, context)\n   142→            finally:\n   143→                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n   144→        return wrapper\n   145→    \n   146→    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n   147→        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n   148→        async def wrapper(request_iterator, context: ServicerContext):\n   149→            # For bi-di streaming, return the response iterator directly\n   150→            try:\n   151→                return behavior(request_iterator, context)\n   152→            finally:\n   153→                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n   154→        return wrapper\n   155→    \n   156→    async def _handle_request(\n   157→        self, \n   158→        behavior: Callable, \n   159→        request: Any, \n   160→        context: ServicerContext, \n   161→        method_name: str,\n   162→        is_streaming: bool = False\n   163→    ) -> Any:\n   164→        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n   165→        \n   166→        # Generate correlation ID\n   167→        correlation_id = str(uuid4())\n   168→        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n   169→        try:\n   170→            context.set_trailing_metadata([\n   171→                ('correlation_id', correlation_id),\n   172→                ('request_id', correlation_id)\n   173→            ])\n   174→        except Exception:\n   175→            pass\n   176→        \n   177→        start_time = time.time()\n   178→        \n   179→        # Update metrics\n   180→        self._metrics['total_requests'] += 1\n   181→        if method_name not in self._metrics['method_stats']:\n   182→            self._metrics['method_stats'][method_name] = {\n   183→                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n   184→            }\n   185→        self._metrics['method_stats'][method_name]['count'] += 1\n   186→        \n   187→        try:\n   188→            # Log request\n   189→            if self.log_requests:\n   190→                self._log_request(method_name, request, correlation_id, context)\n   191→            \n   192→            # Execute the actual method\n   193→            response = await behavior(request, context)\n   194→            \n   195→            # Calculate duration\n   196→            duration = time.time() - start_time\n   197→            \n   198→            # Update success metrics\n   199→            self._metrics['successful_requests'] += 1\n   200→            self._metrics['method_stats'][method_name]['success'] += 1\n   201→            self._update_timing_metrics(method_name, duration)\n   202→            \n   203→            # Log response\n   204→            if self.log_responses:\n   205→                self._log_response(method_name, response, correlation_id, duration, context)\n   206→            \n   207→            # Log performance\n   208→            if self.log_performance:\n   209→                self._log_performance(method_name, duration, correlation_id)\n   210→            \n   211→            return response\n   212→            \n   213→        except Exception as e:\n   214→            # Calculate duration\n   215→            duration = time.time() - start_time\n   216→            \n   217→            # Update error metrics\n   218→            self._metrics['failed_requests'] += 1\n   219→            self._metrics['method_stats'][method_name]['error'] += 1\n   220→            self._update_timing_metrics(method_name, duration)\n   221→            \n   222→            # Log error\n   223→            if self.log_errors:\n   224→                self._log_error(method_name, e, correlation_id, duration, context)\n   225→            \n   226→            # Re-raise the exception\n   227→            raise\n   228→    \n   229→    async def _handle_streaming_request(\n   230→        self,\n   231→        behavior: Callable,\n   232→        request_data: Any,\n   233→        context: ServicerContext,\n   234→        method_name: str,\n   235→        request_streaming: bool = False,\n   236→        response_streaming: bool = True\n   237→    ) -> Any:\n   238→        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n   239→        # For Phase 3, we're focusing on request/response patterns\n   240→        # Streaming would be implemented in a future phase\n   241→        correlation_id = str(uuid4())\n   242→        # Store correlation ID in invocation metadata instead of setting attributes\n   243→        try:\n   244→            context.set_trailing_metadata([\n   245→                ('correlation_id', correlation_id),\n   246→                ('request_id', correlation_id)\n   247→            ])\n   248→        except Exception:\n   249→            # If metadata setting fails, continue without it\n   250→            pass\n   251→        \n   252→        self.logger.info(\n   253→            f\"[{correlation_id}] Streaming method {method_name} called \"\n   254→            f\"(streaming implementation pending)\"\n   255→        )\n   256→        \n   257→        return behavior(request_data, context)\n   258→    \n   259→    def _log_request(\n   260→        self, \n   261→        method_name: str, \n   262→        request: Any, \n   263→        correlation_id: str, \n   264→        context: ServicerContext\n   265→    ):\n   266→        \"\"\"Log incoming request as JSON string.\"\"\"\n   267→        try:\n   268→            # Extract and sanitize request data\n   269→            request_data = self._sanitize_data(self._extract_request_data(request))\n   270→            \n   271→            # Create simple log object\n   272→            log_obj = {\n   273→                'type': 'REQUEST',\n   274→                'method': method_name,\n   275→                'correlation_id': correlation_id,\n   276→                'timestamp': time.time(),\n   277→                'data': request_data\n   278→            }\n   279→            \n   280→            # Log as JSON string\n   281→            self.logger.info(f\"REQUEST: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n   282→            \n   283→        except Exception as e:\n   284→            self.logger.error(f\"Failed to log request for {method_name}: {e}\")\n   285→    \n   286→    def _log_response(\n   287→        self, \n   288→        method_name: str, \n   289→        response: Any, \n   290→        correlation_id: str, \n   291→        duration: float,\n   292→        context: ServicerContext\n   293→    ):\n   294→        \"\"\"Log outgoing response as JSON string.\"\"\"\n   295→        try:\n   296→            # Extract and sanitize response data\n   297→            response_data = self._sanitize_data(self._extract_response_data(response))\n   298→            \n   299→            # Create simple log object\n   300→            log_obj = {\n   301→                'type': 'RESPONSE',\n   302→                'method': method_name,\n   303→                'correlation_id': correlation_id,\n   304→                'duration_ms': round(duration * 1000, 2),\n   305→                'timestamp': time.time(),\n   306→                'data': response_data\n   307→            }\n   308→            \n   309→            # Log as JSON string\n   310→            self.logger.info(f\"RESPONSE: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n   311→            \n   312→        except Exception as e:\n   313→            self.logger.error(f\"Failed to log response for {method_name}: {e}\")\n   314→    \n   315→    def _log_error(\n   316→        self, \n   317→        method_name: str, \n   318→        error: Exception, \n   319→        correlation_id: str, \n   320→        duration: float,\n   321→        context: ServicerContext\n   322→    ):\n   323→        \"\"\"Log error with comprehensive context.\"\"\"\n   324→        \n   325→        log_data = {\n   326→            'event': 'grpc_error',\n   327→            'correlation_id': correlation_id,\n   328→            'method': method_name,\n   329→            'error_type': type(error).__name__,\n   330→            'error_message': str(error),\n   331→            'duration_ms': round(duration * 1000, 2),\n   332→            'timestamp': time.time()\n   333→        }\n   334→        \n   335→        self.logger.error(\n   336→            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n   337→            extra={'structured_data': log_data},\n   338→            exc_info=True\n   339→        )\n   340→    \n   341→    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n   342→        \"\"\"Log performance metrics.\"\"\"\n   343→        \n   344→        if duration > 1.0:  # Log slow requests (>1 second)\n   345→            self.logger.warning(\n   346→                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n   347→            )\n   348→    \n   349→    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n   350→        \"\"\"Extract relevant data from request for logging.\"\"\"\n   351→        try:\n   352→            if hasattr(request, 'DESCRIPTOR'):\n   353→                # Protobuf message - extract all fields including defaults\n   354→                result = {}\n   355→                for field in request.DESCRIPTOR.fields:\n   356→                    field_name = field.name\n   357→                    try:\n   358→                        value = getattr(request, field_name, None)\n   359→                        if value is not None:\n   360→                            if hasattr(value, 'DESCRIPTOR'):\n   361→                                # Nested protobuf message\n   362→                                result[field_name] = self._extract_request_data(value)\n   363→                            elif field.label == field.LABEL_REPEATED:\n   364→                                # Repeated field (list)\n   365→                                result[field_name] = [\n   366→                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n   367→                                    for item in value\n   368→                                ]\n   369→                            else:\n   370→                                result[field_name] = self._serialize_value(value)\n   371→                    except Exception as e:\n   372→                        result[field_name] = f\"<error: {str(e)}>\"\n   373→                return result\n   374→            elif hasattr(request, '__dict__'):\n   375→                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n   376→            else:\n   377→                return {'value': str(request)}\n   378→        except Exception:\n   379→            return {'error': 'Failed to extract request data'}\n   380→    \n   381→    def _serialize_value(self, value: Any) -> Any:\n   382→        \"\"\"Serialize a value for logging.\"\"\"\n   383→        try:\n   384→            if isinstance(value, (str, int, float, bool)):\n   385→                return value\n   386→            elif isinstance(value, bytes):\n   387→                import base64\n   388→                encoded = base64.b64encode(value).decode()\n   389→                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n   390→            elif hasattr(value, 'isoformat'):\n   391→                return value.isoformat()\n   392→            elif hasattr(value, 'name'):  # Enum value\n   393→                return value.name\n   394→            else:\n   395→                str_value = str(value)\n   396→                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n   397→        except Exception:\n   398→            return \"<non-serializable>\"\n   399→    \n   400→    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n   401→        \"\"\"Extract relevant data from response for logging.\"\"\"\n   402→        try:\n   403→            if hasattr(response, 'DESCRIPTOR'):\n   404→                # Protobuf message - extract all fields including defaults\n   405→                result = {}\n   406→                for field in response.DESCRIPTOR.fields:\n   407→                    field_name = field.name\n   408→                    try:\n   409→                        value = getattr(response, field_name, None)\n   410→                        if value is not None:\n   411→                            if hasattr(value, 'DESCRIPTOR'):\n   412→                                # Nested protobuf message\n   413→                                result[field_name] = self._extract_response_data(value)\n   414→                            elif field.label == field.LABEL_REPEATED:\n   415→                                # Repeated field (list)\n   416→                                result[field_name] = [\n   417→                                    self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n   418→                                    for item in value\n   419→                                ]\n   420→                            else:\n   421→                                result[field_name] = self._serialize_value(value)\n   422→                    except Exception as e:\n   423→                        result[field_name] = f\"<error: {str(e)}>\"\n   424→                return result\n   425→            elif hasattr(response, '__dict__'):\n   426→                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n   427→            else:\n   428→                return {'value': str(response)}\n   429→        except Exception:\n   430→            return {'error': 'Failed to extract response data'}\n   431→    \n   432→    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n   433→        \"\"\"Remove sensitive information from logged data.\"\"\"\n   434→        if not isinstance(data, dict):\n   435→            return data\n   436→        \n   437→        sanitized = {}\n   438→        for key, value in data.items():\n   439→            key_lower = key.lower()\n   440→            \n   441→            # Check if field contains sensitive information\n   442→            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n   443→                sanitized[key] = '***REDACTED***'\n   444→            elif isinstance(value, dict):\n   445→                sanitized[key] = self._sanitize_data(value)\n   446→            elif isinstance(value, (list, tuple)):\n   447→                sanitized[key] = [\n   448→                    self._sanitize_data(item) if isinstance(item, dict) else item\n   449→                    for item in value\n   450→                ]\n   451→            else:\n   452→                sanitized[key] = value\n   453→        \n   454→        return sanitized\n   455→    \n   456→    def _update_timing_metrics(self, method_name: str, duration: float):\n   457→        \"\"\"Update timing metrics for performance tracking.\"\"\"\n   458→        method_stats = self._metrics['method_stats'][method_name]\n   459→        current_avg = method_stats['avg_time']\n   460→        count = method_stats['count']\n   461→        \n   462→        # Update method-specific average\n   463→        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n   464→        \n   465→        # Update overall average\n   466→        total_requests = self._metrics['total_requests']\n   467→        current_overall_avg = self._metrics['avg_response_time']\n   468→        self._metrics['avg_response_time'] = (\n   469→            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n   470→        )\n   471→    \n   472→    def get_metrics(self) -> Dict[str, Any]:\n   473→        \"\"\"Get current performance metrics.\"\"\"\n   474→        return {\n   475→            'interceptor_name': 'LoggingInterceptor',\n   476→            'metrics': self._metrics.copy(),\n   477→            'timestamp': time.time()\n   478→        }\n   479→    \n   480→    def reset_metrics(self):\n   481→        \"\"\"Reset performance metrics.\"\"\"\n   482→        self._metrics = {\n   483→            'total_requests': 0,\n   484→            'successful_requests': 0,\n   485→            'failed_requests': 0,\n   486→            'avg_response_time': 0.0,\n   487→            'method_stats': {}\n   488→        }\n   489→\n   490→\n   491→# Convenience function for creating configured logging interceptor\n   492→def create_logging_interceptor(\n   493→    logger_name: str = \"grpc.logging\",\n   494→    log_level: int = logging.INFO,\n   495→    **kwargs\n   496→) -> LoggingInterceptor:\n   497→    \"\"\"Create a pre-configured logging interceptor.\n   498→    \n   499→    Args:\n   500→        logger_name: Name for the logger\n   501→        log_level: Logging level\n   502→        **kwargs: Additional configuration options\n   503→        \n   504→    Returns:\n   505→        Configured LoggingInterceptor instance\n   506→    \"\"\"\n   507→    logger = logging.getLogger(logger_name)\n   508→    return LoggingInterceptor(\n   509→        logger=logger,\n   510→        log_level=log_level,\n   511→        **kwargs\n   512→    )"}]},"uuid":"7767398a-c955-4847-97b4-e9c3e15a53fa","timestamp":"2025-08-20T02:03:48.651Z","toolUseResult":{"filePath":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py","oldString":"\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n\nThis interceptor provides:\n- Detailed request/response logging\n- Performance metrics tracking\n- Error logging and analysis\n- Request correlation and tracing\n- Structured logging with context\n\"\"\"\nimport json\nimport logging\nimport time\nfrom typing import Any, Callable, Iterator, Optional, Dict\nfrom uuid import uuid4\n\nimport grpc\nimport grpc.aio\nfrom grpc.aio import ServerInterceptor, ServicerContext\n\n\nclass LoggingInterceptor(grpc.aio.ServerInterceptor):\n    \"\"\"Advanced logging interceptor for gRPC services.\n    \n    Features:\n    - Request/response logging with structured data\n    - Performance metrics and timing\n    - Error tracking and analysis\n    - Request correlation IDs\n    - Configurable log levels and filters\n    - Security-aware logging (excludes sensitive data)\n    \"\"\"\n    \n    def __init__(\n        self,\n        logger: Optional[logging.Logger] = None,\n        log_level: int = logging.INFO,\n        log_requests: bool = True,\n        log_responses: bool = True,\n        log_errors: bool = True,\n        log_performance: bool = True,\n        exclude_methods: Optional[list] = None,\n        sensitive_fields: Optional[list] = None\n    ):\n        self.logger = logger or logging.getLogger(__name__)\n        self.log_level = log_level\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_errors = log_errors\n        self.log_performance = log_performance\n        self.exclude_methods = exclude_methods or []\n        self.sensitive_fields = sensitive_fields or [\n            'password', 'token', 'secret', 'key', 'credential', 'auth'\n        ]\n        \n        # Performance metrics\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n    \n    async def intercept_service(\n        self,\n        continuation: Callable[[Any], Any],\n        handler_call_details: Any,\n    ) -> Any:\n        \"\"\"Intercept service calls for logging.\"\"\"\n        \n        # Skip excluded methods\n        method_name = handler_call_details.method\n        if method_name in self.exclude_methods:\n            return await continuation(handler_call_details)\n        \n        # Get the original handler\n        handler = await continuation(handler_call_details)\n        if handler is None:\n            return None\n        \n        # Wrap the handler based on its type; bypass reflection and health entirely\n        if method_name in {\n            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n            '/grpc.health.v1.Health/Check',\n            '/grpc.health.v1.Health/Watch',\n        }:\n            return handler\n        if handler.unary_unary:\n            return grpc.unary_unary_rpc_method_handler(\n                self._wrap_unary_unary(handler.unary_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.unary_stream:\n            return grpc.unary_stream_rpc_method_handler(\n                self._wrap_unary_stream(handler.unary_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_unary:\n            return grpc.stream_unary_rpc_method_handler(\n                self._wrap_stream_unary(handler.stream_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_stream:\n            return grpc.stream_stream_rpc_method_handler(\n                self._wrap_stream_stream(handler.stream_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        else:\n            return handler\n    \n    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-unary RPC method.\"\"\"\n        \n        async def wrapper(request: Any, context: ServicerContext) -> Any:\n            return await self._handle_request(\n                behavior, request, context, method_name, is_streaming=False\n            )\n        \n        return wrapper\n    \n    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n        async def wrapper(request: Any, context: ServicerContext):\n            # For streaming responses, return the iterator directly (do not await)\n            try:\n                return behavior(request, context)\n            finally:\n                # Minimal logging to avoid interfering with stream lifecycle\n                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-unary RPC method.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n            # Request is streaming, response is unary; await the final response\n            try:\n                return await behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext):\n            # For bi-di streaming, return the response iterator directly\n            try:\n                return behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n        return wrapper\n    \n    async def _handle_request(\n        self, \n        behavior: Callable, \n        request: Any, \n        context: ServicerContext, \n        method_name: str,\n        is_streaming: bool = False\n    ) -> Any:\n        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n        \n        # Generate correlation ID\n        correlation_id = str(uuid4())\n        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            pass\n        \n        start_time = time.time()\n        \n        # Update metrics\n        self._metrics['total_requests'] += 1\n        if method_name not in self._metrics['method_stats']:\n            self._metrics['method_stats'][method_name] = {\n                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n            }\n        self._metrics['method_stats'][method_name]['count'] += 1\n        \n        try:\n            # Log request\n            if self.log_requests:\n                self._log_request(method_name, request, correlation_id, context)\n            \n            # Execute the actual method\n            response = await behavior(request, context)\n            \n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update success metrics\n            self._metrics['successful_requests'] += 1\n            self._metrics['method_stats'][method_name]['success'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log response\n            if self.log_responses:\n                self._log_response(method_name, response, correlation_id, duration, context)\n            \n            # Log performance\n            if self.log_performance:\n                self._log_performance(method_name, duration, correlation_id)\n            \n            return response\n            \n        except Exception as e:\n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update error metrics\n            self._metrics['failed_requests'] += 1\n            self._metrics['method_stats'][method_name]['error'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log error\n            if self.log_errors:\n                self._log_error(method_name, e, correlation_id, duration, context)\n            \n            # Re-raise the exception\n            raise\n    \n    async def _handle_streaming_request(\n        self,\n        behavior: Callable,\n        request_data: Any,\n        context: ServicerContext,\n        method_name: str,\n        request_streaming: bool = False,\n        response_streaming: bool = True\n    ) -> Any:\n        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n        # For Phase 3, we're focusing on request/response patterns\n        # Streaming would be implemented in a future phase\n        correlation_id = str(uuid4())\n        # Store correlation ID in invocation metadata instead of setting attributes\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            # If metadata setting fails, continue without it\n            pass\n        \n        self.logger.info(\n            f\"[{correlation_id}] Streaming method {method_name} called \"\n            f\"(streaming implementation pending)\"\n        )\n        \n        return behavior(request_data, context)\n    \n    def _log_request(\n        self, \n        method_name: str, \n        request: Any, \n        correlation_id: str, \n        context: ServicerContext\n    ):\n        \"\"\"Log incoming request as JSON string.\"\"\"\n        try:\n            # Extract and sanitize request data\n            request_data = self._sanitize_data(self._extract_request_data(request))\n            \n            # Create simple log object\n            log_obj = {\n                'type': 'REQUEST',\n                'method': method_name,\n                'correlation_id': correlation_id,\n                'timestamp': time.time(),\n                'data': request_data\n            }\n            \n            # Log as JSON string\n            self.logger.info(f\"REQUEST: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log request for {method_name}: {e}\")\n    \n    def _log_response(\n        self, \n        method_name: str, \n        response: Any, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log outgoing response with structured data.\"\"\"\n        \n        # Sanitize response data\n        response_data = self._sanitize_data(self._extract_response_data(response))\n        \n        log_data = {\n            'event': 'grpc_response',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'duration_ms': round(duration * 1000, 2),\n            'response_data': response_data,\n            'timestamp': time.time()\n        }\n        \n        self.logger.log(\n            self.log_level,\n            f\"[{correlation_id}] gRPC Response: {method_name} ({duration*1000:.2f}ms)\",\n            extra={'structured_data': log_data}\n        )\n    \n    def _log_error(\n        self, \n        method_name: str, \n        error: Exception, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log error with comprehensive context.\"\"\"\n        \n        log_data = {\n            'event': 'grpc_error',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'error_type': type(error).__name__,\n            'error_message': str(error),\n            'duration_ms': round(duration * 1000, 2),\n            'timestamp': time.time()\n        }\n        \n        self.logger.error(\n            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n            extra={'structured_data': log_data},\n            exc_info=True\n        )\n    \n    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n        \"\"\"Log performance metrics.\"\"\"\n        \n        if duration > 1.0:  # Log slow requests (>1 second)\n            self.logger.warning(\n                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n            )\n    \n    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(request, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_request_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}\n    \n    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a value for logging.\"\"\"\n        try:\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                import base64\n                encoded = base64.b64encode(value).decode()\n                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n            elif hasattr(value, 'isoformat'):\n                return value.isoformat()\n            elif hasattr(value, 'name'):  # Enum value\n                return value.name\n            else:\n                str_value = str(value)\n                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n        except Exception:\n            return \"<non-serializable>\"\n    \n    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(response, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_response_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}\n    \n    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove sensitive information from logged data.\"\"\"\n        if not isinstance(data, dict):\n            return data\n        \n        sanitized = {}\n        for key, value in data.items():\n            key_lower = key.lower()\n            \n            # Check if field contains sensitive information\n            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n                sanitized[key] = '***REDACTED***'\n            elif isinstance(value, dict):\n                sanitized[key] = self._sanitize_data(value)\n            elif isinstance(value, (list, tuple)):\n                sanitized[key] = [\n                    self._sanitize_data(item) if isinstance(item, dict) else item\n                    for item in value\n                ]\n            else:\n                sanitized[key] = value\n        \n        return sanitized\n    \n    def _update_timing_metrics(self, method_name: str, duration: float):\n        \"\"\"Update timing metrics for performance tracking.\"\"\"\n        method_stats = self._metrics['method_stats'][method_name]\n        current_avg = method_stats['avg_time']\n        count = method_stats['count']\n        \n        # Update method-specific average\n        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n        \n        # Update overall average\n        total_requests = self._metrics['total_requests']\n        current_overall_avg = self._metrics['avg_response_time']\n        self._metrics['avg_response_time'] = (\n            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n        )\n    \n    def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get current performance metrics.\"\"\"\n        return {\n            'interceptor_name': 'LoggingInterceptor',\n            'metrics': self._metrics.copy(),\n            'timestamp': time.time()\n        }\n    \n    def reset_metrics(self):\n        \"\"\"Reset performance metrics.\"\"\"\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n\n\n# Convenience function for creating configured logging interceptor\ndef create_logging_interceptor(\n    logger_name: str = \"grpc.logging\",\n    log_level: int = logging.INFO,\n    **kwargs\n) -> LoggingInterceptor:\n    \"\"\"Create a pre-configured logging interceptor.\n    \n    Args:\n        logger_name: Name for the logger\n        log_level: Logging level\n        **kwargs: Additional configuration options\n        \n    Returns:\n        Configured LoggingInterceptor instance\n    \"\"\"\n    logger = logging.getLogger(logger_name)\n    return LoggingInterceptor(\n        logger=logger,\n        log_level=log_level,\n        **kwargs\n    )","newString":"\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n\nThis interceptor provides:\n- Detailed request/response logging\n- Performance metrics tracking\n- Error logging and analysis\n- Request correlation and tracing\n- Structured logging with context\n\"\"\"\nimport json\nimport logging\nimport time\nfrom typing import Any, Callable, Iterator, Optional, Dict\nfrom uuid import uuid4\n\nimport grpc\nimport grpc.aio\nfrom grpc.aio import ServerInterceptor, ServicerContext\n\n\nclass LoggingInterceptor(grpc.aio.ServerInterceptor):\n    \"\"\"Advanced logging interceptor for gRPC services.\n    \n    Features:\n    - Request/response logging with structured data\n    - Performance metrics and timing\n    - Error tracking and analysis\n    - Request correlation IDs\n    - Configurable log levels and filters\n    - Security-aware logging (excludes sensitive data)\n    \"\"\"\n    \n    def __init__(\n        self,\n        logger: Optional[logging.Logger] = None,\n        log_level: int = logging.INFO,\n        log_requests: bool = True,\n        log_responses: bool = True,\n        log_errors: bool = True,\n        log_performance: bool = True,\n        exclude_methods: Optional[list] = None,\n        sensitive_fields: Optional[list] = None\n    ):\n        self.logger = logger or logging.getLogger(__name__)\n        self.log_level = log_level\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_errors = log_errors\n        self.log_performance = log_performance\n        self.exclude_methods = exclude_methods or []\n        self.sensitive_fields = sensitive_fields or [\n            'password', 'token', 'secret', 'key', 'credential', 'auth'\n        ]\n        \n        # Performance metrics\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n    \n    async def intercept_service(\n        self,\n        continuation: Callable[[Any], Any],\n        handler_call_details: Any,\n    ) -> Any:\n        \"\"\"Intercept service calls for logging.\"\"\"\n        \n        # Skip excluded methods\n        method_name = handler_call_details.method\n        if method_name in self.exclude_methods:\n            return await continuation(handler_call_details)\n        \n        # Get the original handler\n        handler = await continuation(handler_call_details)\n        if handler is None:\n            return None\n        \n        # Wrap the handler based on its type; bypass reflection and health entirely\n        if method_name in {\n            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n            '/grpc.health.v1.Health/Check',\n            '/grpc.health.v1.Health/Watch',\n        }:\n            return handler\n        if handler.unary_unary:\n            return grpc.unary_unary_rpc_method_handler(\n                self._wrap_unary_unary(handler.unary_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.unary_stream:\n            return grpc.unary_stream_rpc_method_handler(\n                self._wrap_unary_stream(handler.unary_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_unary:\n            return grpc.stream_unary_rpc_method_handler(\n                self._wrap_stream_unary(handler.stream_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_stream:\n            return grpc.stream_stream_rpc_method_handler(\n                self._wrap_stream_stream(handler.stream_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        else:\n            return handler\n    \n    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-unary RPC method.\"\"\"\n        \n        async def wrapper(request: Any, context: ServicerContext) -> Any:\n            return await self._handle_request(\n                behavior, request, context, method_name, is_streaming=False\n            )\n        \n        return wrapper\n    \n    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n        async def wrapper(request: Any, context: ServicerContext):\n            # For streaming responses, return the iterator directly (do not await)\n            try:\n                return behavior(request, context)\n            finally:\n                # Minimal logging to avoid interfering with stream lifecycle\n                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-unary RPC method.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n            # Request is streaming, response is unary; await the final response\n            try:\n                return await behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext):\n            # For bi-di streaming, return the response iterator directly\n            try:\n                return behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n        return wrapper\n    \n    async def _handle_request(\n        self, \n        behavior: Callable, \n        request: Any, \n        context: ServicerContext, \n        method_name: str,\n        is_streaming: bool = False\n    ) -> Any:\n        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n        \n        # Generate correlation ID\n        correlation_id = str(uuid4())\n        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            pass\n        \n        start_time = time.time()\n        \n        # Update metrics\n        self._metrics['total_requests'] += 1\n        if method_name not in self._metrics['method_stats']:\n            self._metrics['method_stats'][method_name] = {\n                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n            }\n        self._metrics['method_stats'][method_name]['count'] += 1\n        \n        try:\n            # Log request\n            if self.log_requests:\n                self._log_request(method_name, request, correlation_id, context)\n            \n            # Execute the actual method\n            response = await behavior(request, context)\n            \n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update success metrics\n            self._metrics['successful_requests'] += 1\n            self._metrics['method_stats'][method_name]['success'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log response\n            if self.log_responses:\n                self._log_response(method_name, response, correlation_id, duration, context)\n            \n            # Log performance\n            if self.log_performance:\n                self._log_performance(method_name, duration, correlation_id)\n            \n            return response\n            \n        except Exception as e:\n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update error metrics\n            self._metrics['failed_requests'] += 1\n            self._metrics['method_stats'][method_name]['error'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log error\n            if self.log_errors:\n                self._log_error(method_name, e, correlation_id, duration, context)\n            \n            # Re-raise the exception\n            raise\n    \n    async def _handle_streaming_request(\n        self,\n        behavior: Callable,\n        request_data: Any,\n        context: ServicerContext,\n        method_name: str,\n        request_streaming: bool = False,\n        response_streaming: bool = True\n    ) -> Any:\n        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n        # For Phase 3, we're focusing on request/response patterns\n        # Streaming would be implemented in a future phase\n        correlation_id = str(uuid4())\n        # Store correlation ID in invocation metadata instead of setting attributes\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            # If metadata setting fails, continue without it\n            pass\n        \n        self.logger.info(\n            f\"[{correlation_id}] Streaming method {method_name} called \"\n            f\"(streaming implementation pending)\"\n        )\n        \n        return behavior(request_data, context)\n    \n    def _log_request(\n        self, \n        method_name: str, \n        request: Any, \n        correlation_id: str, \n        context: ServicerContext\n    ):\n        \"\"\"Log incoming request as JSON string.\"\"\"\n        try:\n            # Extract and sanitize request data\n            request_data = self._sanitize_data(self._extract_request_data(request))\n            \n            # Create simple log object\n            log_obj = {\n                'type': 'REQUEST',\n                'method': method_name,\n                'correlation_id': correlation_id,\n                'timestamp': time.time(),\n                'data': request_data\n            }\n            \n            # Log as JSON string\n            self.logger.info(f\"REQUEST: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log request for {method_name}: {e}\")\n    \n    def _log_response(\n        self, \n        method_name: str, \n        response: Any, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log outgoing response as JSON string.\"\"\"\n        try:\n            # Extract and sanitize response data\n            response_data = self._sanitize_data(self._extract_response_data(response))\n            \n            # Create simple log object\n            log_obj = {\n                'type': 'RESPONSE',\n                'method': method_name,\n                'correlation_id': correlation_id,\n                'duration_ms': round(duration * 1000, 2),\n                'timestamp': time.time(),\n                'data': response_data\n            }\n            \n            # Log as JSON string\n            self.logger.info(f\"RESPONSE: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log response for {method_name}: {e}\")\n    \n    def _log_error(\n        self, \n        method_name: str, \n        error: Exception, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log error with comprehensive context.\"\"\"\n        \n        log_data = {\n            'event': 'grpc_error',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'error_type': type(error).__name__,\n            'error_message': str(error),\n            'duration_ms': round(duration * 1000, 2),\n            'timestamp': time.time()\n        }\n        \n        self.logger.error(\n            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n            extra={'structured_data': log_data},\n            exc_info=True\n        )\n    \n    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n        \"\"\"Log performance metrics.\"\"\"\n        \n        if duration > 1.0:  # Log slow requests (>1 second)\n            self.logger.warning(\n                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n            )\n    \n    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(request, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_request_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}\n    \n    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a value for logging.\"\"\"\n        try:\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                import base64\n                encoded = base64.b64encode(value).decode()\n                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n            elif hasattr(value, 'isoformat'):\n                return value.isoformat()\n            elif hasattr(value, 'name'):  # Enum value\n                return value.name\n            else:\n                str_value = str(value)\n                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n        except Exception:\n            return \"<non-serializable>\"\n    \n    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(response, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_response_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}\n    \n    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove sensitive information from logged data.\"\"\"\n        if not isinstance(data, dict):\n            return data\n        \n        sanitized = {}\n        for key, value in data.items():\n            key_lower = key.lower()\n            \n            # Check if field contains sensitive information\n            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n                sanitized[key] = '***REDACTED***'\n            elif isinstance(value, dict):\n                sanitized[key] = self._sanitize_data(value)\n            elif isinstance(value, (list, tuple)):\n                sanitized[key] = [\n                    self._sanitize_data(item) if isinstance(item, dict) else item\n                    for item in value\n                ]\n            else:\n                sanitized[key] = value\n        \n        return sanitized\n    \n    def _update_timing_metrics(self, method_name: str, duration: float):\n        \"\"\"Update timing metrics for performance tracking.\"\"\"\n        method_stats = self._metrics['method_stats'][method_name]\n        current_avg = method_stats['avg_time']\n        count = method_stats['count']\n        \n        # Update method-specific average\n        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n        \n        # Update overall average\n        total_requests = self._metrics['total_requests']\n        current_overall_avg = self._metrics['avg_response_time']\n        self._metrics['avg_response_time'] = (\n            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n        )\n    \n    def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get current performance metrics.\"\"\"\n        return {\n            'interceptor_name': 'LoggingInterceptor',\n            'metrics': self._metrics.copy(),\n            'timestamp': time.time()\n        }\n    \n    def reset_metrics(self):\n        \"\"\"Reset performance metrics.\"\"\"\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n\n\n# Convenience function for creating configured logging interceptor\ndef create_logging_interceptor(\n    logger_name: str = \"grpc.logging\",\n    log_level: int = logging.INFO,\n    **kwargs\n) -> LoggingInterceptor:\n    \"\"\"Create a pre-configured logging interceptor.\n    \n    Args:\n        logger_name: Name for the logger\n        log_level: Logging level\n        **kwargs: Additional configuration options\n        \n    Returns:\n        Configured LoggingInterceptor instance\n    \"\"\"\n    logger = logging.getLogger(logger_name)\n    return LoggingInterceptor(\n        logger=logger,\n        log_level=log_level,\n        **kwargs\n    )","originalFile":"\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n\nThis interceptor provides:\n- Detailed request/response logging\n- Performance metrics tracking\n- Error logging and analysis\n- Request correlation and tracing\n- Structured logging with context\n\"\"\"\nimport json\nimport logging\nimport time\nfrom typing import Any, Callable, Iterator, Optional, Dict\nfrom uuid import uuid4\n\nimport grpc\nimport grpc.aio\nfrom grpc.aio import ServerInterceptor, ServicerContext\n\n\nclass LoggingInterceptor(grpc.aio.ServerInterceptor):\n    \"\"\"Advanced logging interceptor for gRPC services.\n    \n    Features:\n    - Request/response logging with structured data\n    - Performance metrics and timing\n    - Error tracking and analysis\n    - Request correlation IDs\n    - Configurable log levels and filters\n    - Security-aware logging (excludes sensitive data)\n    \"\"\"\n    \n    def __init__(\n        self,\n        logger: Optional[logging.Logger] = None,\n        log_level: int = logging.INFO,\n        log_requests: bool = True,\n        log_responses: bool = True,\n        log_errors: bool = True,\n        log_performance: bool = True,\n        exclude_methods: Optional[list] = None,\n        sensitive_fields: Optional[list] = None\n    ):\n        self.logger = logger or logging.getLogger(__name__)\n        self.log_level = log_level\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_errors = log_errors\n        self.log_performance = log_performance\n        self.exclude_methods = exclude_methods or []\n        self.sensitive_fields = sensitive_fields or [\n            'password', 'token', 'secret', 'key', 'credential', 'auth'\n        ]\n        \n        # Performance metrics\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n    \n    async def intercept_service(\n        self,\n        continuation: Callable[[Any], Any],\n        handler_call_details: Any,\n    ) -> Any:\n        \"\"\"Intercept service calls for logging.\"\"\"\n        \n        # Skip excluded methods\n        method_name = handler_call_details.method\n        if method_name in self.exclude_methods:\n            return await continuation(handler_call_details)\n        \n        # Get the original handler\n        handler = await continuation(handler_call_details)\n        if handler is None:\n            return None\n        \n        # Wrap the handler based on its type; bypass reflection and health entirely\n        if method_name in {\n            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n            '/grpc.health.v1.Health/Check',\n            '/grpc.health.v1.Health/Watch',\n        }:\n            return handler\n        if handler.unary_unary:\n            return grpc.unary_unary_rpc_method_handler(\n                self._wrap_unary_unary(handler.unary_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.unary_stream:\n            return grpc.unary_stream_rpc_method_handler(\n                self._wrap_unary_stream(handler.unary_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_unary:\n            return grpc.stream_unary_rpc_method_handler(\n                self._wrap_stream_unary(handler.stream_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_stream:\n            return grpc.stream_stream_rpc_method_handler(\n                self._wrap_stream_stream(handler.stream_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        else:\n            return handler\n    \n    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-unary RPC method.\"\"\"\n        \n        async def wrapper(request: Any, context: ServicerContext) -> Any:\n            return await self._handle_request(\n                behavior, request, context, method_name, is_streaming=False\n            )\n        \n        return wrapper\n    \n    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n        async def wrapper(request: Any, context: ServicerContext):\n            # For streaming responses, return the iterator directly (do not await)\n            try:\n                return behavior(request, context)\n            finally:\n                # Minimal logging to avoid interfering with stream lifecycle\n                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-unary RPC method.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n            # Request is streaming, response is unary; await the final response\n            try:\n                return await behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext):\n            # For bi-di streaming, return the response iterator directly\n            try:\n                return behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n        return wrapper\n    \n    async def _handle_request(\n        self, \n        behavior: Callable, \n        request: Any, \n        context: ServicerContext, \n        method_name: str,\n        is_streaming: bool = False\n    ) -> Any:\n        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n        \n        # Generate correlation ID\n        correlation_id = str(uuid4())\n        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            pass\n        \n        start_time = time.time()\n        \n        # Update metrics\n        self._metrics['total_requests'] += 1\n        if method_name not in self._metrics['method_stats']:\n            self._metrics['method_stats'][method_name] = {\n                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n            }\n        self._metrics['method_stats'][method_name]['count'] += 1\n        \n        try:\n            # Log request\n            if self.log_requests:\n                self._log_request(method_name, request, correlation_id, context)\n            \n            # Execute the actual method\n            response = await behavior(request, context)\n            \n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update success metrics\n            self._metrics['successful_requests'] += 1\n            self._metrics['method_stats'][method_name]['success'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log response\n            if self.log_responses:\n                self._log_response(method_name, response, correlation_id, duration, context)\n            \n            # Log performance\n            if self.log_performance:\n                self._log_performance(method_name, duration, correlation_id)\n            \n            return response\n            \n        except Exception as e:\n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update error metrics\n            self._metrics['failed_requests'] += 1\n            self._metrics['method_stats'][method_name]['error'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log error\n            if self.log_errors:\n                self._log_error(method_name, e, correlation_id, duration, context)\n            \n            # Re-raise the exception\n            raise\n    \n    async def _handle_streaming_request(\n        self,\n        behavior: Callable,\n        request_data: Any,\n        context: ServicerContext,\n        method_name: str,\n        request_streaming: bool = False,\n        response_streaming: bool = True\n    ) -> Any:\n        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n        # For Phase 3, we're focusing on request/response patterns\n        # Streaming would be implemented in a future phase\n        correlation_id = str(uuid4())\n        # Store correlation ID in invocation metadata instead of setting attributes\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            # If metadata setting fails, continue without it\n            pass\n        \n        self.logger.info(\n            f\"[{correlation_id}] Streaming method {method_name} called \"\n            f\"(streaming implementation pending)\"\n        )\n        \n        return behavior(request_data, context)\n    \n    def _log_request(\n        self, \n        method_name: str, \n        request: Any, \n        correlation_id: str, \n        context: ServicerContext\n    ):\n        \"\"\"Log incoming request as JSON string.\"\"\"\n        try:\n            # Extract and sanitize request data\n            request_data = self._sanitize_data(self._extract_request_data(request))\n            \n            # Create simple log object\n            log_obj = {\n                'type': 'REQUEST',\n                'method': method_name,\n                'correlation_id': correlation_id,\n                'timestamp': time.time(),\n                'data': request_data\n            }\n            \n            # Log as JSON string\n            self.logger.info(f\"REQUEST: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log request for {method_name}: {e}\")\n    \n    def _log_response(\n        self, \n        method_name: str, \n        response: Any, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log outgoing response with structured data.\"\"\"\n        \n        # Sanitize response data\n        response_data = self._sanitize_data(self._extract_response_data(response))\n        \n        log_data = {\n            'event': 'grpc_response',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'duration_ms': round(duration * 1000, 2),\n            'response_data': response_data,\n            'timestamp': time.time()\n        }\n        \n        self.logger.log(\n            self.log_level,\n            f\"[{correlation_id}] gRPC Response: {method_name} ({duration*1000:.2f}ms)\",\n            extra={'structured_data': log_data}\n        )\n    \n    def _log_error(\n        self, \n        method_name: str, \n        error: Exception, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log error with comprehensive context.\"\"\"\n        \n        log_data = {\n            'event': 'grpc_error',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'error_type': type(error).__name__,\n            'error_message': str(error),\n            'duration_ms': round(duration * 1000, 2),\n            'timestamp': time.time()\n        }\n        \n        self.logger.error(\n            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n            extra={'structured_data': log_data},\n            exc_info=True\n        )\n    \n    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n        \"\"\"Log performance metrics.\"\"\"\n        \n        if duration > 1.0:  # Log slow requests (>1 second)\n            self.logger.warning(\n                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n            )\n    \n    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(request, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_request_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}\n    \n    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a value for logging.\"\"\"\n        try:\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                import base64\n                encoded = base64.b64encode(value).decode()\n                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n            elif hasattr(value, 'isoformat'):\n                return value.isoformat()\n            elif hasattr(value, 'name'):  # Enum value\n                return value.name\n            else:\n                str_value = str(value)\n                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n        except Exception:\n            return \"<non-serializable>\"\n    \n    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(response, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_response_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}\n    \n    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove sensitive information from logged data.\"\"\"\n        if not isinstance(data, dict):\n            return data\n        \n        sanitized = {}\n        for key, value in data.items():\n            key_lower = key.lower()\n            \n            # Check if field contains sensitive information\n            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n                sanitized[key] = '***REDACTED***'\n            elif isinstance(value, dict):\n                sanitized[key] = self._sanitize_data(value)\n            elif isinstance(value, (list, tuple)):\n                sanitized[key] = [\n                    self._sanitize_data(item) if isinstance(item, dict) else item\n                    for item in value\n                ]\n            else:\n                sanitized[key] = value\n        \n        return sanitized\n    \n    def _update_timing_metrics(self, method_name: str, duration: float):\n        \"\"\"Update timing metrics for performance tracking.\"\"\"\n        method_stats = self._metrics['method_stats'][method_name]\n        current_avg = method_stats['avg_time']\n        count = method_stats['count']\n        \n        # Update method-specific average\n        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n        \n        # Update overall average\n        total_requests = self._metrics['total_requests']\n        current_overall_avg = self._metrics['avg_response_time']\n        self._metrics['avg_response_time'] = (\n            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n        )\n    \n    def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get current performance metrics.\"\"\"\n        return {\n            'interceptor_name': 'LoggingInterceptor',\n            'metrics': self._metrics.copy(),\n            'timestamp': time.time()\n        }\n    \n    def reset_metrics(self):\n        \"\"\"Reset performance metrics.\"\"\"\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n\n\n# Convenience function for creating configured logging interceptor\ndef create_logging_interceptor(\n    logger_name: str = \"grpc.logging\",\n    log_level: int = logging.INFO,\n    **kwargs\n) -> LoggingInterceptor:\n    \"\"\"Create a pre-configured logging interceptor.\n    \n    Args:\n        logger_name: Name for the logger\n        log_level: Logging level\n        **kwargs: Additional configuration options\n        \n    Returns:\n        Configured LoggingInterceptor instance\n    \"\"\"\n    logger = logging.getLogger(logger_name)\n    return LoggingInterceptor(\n        logger=logger,\n        log_level=log_level,\n        **kwargs\n    )","structuredPatch":[{"oldStart":291,"oldLines":25,"newStart":291,"newLines":26,"lines":["         duration: float,","         context: ServicerContext","     ):","-        \"\"\"Log outgoing response with structured data.\"\"\"","-        ","-        # Sanitize response data","-        response_data = self._sanitize_data(self._extract_response_data(response))","-        ","-        log_data = {","-            'event': 'grpc_response',","-            'correlation_id': correlation_id,","-            'method': method_name,","-            'duration_ms': round(duration * 1000, 2),","-            'response_data': response_data,","-            'timestamp': time.time()","-        }","-        ","-        self.logger.log(","-            self.log_level,","-            f\"[{correlation_id}] gRPC Response: {method_name} ({duration*1000:.2f}ms)\",","-            extra={'structured_data': log_data}","-        )","+        \"\"\"Log outgoing response as JSON string.\"\"\"","+        try:","+            # Extract and sanitize response data","+            response_data = self._sanitize_data(self._extract_response_data(response))","+            ","+            # Create simple log object","+            log_obj = {","+                'type': 'RESPONSE',","+                'method': method_name,","+                'correlation_id': correlation_id,","+                'duration_ms': round(duration * 1000, 2),","+                'timestamp': time.time(),","+                'data': response_data","+            }","+            ","+            # Log as JSON string","+            self.logger.info(f\"RESPONSE: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")","+            ","+        except Exception as e:","+            self.logger.error(f\"Failed to log response for {method_name}: {e}\")","     ","     def _log_error(","         self, "]}],"userModified":false,"replaceAll":false}}
{"parentUuid":"7767398a-c955-4847-97b4-e9c3e15a53fa","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01QZmRjrngMt6J3RXYTGQdkN","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01ThuZXekTtK9zaa8cAK5Vng","name":"TodoWrite","input":{"todos":[{"id":"1","content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"completed","priority":"high"},{"id":"2","content":"Tạo interceptor gRPC để log request/response","status":"completed","priority":"high"},{"id":"3","content":"Thêm logging vào các servicer chính","status":"completed","priority":"high"},{"id":"4","content":"Cấu hình log format và level phù hợp","status":"completed","priority":"medium"},{"id":"5","content":"Kiểm tra và test cấu hình logging","status":"completed","priority":"low"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":7251,"cache_read_input_tokens":83666,"cache_creation":{"ephemeral_5m_input_tokens":7251,"ephemeral_1h_input_tokens":0},"output_tokens":335,"service_tier":"standard"}},"requestId":"req_011CSJ978LwruayZM31eUxBC","type":"assistant","uuid":"d0a8d8ef-67a1-4eda-af2c-c0a335629183","timestamp":"2025-08-20T02:03:58.421Z"}
{"parentUuid":"d0a8d8ef-67a1-4eda-af2c-c0a335629183","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01ThuZXekTtK9zaa8cAK5Vng","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"87e378ae-8589-4a4e-b5df-de7700534d78","timestamp":"2025-08-20T02:03:58.460Z","toolUseResult":{"oldTodos":[{"content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"completed","priority":"high","id":"1"},{"content":"Tạo interceptor gRPC để log request/response","status":"completed","priority":"high","id":"2"},{"content":"Thêm logging vào các servicer chính","status":"completed","priority":"high","id":"3"},{"content":"Cấu hình log format và level phù hợp","status":"completed","priority":"medium","id":"4"},{"content":"Kiểm tra và test cấu hình logging","status":"pending","priority":"low","id":"5"}],"newTodos":[{"content":"Kiểm tra cấu trúc logging hiện tại trong codebase","status":"completed","priority":"high","id":"1"},{"content":"Tạo interceptor gRPC để log request/response","status":"completed","priority":"high","id":"2"},{"content":"Thêm logging vào các servicer chính","status":"completed","priority":"high","id":"3"},{"content":"Cấu hình log format và level phù hợp","status":"completed","priority":"medium","id":"4"},{"content":"Kiểm tra và test cấu hình logging","status":"completed","priority":"low","id":"5"}]}}
{"parentUuid":"87e378ae-8589-4a4e-b5df-de7700534d78","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_013aeoLB7ESpMSc63RUhBFhQ","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Tôi đã hoàn thành việc cải thiện logging system cho bạn. Bây giờ hệ thống sẽ:\n\n## Tính năng đã cải thiện:\n\n1. **Log Request dưới dạng JSON**: Khi nhận request, interceptor sẽ log với format:\n   ```json\n   REQUEST: {\"type\": \"REQUEST\", \"method\": \"/order.OrderService/CreateOrder\", \"correlation_id\": \"uuid\", \"timestamp\": 1234567890, \"data\": {...}}\n   ```\n\n2. **Log Response dưới dạng JSON**: Trước khi trả response cho client:\n   ```json\n   RESPONSE: {\"type\": \"RESPONSE\", \"method\": \"/order.OrderService/CreateOrder\", \"correlation_id\": \"uuid\", \"duration_ms\": 123, \"timestamp\": 1234567890, \"data\": {...}}\n   ```\n\n3. **Trích xuất đầy đủ dữ liệu**: Interceptor trích xuất tất cả fields từ protobuf messages, bao gồm:\n   - Nested messages\n   - Repeated fields (arrays)\n   - Enum values\n   - Primitive types\n\n4. **Bảo mật**: Tự động ẩn các trường nhạy cảm như password, token, secret\n\n5. **Cấu hình logging tách biệt**:\n   - `grpc_server.log`: Tất cả logs\n   - `grpc_requests.log`: Chỉ request/response logs\n   - `grpc_errors.log`: Chỉ error logs\n\n6. **Correlation ID**: Mỗi request có một ID để track qua request-response cycle\n\nBây giờ khi bạn chạy server và gửi request, bạn sẽ thấy logs chi tiết dưới dạng JSON string cho cả request và response trong file `grpc_requests.log` và console."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":582,"cache_read_input_tokens":90917,"cache_creation":{"ephemeral_5m_input_tokens":582,"ephemeral_1h_input_tokens":0},"output_tokens":494,"service_tier":"standard"}},"requestId":"req_011CSJ97pfbgyS4HaTqnp9ky","type":"assistant","uuid":"95612bfe-7f49-4225-a843-90b8a6471965","timestamp":"2025-08-20T02:04:10.711Z"}
{"parentUuid":"95612bfe-7f49-4225-a843-90b8a6471965","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":"Da co log nhung chua chinh xac\nStarting enhanced gRPC server on port 55051...\nFeatures enabled: Enhanced Servicers, Interceptors, Health Checks, Metrics\n2025-08-20 09:04:49,521 - root - INFO - main:219 - Enhanced gRPC server starting with Phase 3 improvements\n2025-08-20 09:04:49,522 - src.presentation.grpc.monitoring.metrics_collector - INFO - start_collection:303 - Started metric collection (interval: 60.0s)\n2025-08-20 09:04:49,522 - src.presentation.grpc.server_config - INFO - setup_metrics:404 - Metrics collection enabled\n2025-08-20 09:04:49,522 - src.presentation.grpc.server_config - INFO - start_server:417 - gRPC server started on 0.0.0.0:55051\n2025-08-20 09:04:49,522 - src.presentation.grpc.server_config - INFO - _log_server_status:464 - Server Status: {'server_address': '0.0.0.0:55051', 'max_workers': 10, 'features': {'authentication': True, 'rate_limiting': False, 'detailed_logging': True, 'health_checks': True, 'metrics': True, 'reflection': True}, 'services': ['UserService', 'BrokerAccountService', 'MarketDataService', 'OrderService', 'HealthService']}\n2025-08-20 09:04:52,438 - grpc.requests - INFO - _log_request:281 - REQUEST: {\"type\": \"REQUEST\", \"method\": \"/order.OrderService/CreateOrder\", \"correlation_id\": \"e9d3d5a6-8c57-4c1f-9336-105732bcca12\", \"timestamp\": 1755655492.438339, \"data\": {\"request_id\": \"\", \"timestamp\": \"\", \"data\": {\"account_number\": \"\", \"symbol\": \"\", \"market_id\": \"\", \"side\": 0, \"order_type\": 0, \"price\": 0.0, \"quantity\": 0, \"channel\": \"\", \"is_stop_order\": false, \"stop_price\": 0.0, \"stop_type\": \"\", \"stop_step\": 0.0}}}\n2025-08-20 09:04:52,439 - src.presentation.grpc.servicers.order_servicer - INFO - _request_context:61 - [337677e6-b2c7-41b1-991e-79ad234fbc17] Starting CreateOrder\n2025-08-20 09:04:52,439 - src.presentation.grpc.servicers.order_servicer - WARNING - CreateOrder:320 - [337677e6-b2c7-41b1-991e-79ad234fbc17] Validation error in CreateOrder: MISSING_SYMBOL: Symbol is required\n2025-08-20 09:04:52,439 - src.presentation.grpc.servicers.order_servicer - INFO - _request_context:73 - [337677e6-b2c7-41b1-991e-79ad234fbc17] Completed CreateOrder in 0.001s\n2025-08-20 09:04:52,440 - grpc.requests - INFO - _log_response:310 - RESPONSE: {\"type\": \"RESPONSE\", \"method\": \"/order.OrderService/CreateOrder\", \"correlation_id\": \"e9d3d5a6-8c57-4c1f-9336-105732bcca12\", \"duration_ms\": 1.68, \"timestamp\": 1755655492.440077, \"data\": {\"success\": false, \"message\": \"MISSING_SYMBOL: Symbol is required\", \"order\": {\"id\": \"\", \"account_id\": \"\", \"broker_order_id\": \"\", \"request_id\": \"\", \"symbol\": \"\", \"market_id\": \"\", \"side\": 0, \"order_type\": 0, \"price\": 0.0, \"quantity\": 0, \"filled_quantity\": 0, \"cancelled_quantity\": 0, \"remaining_quantity\": 0, \"average_price\": 0.0, \"status\": 0, \"broker_status\": \"\", \"channel\": \"\", \"is_stop_order\": false, \"stop_price\": 0.0, \"stop_type\": \"\", \"stop_step\": 0.0, \"created_at\": {\"seconds\": 0, \"nanos\": 0}, \"sent_at\": {\"seconds\": 0, \"nanos\": 0}, \"filled_at\": {\"seconds\": 0, \"nanos\": 0}, \"cancelled_at\": {\"seconds\": 0, \"nanos\": 0}, \"updated_at\": {\"seconds\": 0, \"nanos\": 0}, \"reject_reason\": \"\"}}}\n\nRequest thuc te\n{\n      \"request_id\": \"ba9c425a-0667-4534-9d73-c2c87e686c3b\",\n      \"timestamp\": \"2025-01-19T10:30:00Z\",\n      \"data\": {\n        \"account_number\": \"acc-123456\",\n        \"symbol\": \"VIC\",\n        \"market_id\": \"HOSE\",\n        \"side\": \"ORDER_SIDE_BUY\",\n        \"order_type\": \"ORDER_TYPE_LO\",\n        \"price\": 85000,\n        \"quantity\": 100,\n        \"channel\": \"API\"\n      }\n    }\nResponse thuc te\n{\n    \"success\": false,\n    \"message\": \"MISSING_SYMBOL: Symbol is required\",\n    \"order\": null\n}"},"uuid":"a1b4cc88-621e-4c80-9be5-e4001dbdd17d","timestamp":"2025-08-20T02:07:13.398Z"}
{"parentUuid":"a1b4cc88-621e-4c80-9be5-e4001dbdd17d","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_014QZKJt7DzZVdRTjoCV2ptX","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Tôi thấy vấn đề rồi. Interceptor đang log các field default của protobuf (có giá trị 0, \"\", false) thay vì chỉ log các field thực tế có giá trị. Hãy tôi sửa lại để chỉ log các field có giá trị thực tế:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":1941,"cache_read_input_tokens":91499,"cache_creation":{"ephemeral_5m_input_tokens":1941,"ephemeral_1h_input_tokens":0},"output_tokens":5,"service_tier":"standard"}},"requestId":"req_011CSJ9NC1NiK16WvB68zEuQ","type":"assistant","uuid":"9a50ef72-8202-404a-88ca-b856125e20dd","timestamp":"2025-08-20T02:07:19.521Z"}
{"parentUuid":"9a50ef72-8202-404a-88ca-b856125e20dd","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_014QZKJt7DzZVdRTjoCV2ptX","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01NpDrUeSPLzsVy7MK1J8yKw","name":"Edit","input":{"file_path":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py","old_string":"    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(request, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_request_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}","new_string":"    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract only fields with actual values\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        # Check if field has been set (not default value)\n                        if hasattr(request, 'HasField') and request.HasField(field_name):\n                            value = getattr(request, field_name)\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                nested_data = self._extract_request_data(value)\n                                if nested_data:  # Only add if not empty\n                                    result[field_name] = nested_data\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                if value:  # Only if list is not empty\n                                    result[field_name] = [\n                                        self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                        for item in value\n                                    ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                        else:\n                            # For fields without HasField (primitives), check if not default\n                            value = getattr(request, field_name, None)\n                            if self._is_not_default_value(value, field):\n                                if hasattr(value, 'DESCRIPTOR'):\n                                    nested_data = self._extract_request_data(value)\n                                    if nested_data:\n                                        result[field_name] = nested_data\n                                elif field.label == field.LABEL_REPEATED:\n                                    if value:\n                                        result[field_name] = [\n                                            self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                            for item in value\n                                        ]\n                                else:\n                                    result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        # Only log errors for non-empty fields\n                        if str(e) != \"Message does not have field\":\n                            result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}\n    \n    def _is_not_default_value(self, value: Any, field: Any) -> bool:\n        \"\"\"Check if value is not the default value for the field type.\"\"\"\n        try:\n            if value is None:\n                return False\n            \n            # Check based on field type\n            if field.type == field.TYPE_STRING:\n                return value != \"\"\n            elif field.type in [field.TYPE_INT32, field.TYPE_INT64, field.TYPE_UINT32, field.TYPE_UINT64]:\n                return value != 0\n            elif field.type in [field.TYPE_FLOAT, field.TYPE_DOUBLE]:\n                return value != 0.0\n            elif field.type == field.TYPE_BOOL:\n                return value != False\n            elif field.type == field.TYPE_ENUM:\n                return value != 0\n            else:\n                return True\n        except:\n            return True"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":1941,"cache_read_input_tokens":91499,"cache_creation":{"ephemeral_5m_input_tokens":1941,"ephemeral_1h_input_tokens":0},"output_tokens":1504,"service_tier":"standard"}},"requestId":"req_011CSJ9NC1NiK16WvB68zEuQ","type":"assistant","uuid":"e546908c-83aa-4040-b686-52835d8954c5","timestamp":"2025-08-20T02:07:31.450Z"}
{"parentUuid":"e546908c-83aa-4040-b686-52835d8954c5","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01NpDrUeSPLzsVy7MK1J8yKw","type":"tool_result","content":"The file /Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   345→            self.logger.warning(\n   346→                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n   347→            )\n   348→    \n   349→    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n   350→        \"\"\"Extract relevant data from request for logging.\"\"\"\n   351→        try:\n   352→            if hasattr(request, 'DESCRIPTOR'):\n   353→                # Protobuf message - extract only fields with actual values\n   354→                result = {}\n   355→                for field in request.DESCRIPTOR.fields:\n   356→                    field_name = field.name\n   357→                    try:\n   358→                        # Check if field has been set (not default value)\n   359→                        if hasattr(request, 'HasField') and request.HasField(field_name):\n   360→                            value = getattr(request, field_name)\n   361→                            if hasattr(value, 'DESCRIPTOR'):\n   362→                                # Nested protobuf message\n   363→                                nested_data = self._extract_request_data(value)\n   364→                                if nested_data:  # Only add if not empty\n   365→                                    result[field_name] = nested_data\n   366→                            elif field.label == field.LABEL_REPEATED:\n   367→                                # Repeated field (list)\n   368→                                if value:  # Only if list is not empty\n   369→                                    result[field_name] = [\n   370→                                        self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n   371→                                        for item in value\n   372→                                    ]\n   373→                            else:\n   374→                                result[field_name] = self._serialize_value(value)\n   375→                        else:\n   376→                            # For fields without HasField (primitives), check if not default\n   377→                            value = getattr(request, field_name, None)\n   378→                            if self._is_not_default_value(value, field):\n   379→                                if hasattr(value, 'DESCRIPTOR'):\n   380→                                    nested_data = self._extract_request_data(value)\n   381→                                    if nested_data:\n   382→                                        result[field_name] = nested_data\n   383→                                elif field.label == field.LABEL_REPEATED:\n   384→                                    if value:\n   385→                                        result[field_name] = [\n   386→                                            self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n   387→                                            for item in value\n   388→                                        ]\n   389→                                else:\n   390→                                    result[field_name] = self._serialize_value(value)\n   391→                    except Exception as e:\n   392→                        # Only log errors for non-empty fields\n   393→                        if str(e) != \"Message does not have field\":\n   394→                            result[field_name] = f\"<error: {str(e)}>\"\n   395→                return result\n   396→            elif hasattr(request, '__dict__'):\n   397→                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n   398→            else:\n   399→                return {'value': str(request)}\n   400→        except Exception:\n   401→            return {'error': 'Failed to extract request data'}\n   402→    \n   403→    def _is_not_default_value(self, value: Any, field: Any) -> bool:\n   404→        \"\"\"Check if value is not the default value for the field type.\"\"\"\n   405→        try:\n   406→            if value is None:\n   407→                return False\n   408→            \n   409→            # Check based on field type\n   410→            if field.type == field.TYPE_STRING:\n   411→                return value != \"\"\n   412→            elif field.type in [field.TYPE_INT32, field.TYPE_INT64, field.TYPE_UINT32, field.TYPE_UINT64]:\n   413→                return value != 0\n   414→            elif field.type in [field.TYPE_FLOAT, field.TYPE_DOUBLE]:\n   415→                return value != 0.0\n   416→            elif field.type == field.TYPE_BOOL:\n   417→                return value != False\n   418→            elif field.type == field.TYPE_ENUM:\n   419→                return value != 0\n   420→            else:\n   421→                return True\n   422→        except:\n   423→            return True\n   424→    \n   425→    def _serialize_value(self, value: Any) -> Any:\n   426→        \"\"\"Serialize a value for logging.\"\"\"\n   427→        try:"}]},"uuid":"d80cabad-66f0-469d-b62f-ac7445efb8ed","timestamp":"2025-08-20T02:07:58.705Z","toolUseResult":{"filePath":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py","oldString":"    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(request, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_request_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}","newString":"    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract only fields with actual values\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        # Check if field has been set (not default value)\n                        if hasattr(request, 'HasField') and request.HasField(field_name):\n                            value = getattr(request, field_name)\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                nested_data = self._extract_request_data(value)\n                                if nested_data:  # Only add if not empty\n                                    result[field_name] = nested_data\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                if value:  # Only if list is not empty\n                                    result[field_name] = [\n                                        self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                        for item in value\n                                    ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                        else:\n                            # For fields without HasField (primitives), check if not default\n                            value = getattr(request, field_name, None)\n                            if self._is_not_default_value(value, field):\n                                if hasattr(value, 'DESCRIPTOR'):\n                                    nested_data = self._extract_request_data(value)\n                                    if nested_data:\n                                        result[field_name] = nested_data\n                                elif field.label == field.LABEL_REPEATED:\n                                    if value:\n                                        result[field_name] = [\n                                            self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                            for item in value\n                                        ]\n                                else:\n                                    result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        # Only log errors for non-empty fields\n                        if str(e) != \"Message does not have field\":\n                            result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}\n    \n    def _is_not_default_value(self, value: Any, field: Any) -> bool:\n        \"\"\"Check if value is not the default value for the field type.\"\"\"\n        try:\n            if value is None:\n                return False\n            \n            # Check based on field type\n            if field.type == field.TYPE_STRING:\n                return value != \"\"\n            elif field.type in [field.TYPE_INT32, field.TYPE_INT64, field.TYPE_UINT32, field.TYPE_UINT64]:\n                return value != 0\n            elif field.type in [field.TYPE_FLOAT, field.TYPE_DOUBLE]:\n                return value != 0.0\n            elif field.type == field.TYPE_BOOL:\n                return value != False\n            elif field.type == field.TYPE_ENUM:\n                return value != 0\n            else:\n                return True\n        except:\n            return True","originalFile":"\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n\nThis interceptor provides:\n- Detailed request/response logging\n- Performance metrics tracking\n- Error logging and analysis\n- Request correlation and tracing\n- Structured logging with context\n\"\"\"\nimport json\nimport logging\nimport time\nfrom typing import Any, Callable, Iterator, Optional, Dict\nfrom uuid import uuid4\n\nimport grpc\nimport grpc.aio\nfrom grpc.aio import ServerInterceptor, ServicerContext\n\n\nclass LoggingInterceptor(grpc.aio.ServerInterceptor):\n    \"\"\"Advanced logging interceptor for gRPC services.\n    \n    Features:\n    - Request/response logging with structured data\n    - Performance metrics and timing\n    - Error tracking and analysis\n    - Request correlation IDs\n    - Configurable log levels and filters\n    - Security-aware logging (excludes sensitive data)\n    \"\"\"\n    \n    def __init__(\n        self,\n        logger: Optional[logging.Logger] = None,\n        log_level: int = logging.INFO,\n        log_requests: bool = True,\n        log_responses: bool = True,\n        log_errors: bool = True,\n        log_performance: bool = True,\n        exclude_methods: Optional[list] = None,\n        sensitive_fields: Optional[list] = None\n    ):\n        self.logger = logger or logging.getLogger(__name__)\n        self.log_level = log_level\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_errors = log_errors\n        self.log_performance = log_performance\n        self.exclude_methods = exclude_methods or []\n        self.sensitive_fields = sensitive_fields or [\n            'password', 'token', 'secret', 'key', 'credential', 'auth'\n        ]\n        \n        # Performance metrics\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n    \n    async def intercept_service(\n        self,\n        continuation: Callable[[Any], Any],\n        handler_call_details: Any,\n    ) -> Any:\n        \"\"\"Intercept service calls for logging.\"\"\"\n        \n        # Skip excluded methods\n        method_name = handler_call_details.method\n        if method_name in self.exclude_methods:\n            return await continuation(handler_call_details)\n        \n        # Get the original handler\n        handler = await continuation(handler_call_details)\n        if handler is None:\n            return None\n        \n        # Wrap the handler based on its type; bypass reflection and health entirely\n        if method_name in {\n            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n            '/grpc.health.v1.Health/Check',\n            '/grpc.health.v1.Health/Watch',\n        }:\n            return handler\n        if handler.unary_unary:\n            return grpc.unary_unary_rpc_method_handler(\n                self._wrap_unary_unary(handler.unary_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.unary_stream:\n            return grpc.unary_stream_rpc_method_handler(\n                self._wrap_unary_stream(handler.unary_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_unary:\n            return grpc.stream_unary_rpc_method_handler(\n                self._wrap_stream_unary(handler.stream_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_stream:\n            return grpc.stream_stream_rpc_method_handler(\n                self._wrap_stream_stream(handler.stream_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        else:\n            return handler\n    \n    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-unary RPC method.\"\"\"\n        \n        async def wrapper(request: Any, context: ServicerContext) -> Any:\n            return await self._handle_request(\n                behavior, request, context, method_name, is_streaming=False\n            )\n        \n        return wrapper\n    \n    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n        async def wrapper(request: Any, context: ServicerContext):\n            # For streaming responses, return the iterator directly (do not await)\n            try:\n                return behavior(request, context)\n            finally:\n                # Minimal logging to avoid interfering with stream lifecycle\n                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-unary RPC method.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n            # Request is streaming, response is unary; await the final response\n            try:\n                return await behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext):\n            # For bi-di streaming, return the response iterator directly\n            try:\n                return behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n        return wrapper\n    \n    async def _handle_request(\n        self, \n        behavior: Callable, \n        request: Any, \n        context: ServicerContext, \n        method_name: str,\n        is_streaming: bool = False\n    ) -> Any:\n        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n        \n        # Generate correlation ID\n        correlation_id = str(uuid4())\n        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            pass\n        \n        start_time = time.time()\n        \n        # Update metrics\n        self._metrics['total_requests'] += 1\n        if method_name not in self._metrics['method_stats']:\n            self._metrics['method_stats'][method_name] = {\n                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n            }\n        self._metrics['method_stats'][method_name]['count'] += 1\n        \n        try:\n            # Log request\n            if self.log_requests:\n                self._log_request(method_name, request, correlation_id, context)\n            \n            # Execute the actual method\n            response = await behavior(request, context)\n            \n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update success metrics\n            self._metrics['successful_requests'] += 1\n            self._metrics['method_stats'][method_name]['success'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log response\n            if self.log_responses:\n                self._log_response(method_name, response, correlation_id, duration, context)\n            \n            # Log performance\n            if self.log_performance:\n                self._log_performance(method_name, duration, correlation_id)\n            \n            return response\n            \n        except Exception as e:\n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update error metrics\n            self._metrics['failed_requests'] += 1\n            self._metrics['method_stats'][method_name]['error'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log error\n            if self.log_errors:\n                self._log_error(method_name, e, correlation_id, duration, context)\n            \n            # Re-raise the exception\n            raise\n    \n    async def _handle_streaming_request(\n        self,\n        behavior: Callable,\n        request_data: Any,\n        context: ServicerContext,\n        method_name: str,\n        request_streaming: bool = False,\n        response_streaming: bool = True\n    ) -> Any:\n        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n        # For Phase 3, we're focusing on request/response patterns\n        # Streaming would be implemented in a future phase\n        correlation_id = str(uuid4())\n        # Store correlation ID in invocation metadata instead of setting attributes\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            # If metadata setting fails, continue without it\n            pass\n        \n        self.logger.info(\n            f\"[{correlation_id}] Streaming method {method_name} called \"\n            f\"(streaming implementation pending)\"\n        )\n        \n        return behavior(request_data, context)\n    \n    def _log_request(\n        self, \n        method_name: str, \n        request: Any, \n        correlation_id: str, \n        context: ServicerContext\n    ):\n        \"\"\"Log incoming request as JSON string.\"\"\"\n        try:\n            # Extract and sanitize request data\n            request_data = self._sanitize_data(self._extract_request_data(request))\n            \n            # Create simple log object\n            log_obj = {\n                'type': 'REQUEST',\n                'method': method_name,\n                'correlation_id': correlation_id,\n                'timestamp': time.time(),\n                'data': request_data\n            }\n            \n            # Log as JSON string\n            self.logger.info(f\"REQUEST: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log request for {method_name}: {e}\")\n    \n    def _log_response(\n        self, \n        method_name: str, \n        response: Any, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log outgoing response as JSON string.\"\"\"\n        try:\n            # Extract and sanitize response data\n            response_data = self._sanitize_data(self._extract_response_data(response))\n            \n            # Create simple log object\n            log_obj = {\n                'type': 'RESPONSE',\n                'method': method_name,\n                'correlation_id': correlation_id,\n                'duration_ms': round(duration * 1000, 2),\n                'timestamp': time.time(),\n                'data': response_data\n            }\n            \n            # Log as JSON string\n            self.logger.info(f\"RESPONSE: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log response for {method_name}: {e}\")\n    \n    def _log_error(\n        self, \n        method_name: str, \n        error: Exception, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log error with comprehensive context.\"\"\"\n        \n        log_data = {\n            'event': 'grpc_error',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'error_type': type(error).__name__,\n            'error_message': str(error),\n            'duration_ms': round(duration * 1000, 2),\n            'timestamp': time.time()\n        }\n        \n        self.logger.error(\n            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n            extra={'structured_data': log_data},\n            exc_info=True\n        )\n    \n    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n        \"\"\"Log performance metrics.\"\"\"\n        \n        if duration > 1.0:  # Log slow requests (>1 second)\n            self.logger.warning(\n                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n            )\n    \n    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(request, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_request_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}\n    \n    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a value for logging.\"\"\"\n        try:\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                import base64\n                encoded = base64.b64encode(value).decode()\n                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n            elif hasattr(value, 'isoformat'):\n                return value.isoformat()\n            elif hasattr(value, 'name'):  # Enum value\n                return value.name\n            else:\n                str_value = str(value)\n                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n        except Exception:\n            return \"<non-serializable>\"\n    \n    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(response, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_response_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}\n    \n    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove sensitive information from logged data.\"\"\"\n        if not isinstance(data, dict):\n            return data\n        \n        sanitized = {}\n        for key, value in data.items():\n            key_lower = key.lower()\n            \n            # Check if field contains sensitive information\n            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n                sanitized[key] = '***REDACTED***'\n            elif isinstance(value, dict):\n                sanitized[key] = self._sanitize_data(value)\n            elif isinstance(value, (list, tuple)):\n                sanitized[key] = [\n                    self._sanitize_data(item) if isinstance(item, dict) else item\n                    for item in value\n                ]\n            else:\n                sanitized[key] = value\n        \n        return sanitized\n    \n    def _update_timing_metrics(self, method_name: str, duration: float):\n        \"\"\"Update timing metrics for performance tracking.\"\"\"\n        method_stats = self._metrics['method_stats'][method_name]\n        current_avg = method_stats['avg_time']\n        count = method_stats['count']\n        \n        # Update method-specific average\n        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n        \n        # Update overall average\n        total_requests = self._metrics['total_requests']\n        current_overall_avg = self._metrics['avg_response_time']\n        self._metrics['avg_response_time'] = (\n            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n        )\n    \n    def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get current performance metrics.\"\"\"\n        return {\n            'interceptor_name': 'LoggingInterceptor',\n            'metrics': self._metrics.copy(),\n            'timestamp': time.time()\n        }\n    \n    def reset_metrics(self):\n        \"\"\"Reset performance metrics.\"\"\"\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n\n\n# Convenience function for creating configured logging interceptor\ndef create_logging_interceptor(\n    logger_name: str = \"grpc.logging\",\n    log_level: int = logging.INFO,\n    **kwargs\n) -> LoggingInterceptor:\n    \"\"\"Create a pre-configured logging interceptor.\n    \n    Args:\n        logger_name: Name for the logger\n        log_level: Logging level\n        **kwargs: Additional configuration options\n        \n    Returns:\n        Configured LoggingInterceptor instance\n    \"\"\"\n    logger = logging.getLogger(logger_name)\n    return LoggingInterceptor(\n        logger=logger,\n        log_level=log_level,\n        **kwargs\n    )","structuredPatch":[{"oldStart":350,"oldLines":26,"newStart":350,"newLines":48,"lines":["         \"\"\"Extract relevant data from request for logging.\"\"\"","         try:","             if hasattr(request, 'DESCRIPTOR'):","-                # Protobuf message - extract all fields including defaults","+                # Protobuf message - extract only fields with actual values","                 result = {}","                 for field in request.DESCRIPTOR.fields:","                     field_name = field.name","                     try:","-                        value = getattr(request, field_name, None)","-                        if value is not None:","+                        # Check if field has been set (not default value)","+                        if hasattr(request, 'HasField') and request.HasField(field_name):","+                            value = getattr(request, field_name)","                             if hasattr(value, 'DESCRIPTOR'):","                                 # Nested protobuf message","-                                result[field_name] = self._extract_request_data(value)","+                                nested_data = self._extract_request_data(value)","+                                if nested_data:  # Only add if not empty","+                                    result[field_name] = nested_data","                             elif field.label == field.LABEL_REPEATED:","                                 # Repeated field (list)","-                                result[field_name] = [","-                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)","-                                    for item in value","-                                ]","+                                if value:  # Only if list is not empty","+                                    result[field_name] = [","+                                        self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)","+                                        for item in value","+                                    ]","                             else:","                                 result[field_name] = self._serialize_value(value)","+                        else:","+                            # For fields without HasField (primitives), check if not default","+                            value = getattr(request, field_name, None)","+                            if self._is_not_default_value(value, field):","+                                if hasattr(value, 'DESCRIPTOR'):","+                                    nested_data = self._extract_request_data(value)","+                                    if nested_data:","+                                        result[field_name] = nested_data","+                                elif field.label == field.LABEL_REPEATED:","+                                    if value:","+                                        result[field_name] = [","+                                            self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)","+                                            for item in value","+                                        ]","+                                else:","+                                    result[field_name] = self._serialize_value(value)","                     except Exception as e:","-                        result[field_name] = f\"<error: {str(e)}>\"","+                        # Only log errors for non-empty fields","+                        if str(e) != \"Message does not have field\":","+                            result[field_name] = f\"<error: {str(e)}>\"","                 return result","             elif hasattr(request, '__dict__'):","                 return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}"]},{"oldStart":378,"oldLines":6,"newStart":400,"newLines":28,"lines":["         except Exception:","             return {'error': 'Failed to extract request data'}","     ","+    def _is_not_default_value(self, value: Any, field: Any) -> bool:","+        \"\"\"Check if value is not the default value for the field type.\"\"\"","+        try:","+            if value is None:","+                return False","+            ","+            # Check based on field type","+            if field.type == field.TYPE_STRING:","+                return value != \"\"","+            elif field.type in [field.TYPE_INT32, field.TYPE_INT64, field.TYPE_UINT32, field.TYPE_UINT64]:","+                return value != 0","+            elif field.type in [field.TYPE_FLOAT, field.TYPE_DOUBLE]:","+                return value != 0.0","+            elif field.type == field.TYPE_BOOL:","+                return value != False","+            elif field.type == field.TYPE_ENUM:","+                return value != 0","+            else:","+                return True","+        except:","+            return True","+    ","     def _serialize_value(self, value: Any) -> Any:","         \"\"\"Serialize a value for logging.\"\"\"","         try:"]}],"userModified":false,"replaceAll":false}}
{"parentUuid":"d80cabad-66f0-469d-b62f-ac7445efb8ed","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01UzbpM8Fkw9Y5sxJoqQhTQZ","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Bây giờ tôi cần cập nhật `_extract_response_data` với logic tương tự:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":2815,"cache_read_input_tokens":93440,"cache_creation":{"ephemeral_5m_input_tokens":2815,"ephemeral_1h_input_tokens":0},"output_tokens":2,"service_tier":"standard"}},"requestId":"req_011CSJ9RZ4F9A766Hpi6p5v5","type":"assistant","uuid":"923e6970-cab6-4042-aa5d-5285c9a67f92","timestamp":"2025-08-20T02:08:04.194Z"}
{"parentUuid":"923e6970-cab6-4042-aa5d-5285c9a67f92","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01UzbpM8Fkw9Y5sxJoqQhTQZ","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_018fCdfToV55S7WwBSX6C2g9","name":"Edit","input":{"file_path":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py","old_string":"    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(response, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_response_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}","new_string":"    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message - extract only fields with actual values\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        # Check if field has been set (not default value)\n                        if hasattr(response, 'HasField') and response.HasField(field_name):\n                            value = getattr(response, field_name)\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                nested_data = self._extract_response_data(value)\n                                if nested_data:  # Only add if not empty\n                                    result[field_name] = nested_data\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                if value:  # Only if list is not empty\n                                    result[field_name] = [\n                                        self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                        for item in value\n                                    ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                        else:\n                            # For fields without HasField (primitives), check if not default\n                            value = getattr(response, field_name, None)\n                            if self._is_not_default_value(value, field):\n                                if hasattr(value, 'DESCRIPTOR'):\n                                    nested_data = self._extract_response_data(value)\n                                    if nested_data:\n                                        result[field_name] = nested_data\n                                elif field.label == field.LABEL_REPEATED:\n                                    if value:\n                                        result[field_name] = [\n                                            self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                            for item in value\n                                        ]\n                                else:\n                                    result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        # Only log errors for non-empty fields\n                        if str(e) != \"Message does not have field\":\n                            result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":2815,"cache_read_input_tokens":93440,"cache_creation":{"ephemeral_5m_input_tokens":2815,"ephemeral_1h_input_tokens":0},"output_tokens":1222,"service_tier":"standard"}},"requestId":"req_011CSJ9RZ4F9A766Hpi6p5v5","type":"assistant","uuid":"edbf901d-2bf8-421f-9549-f548b492621f","timestamp":"2025-08-20T02:08:12.910Z"}
{"parentUuid":"edbf901d-2bf8-421f-9549-f548b492621f","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_018fCdfToV55S7WwBSX6C2g9","type":"tool_result","content":"The file /Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   440→                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n   441→        except Exception:\n   442→            return \"<non-serializable>\"\n   443→    \n   444→    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n   445→        \"\"\"Extract relevant data from response for logging.\"\"\"\n   446→        try:\n   447→            if hasattr(response, 'DESCRIPTOR'):\n   448→                # Protobuf message - extract only fields with actual values\n   449→                result = {}\n   450→                for field in response.DESCRIPTOR.fields:\n   451→                    field_name = field.name\n   452→                    try:\n   453→                        # Check if field has been set (not default value)\n   454→                        if hasattr(response, 'HasField') and response.HasField(field_name):\n   455→                            value = getattr(response, field_name)\n   456→                            if hasattr(value, 'DESCRIPTOR'):\n   457→                                # Nested protobuf message\n   458→                                nested_data = self._extract_response_data(value)\n   459→                                if nested_data:  # Only add if not empty\n   460→                                    result[field_name] = nested_data\n   461→                            elif field.label == field.LABEL_REPEATED:\n   462→                                # Repeated field (list)\n   463→                                if value:  # Only if list is not empty\n   464→                                    result[field_name] = [\n   465→                                        self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n   466→                                        for item in value\n   467→                                    ]\n   468→                            else:\n   469→                                result[field_name] = self._serialize_value(value)\n   470→                        else:\n   471→                            # For fields without HasField (primitives), check if not default\n   472→                            value = getattr(response, field_name, None)\n   473→                            if self._is_not_default_value(value, field):\n   474→                                if hasattr(value, 'DESCRIPTOR'):\n   475→                                    nested_data = self._extract_response_data(value)\n   476→                                    if nested_data:\n   477→                                        result[field_name] = nested_data\n   478→                                elif field.label == field.LABEL_REPEATED:\n   479→                                    if value:\n   480→                                        result[field_name] = [\n   481→                                            self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n   482→                                            for item in value\n   483→                                        ]\n   484→                                else:\n   485→                                    result[field_name] = self._serialize_value(value)\n   486→                    except Exception as e:\n   487→                        # Only log errors for non-empty fields\n   488→                        if str(e) != \"Message does not have field\":\n   489→                            result[field_name] = f\"<error: {str(e)}>\"\n   490→                return result\n   491→            elif hasattr(response, '__dict__'):\n   492→                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n   493→            else:\n   494→                return {'value': str(response)}\n   495→        except Exception:\n   496→            return {'error': 'Failed to extract response data'}\n   497→    \n   498→    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n   499→        \"\"\"Remove sensitive information from logged data.\"\"\"\n   500→        if not isinstance(data, dict):"}]},"uuid":"d7ab13c8-4cc9-4e81-bcc5-a76535536032","timestamp":"2025-08-20T02:09:20.189Z","toolUseResult":{"filePath":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py","oldString":"    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(response, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_response_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}","newString":"    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message - extract only fields with actual values\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        # Check if field has been set (not default value)\n                        if hasattr(response, 'HasField') and response.HasField(field_name):\n                            value = getattr(response, field_name)\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                nested_data = self._extract_response_data(value)\n                                if nested_data:  # Only add if not empty\n                                    result[field_name] = nested_data\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                if value:  # Only if list is not empty\n                                    result[field_name] = [\n                                        self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                        for item in value\n                                    ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                        else:\n                            # For fields without HasField (primitives), check if not default\n                            value = getattr(response, field_name, None)\n                            if self._is_not_default_value(value, field):\n                                if hasattr(value, 'DESCRIPTOR'):\n                                    nested_data = self._extract_response_data(value)\n                                    if nested_data:\n                                        result[field_name] = nested_data\n                                elif field.label == field.LABEL_REPEATED:\n                                    if value:\n                                        result[field_name] = [\n                                            self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                            for item in value\n                                        ]\n                                else:\n                                    result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        # Only log errors for non-empty fields\n                        if str(e) != \"Message does not have field\":\n                            result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}","originalFile":"\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n\nThis interceptor provides:\n- Detailed request/response logging\n- Performance metrics tracking\n- Error logging and analysis\n- Request correlation and tracing\n- Structured logging with context\n\"\"\"\nimport json\nimport logging\nimport time\nfrom typing import Any, Callable, Iterator, Optional, Dict\nfrom uuid import uuid4\n\nimport grpc\nimport grpc.aio\nfrom grpc.aio import ServerInterceptor, ServicerContext\n\n\nclass LoggingInterceptor(grpc.aio.ServerInterceptor):\n    \"\"\"Advanced logging interceptor for gRPC services.\n    \n    Features:\n    - Request/response logging with structured data\n    - Performance metrics and timing\n    - Error tracking and analysis\n    - Request correlation IDs\n    - Configurable log levels and filters\n    - Security-aware logging (excludes sensitive data)\n    \"\"\"\n    \n    def __init__(\n        self,\n        logger: Optional[logging.Logger] = None,\n        log_level: int = logging.INFO,\n        log_requests: bool = True,\n        log_responses: bool = True,\n        log_errors: bool = True,\n        log_performance: bool = True,\n        exclude_methods: Optional[list] = None,\n        sensitive_fields: Optional[list] = None\n    ):\n        self.logger = logger or logging.getLogger(__name__)\n        self.log_level = log_level\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_errors = log_errors\n        self.log_performance = log_performance\n        self.exclude_methods = exclude_methods or []\n        self.sensitive_fields = sensitive_fields or [\n            'password', 'token', 'secret', 'key', 'credential', 'auth'\n        ]\n        \n        # Performance metrics\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n    \n    async def intercept_service(\n        self,\n        continuation: Callable[[Any], Any],\n        handler_call_details: Any,\n    ) -> Any:\n        \"\"\"Intercept service calls for logging.\"\"\"\n        \n        # Skip excluded methods\n        method_name = handler_call_details.method\n        if method_name in self.exclude_methods:\n            return await continuation(handler_call_details)\n        \n        # Get the original handler\n        handler = await continuation(handler_call_details)\n        if handler is None:\n            return None\n        \n        # Wrap the handler based on its type; bypass reflection and health entirely\n        if method_name in {\n            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n            '/grpc.health.v1.Health/Check',\n            '/grpc.health.v1.Health/Watch',\n        }:\n            return handler\n        if handler.unary_unary:\n            return grpc.unary_unary_rpc_method_handler(\n                self._wrap_unary_unary(handler.unary_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.unary_stream:\n            return grpc.unary_stream_rpc_method_handler(\n                self._wrap_unary_stream(handler.unary_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_unary:\n            return grpc.stream_unary_rpc_method_handler(\n                self._wrap_stream_unary(handler.stream_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_stream:\n            return grpc.stream_stream_rpc_method_handler(\n                self._wrap_stream_stream(handler.stream_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        else:\n            return handler\n    \n    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-unary RPC method.\"\"\"\n        \n        async def wrapper(request: Any, context: ServicerContext) -> Any:\n            return await self._handle_request(\n                behavior, request, context, method_name, is_streaming=False\n            )\n        \n        return wrapper\n    \n    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n        async def wrapper(request: Any, context: ServicerContext):\n            # For streaming responses, return the iterator directly (do not await)\n            try:\n                return behavior(request, context)\n            finally:\n                # Minimal logging to avoid interfering with stream lifecycle\n                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-unary RPC method.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n            # Request is streaming, response is unary; await the final response\n            try:\n                return await behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext):\n            # For bi-di streaming, return the response iterator directly\n            try:\n                return behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n        return wrapper\n    \n    async def _handle_request(\n        self, \n        behavior: Callable, \n        request: Any, \n        context: ServicerContext, \n        method_name: str,\n        is_streaming: bool = False\n    ) -> Any:\n        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n        \n        # Generate correlation ID\n        correlation_id = str(uuid4())\n        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            pass\n        \n        start_time = time.time()\n        \n        # Update metrics\n        self._metrics['total_requests'] += 1\n        if method_name not in self._metrics['method_stats']:\n            self._metrics['method_stats'][method_name] = {\n                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n            }\n        self._metrics['method_stats'][method_name]['count'] += 1\n        \n        try:\n            # Log request\n            if self.log_requests:\n                self._log_request(method_name, request, correlation_id, context)\n            \n            # Execute the actual method\n            response = await behavior(request, context)\n            \n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update success metrics\n            self._metrics['successful_requests'] += 1\n            self._metrics['method_stats'][method_name]['success'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log response\n            if self.log_responses:\n                self._log_response(method_name, response, correlation_id, duration, context)\n            \n            # Log performance\n            if self.log_performance:\n                self._log_performance(method_name, duration, correlation_id)\n            \n            return response\n            \n        except Exception as e:\n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update error metrics\n            self._metrics['failed_requests'] += 1\n            self._metrics['method_stats'][method_name]['error'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log error\n            if self.log_errors:\n                self._log_error(method_name, e, correlation_id, duration, context)\n            \n            # Re-raise the exception\n            raise\n    \n    async def _handle_streaming_request(\n        self,\n        behavior: Callable,\n        request_data: Any,\n        context: ServicerContext,\n        method_name: str,\n        request_streaming: bool = False,\n        response_streaming: bool = True\n    ) -> Any:\n        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n        # For Phase 3, we're focusing on request/response patterns\n        # Streaming would be implemented in a future phase\n        correlation_id = str(uuid4())\n        # Store correlation ID in invocation metadata instead of setting attributes\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            # If metadata setting fails, continue without it\n            pass\n        \n        self.logger.info(\n            f\"[{correlation_id}] Streaming method {method_name} called \"\n            f\"(streaming implementation pending)\"\n        )\n        \n        return behavior(request_data, context)\n    \n    def _log_request(\n        self, \n        method_name: str, \n        request: Any, \n        correlation_id: str, \n        context: ServicerContext\n    ):\n        \"\"\"Log incoming request as JSON string.\"\"\"\n        try:\n            # Extract and sanitize request data\n            request_data = self._sanitize_data(self._extract_request_data(request))\n            \n            # Create simple log object\n            log_obj = {\n                'type': 'REQUEST',\n                'method': method_name,\n                'correlation_id': correlation_id,\n                'timestamp': time.time(),\n                'data': request_data\n            }\n            \n            # Log as JSON string\n            self.logger.info(f\"REQUEST: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log request for {method_name}: {e}\")\n    \n    def _log_response(\n        self, \n        method_name: str, \n        response: Any, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log outgoing response as JSON string.\"\"\"\n        try:\n            # Extract and sanitize response data\n            response_data = self._sanitize_data(self._extract_response_data(response))\n            \n            # Create simple log object\n            log_obj = {\n                'type': 'RESPONSE',\n                'method': method_name,\n                'correlation_id': correlation_id,\n                'duration_ms': round(duration * 1000, 2),\n                'timestamp': time.time(),\n                'data': response_data\n            }\n            \n            # Log as JSON string\n            self.logger.info(f\"RESPONSE: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log response for {method_name}: {e}\")\n    \n    def _log_error(\n        self, \n        method_name: str, \n        error: Exception, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log error with comprehensive context.\"\"\"\n        \n        log_data = {\n            'event': 'grpc_error',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'error_type': type(error).__name__,\n            'error_message': str(error),\n            'duration_ms': round(duration * 1000, 2),\n            'timestamp': time.time()\n        }\n        \n        self.logger.error(\n            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n            extra={'structured_data': log_data},\n            exc_info=True\n        )\n    \n    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n        \"\"\"Log performance metrics.\"\"\"\n        \n        if duration > 1.0:  # Log slow requests (>1 second)\n            self.logger.warning(\n                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n            )\n    \n    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract only fields with actual values\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        # Check if field has been set (not default value)\n                        if hasattr(request, 'HasField') and request.HasField(field_name):\n                            value = getattr(request, field_name)\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                nested_data = self._extract_request_data(value)\n                                if nested_data:  # Only add if not empty\n                                    result[field_name] = nested_data\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                if value:  # Only if list is not empty\n                                    result[field_name] = [\n                                        self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                        for item in value\n                                    ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                        else:\n                            # For fields without HasField (primitives), check if not default\n                            value = getattr(request, field_name, None)\n                            if self._is_not_default_value(value, field):\n                                if hasattr(value, 'DESCRIPTOR'):\n                                    nested_data = self._extract_request_data(value)\n                                    if nested_data:\n                                        result[field_name] = nested_data\n                                elif field.label == field.LABEL_REPEATED:\n                                    if value:\n                                        result[field_name] = [\n                                            self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                            for item in value\n                                        ]\n                                else:\n                                    result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        # Only log errors for non-empty fields\n                        if str(e) != \"Message does not have field\":\n                            result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}\n    \n    def _is_not_default_value(self, value: Any, field: Any) -> bool:\n        \"\"\"Check if value is not the default value for the field type.\"\"\"\n        try:\n            if value is None:\n                return False\n            \n            # Check based on field type\n            if field.type == field.TYPE_STRING:\n                return value != \"\"\n            elif field.type in [field.TYPE_INT32, field.TYPE_INT64, field.TYPE_UINT32, field.TYPE_UINT64]:\n                return value != 0\n            elif field.type in [field.TYPE_FLOAT, field.TYPE_DOUBLE]:\n                return value != 0.0\n            elif field.type == field.TYPE_BOOL:\n                return value != False\n            elif field.type == field.TYPE_ENUM:\n                return value != 0\n            else:\n                return True\n        except:\n            return True\n    \n    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a value for logging.\"\"\"\n        try:\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                import base64\n                encoded = base64.b64encode(value).decode()\n                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n            elif hasattr(value, 'isoformat'):\n                return value.isoformat()\n            elif hasattr(value, 'name'):  # Enum value\n                return value.name\n            else:\n                str_value = str(value)\n                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n        except Exception:\n            return \"<non-serializable>\"\n    \n    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message - extract all fields including defaults\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(response, field_name, None)\n                        if value is not None:\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                result[field_name] = self._extract_response_data(value)\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                result[field_name] = [\n                                    self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}\n    \n    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove sensitive information from logged data.\"\"\"\n        if not isinstance(data, dict):\n            return data\n        \n        sanitized = {}\n        for key, value in data.items():\n            key_lower = key.lower()\n            \n            # Check if field contains sensitive information\n            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n                sanitized[key] = '***REDACTED***'\n            elif isinstance(value, dict):\n                sanitized[key] = self._sanitize_data(value)\n            elif isinstance(value, (list, tuple)):\n                sanitized[key] = [\n                    self._sanitize_data(item) if isinstance(item, dict) else item\n                    for item in value\n                ]\n            else:\n                sanitized[key] = value\n        \n        return sanitized\n    \n    def _update_timing_metrics(self, method_name: str, duration: float):\n        \"\"\"Update timing metrics for performance tracking.\"\"\"\n        method_stats = self._metrics['method_stats'][method_name]\n        current_avg = method_stats['avg_time']\n        count = method_stats['count']\n        \n        # Update method-specific average\n        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n        \n        # Update overall average\n        total_requests = self._metrics['total_requests']\n        current_overall_avg = self._metrics['avg_response_time']\n        self._metrics['avg_response_time'] = (\n            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n        )\n    \n    def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get current performance metrics.\"\"\"\n        return {\n            'interceptor_name': 'LoggingInterceptor',\n            'metrics': self._metrics.copy(),\n            'timestamp': time.time()\n        }\n    \n    def reset_metrics(self):\n        \"\"\"Reset performance metrics.\"\"\"\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n\n\n# Convenience function for creating configured logging interceptor\ndef create_logging_interceptor(\n    logger_name: str = \"grpc.logging\",\n    log_level: int = logging.INFO,\n    **kwargs\n) -> LoggingInterceptor:\n    \"\"\"Create a pre-configured logging interceptor.\n    \n    Args:\n        logger_name: Name for the logger\n        log_level: Logging level\n        **kwargs: Additional configuration options\n        \n    Returns:\n        Configured LoggingInterceptor instance\n    \"\"\"\n    logger = logging.getLogger(logger_name)\n    return LoggingInterceptor(\n        logger=logger,\n        log_level=log_level,\n        **kwargs\n    )","structuredPatch":[{"oldStart":445,"oldLines":26,"newStart":445,"newLines":48,"lines":["         \"\"\"Extract relevant data from response for logging.\"\"\"","         try:","             if hasattr(response, 'DESCRIPTOR'):","-                # Protobuf message - extract all fields including defaults","+                # Protobuf message - extract only fields with actual values","                 result = {}","                 for field in response.DESCRIPTOR.fields:","                     field_name = field.name","                     try:","-                        value = getattr(response, field_name, None)","-                        if value is not None:","+                        # Check if field has been set (not default value)","+                        if hasattr(response, 'HasField') and response.HasField(field_name):","+                            value = getattr(response, field_name)","                             if hasattr(value, 'DESCRIPTOR'):","                                 # Nested protobuf message","-                                result[field_name] = self._extract_response_data(value)","+                                nested_data = self._extract_response_data(value)","+                                if nested_data:  # Only add if not empty","+                                    result[field_name] = nested_data","                             elif field.label == field.LABEL_REPEATED:","                                 # Repeated field (list)","-                                result[field_name] = [","-                                    self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)","-                                    for item in value","-                                ]","+                                if value:  # Only if list is not empty","+                                    result[field_name] = [","+                                        self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)","+                                        for item in value","+                                    ]","                             else:","                                 result[field_name] = self._serialize_value(value)","+                        else:","+                            # For fields without HasField (primitives), check if not default","+                            value = getattr(response, field_name, None)","+                            if self._is_not_default_value(value, field):","+                                if hasattr(value, 'DESCRIPTOR'):","+                                    nested_data = self._extract_response_data(value)","+                                    if nested_data:","+                                        result[field_name] = nested_data","+                                elif field.label == field.LABEL_REPEATED:","+                                    if value:","+                                        result[field_name] = [","+                                            self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)","+                                            for item in value","+                                        ]","+                                else:","+                                    result[field_name] = self._serialize_value(value)","                     except Exception as e:","-                        result[field_name] = f\"<error: {str(e)}>\"","+                        # Only log errors for non-empty fields","+                        if str(e) != \"Message does not have field\":","+                            result[field_name] = f\"<error: {str(e)}>\"","                 return result","             elif hasattr(response, '__dict__'):","                 return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}"]}],"userModified":false,"replaceAll":false}}
{"parentUuid":"d7ab13c8-4cc9-4e81-bcc5-a76535536032","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01DTj1uerLv6wBHTXFYDKH3L","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Tôi cần cập nhật `_serialize_value` để handle enum values tốt hơn:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":2262,"cache_read_input_tokens":96255,"cache_creation":{"ephemeral_5m_input_tokens":2262,"ephemeral_1h_input_tokens":0},"output_tokens":5,"service_tier":"standard"}},"requestId":"req_011CSJ9XZG4cND6iUzmnvjfd","type":"assistant","uuid":"e2f22cdc-b2dc-4394-87ac-ead415ea8f0a","timestamp":"2025-08-20T02:09:26.289Z"}
{"parentUuid":"e2f22cdc-b2dc-4394-87ac-ead415ea8f0a","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01DTj1uerLv6wBHTXFYDKH3L","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01A2ECk8uL9a57E9XXCCcYDu","name":"Edit","input":{"file_path":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py","old_string":"    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a value for logging.\"\"\"\n        try:\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                import base64\n                encoded = base64.b64encode(value).decode()\n                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n            elif hasattr(value, 'isoformat'):\n                return value.isoformat()\n            elif hasattr(value, 'name'):  # Enum value\n                return value.name\n            else:\n                str_value = str(value)\n                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n        except Exception:\n            return \"<non-serializable>\"","new_string":"    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a value for logging.\"\"\"\n        try:\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                import base64\n                encoded = base64.b64encode(value).decode()\n                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n            elif hasattr(value, 'isoformat'):\n                return value.isoformat()\n            elif hasattr(value, 'name') and hasattr(value, 'value'):  # Enum value\n                return value.name if value.name else value.value\n            elif hasattr(value, '__class__') and 'enum' in str(type(value)).lower():\n                # Handle enum values that might not have name attribute\n                return str(value)\n            else:\n                str_value = str(value)\n                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n        except Exception:\n            return \"<non-serializable>\""}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":2262,"cache_read_input_tokens":96255,"cache_creation":{"ephemeral_5m_input_tokens":2262,"ephemeral_1h_input_tokens":0},"output_tokens":632,"service_tier":"standard"}},"requestId":"req_011CSJ9XZG4cND6iUzmnvjfd","type":"assistant","uuid":"475cad5c-388e-4433-9205-498feb2c963c","timestamp":"2025-08-20T02:09:31.017Z"}
{"parentUuid":"475cad5c-388e-4433-9205-498feb2c963c","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01A2ECk8uL9a57E9XXCCcYDu","type":"tool_result","content":"The file /Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   421→                return True\n   422→        except:\n   423→            return True\n   424→    \n   425→    def _serialize_value(self, value: Any) -> Any:\n   426→        \"\"\"Serialize a value for logging.\"\"\"\n   427→        try:\n   428→            if isinstance(value, (str, int, float, bool)):\n   429→                return value\n   430→            elif isinstance(value, bytes):\n   431→                import base64\n   432→                encoded = base64.b64encode(value).decode()\n   433→                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n   434→            elif hasattr(value, 'isoformat'):\n   435→                return value.isoformat()\n   436→            elif hasattr(value, 'name') and hasattr(value, 'value'):  # Enum value\n   437→                return value.name if value.name else value.value\n   438→            elif hasattr(value, '__class__') and 'enum' in str(type(value)).lower():\n   439→                # Handle enum values that might not have name attribute\n   440→                return str(value)\n   441→            else:\n   442→                str_value = str(value)\n   443→                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n   444→        except Exception:\n   445→            return \"<non-serializable>\"\n   446→    \n   447→    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n   448→        \"\"\"Extract relevant data from response for logging.\"\"\"\n   449→        try:"}]},"uuid":"359a81b3-5567-4d09-8f36-b503ebcce15b","timestamp":"2025-08-20T02:09:52.315Z","toolUseResult":{"filePath":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py","oldString":"    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a value for logging.\"\"\"\n        try:\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                import base64\n                encoded = base64.b64encode(value).decode()\n                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n            elif hasattr(value, 'isoformat'):\n                return value.isoformat()\n            elif hasattr(value, 'name'):  # Enum value\n                return value.name\n            else:\n                str_value = str(value)\n                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n        except Exception:\n            return \"<non-serializable>\"","newString":"    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a value for logging.\"\"\"\n        try:\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                import base64\n                encoded = base64.b64encode(value).decode()\n                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n            elif hasattr(value, 'isoformat'):\n                return value.isoformat()\n            elif hasattr(value, 'name') and hasattr(value, 'value'):  # Enum value\n                return value.name if value.name else value.value\n            elif hasattr(value, '__class__') and 'enum' in str(type(value)).lower():\n                # Handle enum values that might not have name attribute\n                return str(value)\n            else:\n                str_value = str(value)\n                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n        except Exception:\n            return \"<non-serializable>\"","originalFile":"\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n\nThis interceptor provides:\n- Detailed request/response logging\n- Performance metrics tracking\n- Error logging and analysis\n- Request correlation and tracing\n- Structured logging with context\n\"\"\"\nimport json\nimport logging\nimport time\nfrom typing import Any, Callable, Iterator, Optional, Dict\nfrom uuid import uuid4\n\nimport grpc\nimport grpc.aio\nfrom grpc.aio import ServerInterceptor, ServicerContext\n\n\nclass LoggingInterceptor(grpc.aio.ServerInterceptor):\n    \"\"\"Advanced logging interceptor for gRPC services.\n    \n    Features:\n    - Request/response logging with structured data\n    - Performance metrics and timing\n    - Error tracking and analysis\n    - Request correlation IDs\n    - Configurable log levels and filters\n    - Security-aware logging (excludes sensitive data)\n    \"\"\"\n    \n    def __init__(\n        self,\n        logger: Optional[logging.Logger] = None,\n        log_level: int = logging.INFO,\n        log_requests: bool = True,\n        log_responses: bool = True,\n        log_errors: bool = True,\n        log_performance: bool = True,\n        exclude_methods: Optional[list] = None,\n        sensitive_fields: Optional[list] = None\n    ):\n        self.logger = logger or logging.getLogger(__name__)\n        self.log_level = log_level\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_errors = log_errors\n        self.log_performance = log_performance\n        self.exclude_methods = exclude_methods or []\n        self.sensitive_fields = sensitive_fields or [\n            'password', 'token', 'secret', 'key', 'credential', 'auth'\n        ]\n        \n        # Performance metrics\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n    \n    async def intercept_service(\n        self,\n        continuation: Callable[[Any], Any],\n        handler_call_details: Any,\n    ) -> Any:\n        \"\"\"Intercept service calls for logging.\"\"\"\n        \n        # Skip excluded methods\n        method_name = handler_call_details.method\n        if method_name in self.exclude_methods:\n            return await continuation(handler_call_details)\n        \n        # Get the original handler\n        handler = await continuation(handler_call_details)\n        if handler is None:\n            return None\n        \n        # Wrap the handler based on its type; bypass reflection and health entirely\n        if method_name in {\n            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n            '/grpc.health.v1.Health/Check',\n            '/grpc.health.v1.Health/Watch',\n        }:\n            return handler\n        if handler.unary_unary:\n            return grpc.unary_unary_rpc_method_handler(\n                self._wrap_unary_unary(handler.unary_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.unary_stream:\n            return grpc.unary_stream_rpc_method_handler(\n                self._wrap_unary_stream(handler.unary_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_unary:\n            return grpc.stream_unary_rpc_method_handler(\n                self._wrap_stream_unary(handler.stream_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_stream:\n            return grpc.stream_stream_rpc_method_handler(\n                self._wrap_stream_stream(handler.stream_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        else:\n            return handler\n    \n    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-unary RPC method.\"\"\"\n        \n        async def wrapper(request: Any, context: ServicerContext) -> Any:\n            return await self._handle_request(\n                behavior, request, context, method_name, is_streaming=False\n            )\n        \n        return wrapper\n    \n    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n        async def wrapper(request: Any, context: ServicerContext):\n            # For streaming responses, return the iterator directly (do not await)\n            try:\n                return behavior(request, context)\n            finally:\n                # Minimal logging to avoid interfering with stream lifecycle\n                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-unary RPC method.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n            # Request is streaming, response is unary; await the final response\n            try:\n                return await behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext):\n            # For bi-di streaming, return the response iterator directly\n            try:\n                return behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n        return wrapper\n    \n    async def _handle_request(\n        self, \n        behavior: Callable, \n        request: Any, \n        context: ServicerContext, \n        method_name: str,\n        is_streaming: bool = False\n    ) -> Any:\n        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n        \n        # Generate correlation ID\n        correlation_id = str(uuid4())\n        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            pass\n        \n        start_time = time.time()\n        \n        # Update metrics\n        self._metrics['total_requests'] += 1\n        if method_name not in self._metrics['method_stats']:\n            self._metrics['method_stats'][method_name] = {\n                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n            }\n        self._metrics['method_stats'][method_name]['count'] += 1\n        \n        try:\n            # Log request\n            if self.log_requests:\n                self._log_request(method_name, request, correlation_id, context)\n            \n            # Execute the actual method\n            response = await behavior(request, context)\n            \n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update success metrics\n            self._metrics['successful_requests'] += 1\n            self._metrics['method_stats'][method_name]['success'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log response\n            if self.log_responses:\n                self._log_response(method_name, response, correlation_id, duration, context)\n            \n            # Log performance\n            if self.log_performance:\n                self._log_performance(method_name, duration, correlation_id)\n            \n            return response\n            \n        except Exception as e:\n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update error metrics\n            self._metrics['failed_requests'] += 1\n            self._metrics['method_stats'][method_name]['error'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log error\n            if self.log_errors:\n                self._log_error(method_name, e, correlation_id, duration, context)\n            \n            # Re-raise the exception\n            raise\n    \n    async def _handle_streaming_request(\n        self,\n        behavior: Callable,\n        request_data: Any,\n        context: ServicerContext,\n        method_name: str,\n        request_streaming: bool = False,\n        response_streaming: bool = True\n    ) -> Any:\n        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n        # For Phase 3, we're focusing on request/response patterns\n        # Streaming would be implemented in a future phase\n        correlation_id = str(uuid4())\n        # Store correlation ID in invocation metadata instead of setting attributes\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            # If metadata setting fails, continue without it\n            pass\n        \n        self.logger.info(\n            f\"[{correlation_id}] Streaming method {method_name} called \"\n            f\"(streaming implementation pending)\"\n        )\n        \n        return behavior(request_data, context)\n    \n    def _log_request(\n        self, \n        method_name: str, \n        request: Any, \n        correlation_id: str, \n        context: ServicerContext\n    ):\n        \"\"\"Log incoming request as JSON string.\"\"\"\n        try:\n            # Extract and sanitize request data\n            request_data = self._sanitize_data(self._extract_request_data(request))\n            \n            # Create simple log object\n            log_obj = {\n                'type': 'REQUEST',\n                'method': method_name,\n                'correlation_id': correlation_id,\n                'timestamp': time.time(),\n                'data': request_data\n            }\n            \n            # Log as JSON string\n            self.logger.info(f\"REQUEST: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log request for {method_name}: {e}\")\n    \n    def _log_response(\n        self, \n        method_name: str, \n        response: Any, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log outgoing response as JSON string.\"\"\"\n        try:\n            # Extract and sanitize response data\n            response_data = self._sanitize_data(self._extract_response_data(response))\n            \n            # Create simple log object\n            log_obj = {\n                'type': 'RESPONSE',\n                'method': method_name,\n                'correlation_id': correlation_id,\n                'duration_ms': round(duration * 1000, 2),\n                'timestamp': time.time(),\n                'data': response_data\n            }\n            \n            # Log as JSON string\n            self.logger.info(f\"RESPONSE: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log response for {method_name}: {e}\")\n    \n    def _log_error(\n        self, \n        method_name: str, \n        error: Exception, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log error with comprehensive context.\"\"\"\n        \n        log_data = {\n            'event': 'grpc_error',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'error_type': type(error).__name__,\n            'error_message': str(error),\n            'duration_ms': round(duration * 1000, 2),\n            'timestamp': time.time()\n        }\n        \n        self.logger.error(\n            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n            extra={'structured_data': log_data},\n            exc_info=True\n        )\n    \n    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n        \"\"\"Log performance metrics.\"\"\"\n        \n        if duration > 1.0:  # Log slow requests (>1 second)\n            self.logger.warning(\n                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n            )\n    \n    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract only fields with actual values\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        # Check if field has been set (not default value)\n                        if hasattr(request, 'HasField') and request.HasField(field_name):\n                            value = getattr(request, field_name)\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                nested_data = self._extract_request_data(value)\n                                if nested_data:  # Only add if not empty\n                                    result[field_name] = nested_data\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                if value:  # Only if list is not empty\n                                    result[field_name] = [\n                                        self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                        for item in value\n                                    ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                        else:\n                            # For fields without HasField (primitives), check if not default\n                            value = getattr(request, field_name, None)\n                            if self._is_not_default_value(value, field):\n                                if hasattr(value, 'DESCRIPTOR'):\n                                    nested_data = self._extract_request_data(value)\n                                    if nested_data:\n                                        result[field_name] = nested_data\n                                elif field.label == field.LABEL_REPEATED:\n                                    if value:\n                                        result[field_name] = [\n                                            self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                            for item in value\n                                        ]\n                                else:\n                                    result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        # Only log errors for non-empty fields\n                        if str(e) != \"Message does not have field\":\n                            result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}\n    \n    def _is_not_default_value(self, value: Any, field: Any) -> bool:\n        \"\"\"Check if value is not the default value for the field type.\"\"\"\n        try:\n            if value is None:\n                return False\n            \n            # Check based on field type\n            if field.type == field.TYPE_STRING:\n                return value != \"\"\n            elif field.type in [field.TYPE_INT32, field.TYPE_INT64, field.TYPE_UINT32, field.TYPE_UINT64]:\n                return value != 0\n            elif field.type in [field.TYPE_FLOAT, field.TYPE_DOUBLE]:\n                return value != 0.0\n            elif field.type == field.TYPE_BOOL:\n                return value != False\n            elif field.type == field.TYPE_ENUM:\n                return value != 0\n            else:\n                return True\n        except:\n            return True\n    \n    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a value for logging.\"\"\"\n        try:\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                import base64\n                encoded = base64.b64encode(value).decode()\n                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n            elif hasattr(value, 'isoformat'):\n                return value.isoformat()\n            elif hasattr(value, 'name'):  # Enum value\n                return value.name\n            else:\n                str_value = str(value)\n                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n        except Exception:\n            return \"<non-serializable>\"\n    \n    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message - extract only fields with actual values\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        # Check if field has been set (not default value)\n                        if hasattr(response, 'HasField') and response.HasField(field_name):\n                            value = getattr(response, field_name)\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                nested_data = self._extract_response_data(value)\n                                if nested_data:  # Only add if not empty\n                                    result[field_name] = nested_data\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                if value:  # Only if list is not empty\n                                    result[field_name] = [\n                                        self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                        for item in value\n                                    ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                        else:\n                            # For fields without HasField (primitives), check if not default\n                            value = getattr(response, field_name, None)\n                            if self._is_not_default_value(value, field):\n                                if hasattr(value, 'DESCRIPTOR'):\n                                    nested_data = self._extract_response_data(value)\n                                    if nested_data:\n                                        result[field_name] = nested_data\n                                elif field.label == field.LABEL_REPEATED:\n                                    if value:\n                                        result[field_name] = [\n                                            self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                            for item in value\n                                        ]\n                                else:\n                                    result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        # Only log errors for non-empty fields\n                        if str(e) != \"Message does not have field\":\n                            result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}\n    \n    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove sensitive information from logged data.\"\"\"\n        if not isinstance(data, dict):\n            return data\n        \n        sanitized = {}\n        for key, value in data.items():\n            key_lower = key.lower()\n            \n            # Check if field contains sensitive information\n            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n                sanitized[key] = '***REDACTED***'\n            elif isinstance(value, dict):\n                sanitized[key] = self._sanitize_data(value)\n            elif isinstance(value, (list, tuple)):\n                sanitized[key] = [\n                    self._sanitize_data(item) if isinstance(item, dict) else item\n                    for item in value\n                ]\n            else:\n                sanitized[key] = value\n        \n        return sanitized\n    \n    def _update_timing_metrics(self, method_name: str, duration: float):\n        \"\"\"Update timing metrics for performance tracking.\"\"\"\n        method_stats = self._metrics['method_stats'][method_name]\n        current_avg = method_stats['avg_time']\n        count = method_stats['count']\n        \n        # Update method-specific average\n        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n        \n        # Update overall average\n        total_requests = self._metrics['total_requests']\n        current_overall_avg = self._metrics['avg_response_time']\n        self._metrics['avg_response_time'] = (\n            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n        )\n    \n    def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get current performance metrics.\"\"\"\n        return {\n            'interceptor_name': 'LoggingInterceptor',\n            'metrics': self._metrics.copy(),\n            'timestamp': time.time()\n        }\n    \n    def reset_metrics(self):\n        \"\"\"Reset performance metrics.\"\"\"\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n\n\n# Convenience function for creating configured logging interceptor\ndef create_logging_interceptor(\n    logger_name: str = \"grpc.logging\",\n    log_level: int = logging.INFO,\n    **kwargs\n) -> LoggingInterceptor:\n    \"\"\"Create a pre-configured logging interceptor.\n    \n    Args:\n        logger_name: Name for the logger\n        log_level: Logging level\n        **kwargs: Additional configuration options\n        \n    Returns:\n        Configured LoggingInterceptor instance\n    \"\"\"\n    logger = logging.getLogger(logger_name)\n    return LoggingInterceptor(\n        logger=logger,\n        log_level=log_level,\n        **kwargs\n    )","structuredPatch":[{"oldStart":433,"oldLines":8,"newStart":433,"newLines":11,"lines":["                 return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"","             elif hasattr(value, 'isoformat'):","                 return value.isoformat()","-            elif hasattr(value, 'name'):  # Enum value","-                return value.name","+            elif hasattr(value, 'name') and hasattr(value, 'value'):  # Enum value","+                return value.name if value.name else value.value","+            elif hasattr(value, '__class__') and 'enum' in str(type(value)).lower():","+                # Handle enum values that might not have name attribute","+                return str(value)","             else:","                 str_value = str(value)","                 return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value"]}],"userModified":false,"replaceAll":false}}
{"parentUuid":"359a81b3-5567-4d09-8f36-b503ebcce15b","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01Q779d3AEvzc3LD5VJeZoSv","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Bây giờ tôi đã hoàn tất việc cải thiện logging interceptor. Tóm tắt những gì đã thay đổi:\n\n## Cải thiện chính:\n\n1. **Chỉ log các field có giá trị thực tế**: Thay vì log tất cả field bao gồm default values (0, \"\", false), interceptor giờ chỉ log các field được set với giá trị thực tế.\n\n2. **Sử dụng `HasField()` cho message fields**: Kiểm tra xem field có được set hay không trước khi log.\n\n3. **Kiểm tra default values cho primitive fields**: Dùng function `_is_not_default_value()` để loại bỏ các giá trị mặc định.\n\n4. **Cải thiện xử lý enum values**: Hiển thị tên enum thay vì số.\n\n5. **Tối ưu hóa nested messages**: Chỉ thêm nested object nếu có dữ liệu thực tế.\n\n## Kết quả mong đợi:\n\nKhi bạn gửi request:\n```json\n{\n  \"request_id\": \"ba9c425a-0667-4534-9d73-c2c87e686c3b\",\n  \"timestamp\": \"2025-01-19T10:30:00Z\",\n  \"data\": {\n    \"account_number\": \"acc-123456\",\n    \"symbol\": \"VIC\",\n    \"market_id\": \"HOSE\",\n    \"side\": \"ORDER_SIDE_BUY\",\n    \"order_type\": \"ORDER_TYPE_LO\",\n    \"price\": 85000,\n    \"quantity\": 100,\n    \"channel\": \"API\"\n  }\n}\n```\n\nLog sẽ hiển thị:\n```json\nREQUEST: {\"type\": \"REQUEST\", \"method\": \"/order.OrderService/CreateOrder\", \"correlation_id\": \"uuid\", \"timestamp\": 1234567890, \"data\": {\"request_id\": \"ba9c425a-0667-4534-9d73-c2c87e686c3b\", \"timestamp\": \"2025-01-19T10:30:00Z\", \"data\": {\"account_number\": \"acc-123456\", \"symbol\": \"VIC\", \"market_id\": \"HOSE\", \"side\": \"ORDER_SIDE_BUY\", \"order_type\": \"ORDER_TYPE_LO\", \"price\": 85000, \"quantity\": 100, \"channel\": \"API\"}}}\n```\n\nVà response:\n```json\nRESPONSE: {\"type\": \"RESPONSE\", \"method\": \"/order.OrderService/CreateOrder\", \"correlation_id\": \"uuid\", \"duration_ms\": 123, \"timestamp\": 1234567890, \"data\": {\"success\": false, \"message\": \"MISSING_SYMBOL: Symbol is required\"}}\n```\n\nGiờ logs sẽ chính xác hơn và chỉ hiển thị dữ liệu thực tế thay vì các field trống."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1117,"cache_read_input_tokens":98517,"cache_creation":{"ephemeral_5m_input_tokens":1117,"ephemeral_1h_input_tokens":0},"output_tokens":780,"service_tier":"standard"}},"requestId":"req_011CSJ9ZvSqGipiH2cG7yHnz","type":"assistant","uuid":"aff5f631-18be-40e5-8dc6-815ec5ccc04e","timestamp":"2025-08-20T02:10:08.093Z"}
{"parentUuid":"aff5f631-18be-40e5-8dc6-815ec5ccc04e","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":"2025-08-20 09:10:36,255 - src.presentation.grpc.server_config - INFO - setup_metrics:404 - Metrics collection enabled\n2025-08-20 09:10:36,255 - src.presentation.grpc.server_config - INFO - start_server:417 - gRPC server started on 0.0.0.0:55051\n2025-08-20 09:10:36,255 - src.presentation.grpc.server_config - INFO - _log_server_status:464 - Server Status: {'server_address': '0.0.0.0:55051', 'max_workers': 10, 'features': {'authentication': True, 'rate_limiting': False, 'detailed_logging': True, 'health_checks': True, 'metrics': True, 'reflection': True}, 'services': ['UserService', 'BrokerAccountService', 'MarketDataService', 'OrderService', 'HealthService']}\n2025-08-20 09:10:41,748 - grpc.requests - INFO - _log_request:281 - REQUEST: {\"type\": \"REQUEST\", \"method\": \"/order.OrderService/CreateOrder\", \"correlation_id\": \"245b6253-6092-4ba3-83d8-b734066c8f36\", \"timestamp\": 1755655841.748708, \"data\": {\"request_id\": \"<error: Field order.CreateOrderRequest.request_id does not have presence.>\", \"timestamp\": \"<error: Field order.CreateOrderRequest.timestamp does not have presence.>\", \"data\": {\"account_number\": \"<error: Field order.CreateOrderRequestData.account_number does not have presence.>\", \"symbol\": \"<error: Field order.CreateOrderRequestData.symbol does not have presence.>\", \"market_id\": \"<error: Field order.CreateOrderRequestData.market_id does not have presence.>\", \"side\": \"<error: Field order.CreateOrderRequestData.side does not have presence.>\", \"order_type\": \"<error: Field order.CreateOrderRequestData.order_type does not have presence.>\", \"price\": \"<error: Field order.CreateOrderRequestData.price does not have presence.>\", \"quantity\": \"<error: Field order.CreateOrderRequestData.quantity does not have presence.>\", \"channel\": \"<error: Field order.CreateOrderRequestData.channel does not have presence.>\", \"is_stop_order\": \"<error: Field order.CreateOrderRequestData.is_stop_order does not have presence.>\", \"stop_price\": \"<error: Field order.CreateOrderRequestData.stop_price does not have presence.>\", \"stop_type\": \"<error: Field order.CreateOrderRequestData.stop_type does not have presence.>\", \"stop_step\": \"<error: Field order.CreateOrderRequestData.stop_step does not have presence.>\"}}}\n2025-08-20 09:10:41,749 - src.presentation.grpc.servicers.order_servicer - INFO - _request_context:61 - [23ab7f14-9a49-49db-9471-0c06d9fa48e9] Starting CreateOrder\n2025-08-20 09:10:41,751 - src.presentation.grpc.servicers.order_servicer - WARNING - CreateOrder:320 - [23ab7f14-9a49-49db-9471-0c06d9fa48e9] Validation error in CreateOrder: MISSING_SYMBOL: Symbol is required\n2025-08-20 09:10:41,751 - src.presentation.grpc.servicers.order_servicer - INFO - _request_context:73 - [23ab7f14-9a49-49db-9471-0c06d9fa48e9] Completed CreateOrder in 0.002s\n2025-08-20 09:10:41,752 - grpc.requests - INFO - _log_response:310 - RESPONSE: {\"type\": \"RESPONSE\", \"method\": \"/order.OrderService/CreateOrder\", \"correlation_id\": \"245b6253-6092-4ba3-83d8-b734066c8f36\", \"duration_ms\": 2.9, \"timestamp\": 1755655841.751966, \"data\": {\"success\": \"<error: Field order.CreateOrderResponse.success does not have presence.>\", \"message\": \"<error: Field order.CreateOrderResponse.message does not have presence.>\", \"order\": {\"id\": \"<error: Field order.Order.id does not have presence.>\", \"account_id\": \"<error: Field order.Order.account_id does not have presence.>\", \"broker_order_id\": \"<error: Field order.Order.broker_order_id does not have presence.>\", \"request_id\": \"<error: Field order.Order.request_id does not have presence.>\", \"symbol\": \"<error: Field order.Order.symbol does not have presence.>\", \"market_id\": \"<error: Field order.Order.market_id does not have presence.>\", \"side\": \"<error: Field order.Order.side does not have presence.>\", \"order_type\": \"<error: Field order.Order.order_type does not have presence.>\", \"price\": \"<error: Field order.Order.price does not have presence.>\", \"quantity\": \"<error: Field order.Order.quantity does not have presence.>\", \"filled_quantity\": \"<error: Field order.Order.filled_quantity does not have presence.>\", \"cancelled_quantity\": \"<error: Field order.Order.cancelled_quantity does not have presence.>\", \"remaining_quantity\": \"<error: Field order.Order.remaining_quantity does not have presence.>\", \"average_price\": \"<error: Field order.Order.average_price does not have presence.>\", \"status\": \"<error: Field order.Order.status does not have presence.>\", \"broker_status\": \"<error: Field order.Order.broker_status does not have presence.>\", \"channel\": \"<error: Field order.Order.channel does not have presence.>\", \"is_stop_order\": \"<error: Field order.Order.is_stop_order does not have presence.>\", \"stop_price\": \"<error: Field order.Order.stop_price does not have presence.>\", \"stop_type\": \"<error: Field order.Order.stop_type does not have presence.>\", \"stop_step\": \"<error: Field order.Order.stop_step does not have presence.>\", \"created_at\": {\"seconds\": \"<error: Field google.protobuf.Timestamp.seconds does not have presence.>\", \"nanos\": \"<error: Field google.protobuf.Timestamp.nanos does not have presence.>\"}, \"sent_at\": {\"seconds\": \"<error: Field google.protobuf.Timestamp.seconds does not have presence.>\", \"nanos\": \"<error: Field google.protobuf.Timestamp.nanos does not have presence.>\"}, \"filled_at\": {\"seconds\": \"<error: Field google.protobuf.Timestamp.seconds does not have presence.>\", \"nanos\": \"<error: Field google.protobuf.Timestamp.nanos does not have presence.>\"}, \"cancelled_at\": {\"seconds\": \"<error: Field google.protobuf.Timestamp.seconds does not have presence.>\", \"nanos\": \"<error: Field google.protobuf.Timestamp.nanos does not have presence.>\"}, \"updated_at\": {\"seconds\": \"<error: Field google.protobuf.Timestamp.seconds does not have presence.>\", \"nanos\": \"<error: Field google.protobuf.Timestamp.nanos does not have presence.>\"}, \"reject_reason\": \"<error: Field order.Order.reject_reason does not have presence.>\"}}}\n\nChuong trinh van dang bị lôi khong nhan duoc cac thong tin trong reques. Hay giup toi kiem tra lai"},"uuid":"bf228d0a-54c4-44bb-b08d-6588f08a1da7","timestamp":"2025-08-20T02:12:09.434Z"}
{"parentUuid":"bf228d0a-54c4-44bb-b08d-6588f08a1da7","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_019gqPqdyxSMSb35jeHRFBKp","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Tôi thấy vấn đề rồi. Protobuf version 3 không có `HasField()` cho các scalar fields (string, int, bool, etc.) mà chỉ có cho message fields. Tôi cần sửa lại logic để handle đúng cách:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":2724,"cache_read_input_tokens":99634,"cache_creation":{"ephemeral_5m_input_tokens":2724,"ephemeral_1h_input_tokens":0},"output_tokens":2,"service_tier":"standard"}},"requestId":"req_011CSJ9k1aBcqaCu9L8ZfBSH","type":"assistant","uuid":"4b4f1194-5ef5-412d-8658-83bc2e8a0802","timestamp":"2025-08-20T02:12:16.496Z"}
{"parentUuid":"4b4f1194-5ef5-412d-8658-83bc2e8a0802","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_019gqPqdyxSMSb35jeHRFBKp","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01C4cnqhK1vKv4ShmUgQGgqW","name":"Edit","input":{"file_path":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py","old_string":"    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract only fields with actual values\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        # Check if field has been set (not default value)\n                        if hasattr(request, 'HasField') and request.HasField(field_name):\n                            value = getattr(request, field_name)\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                nested_data = self._extract_request_data(value)\n                                if nested_data:  # Only add if not empty\n                                    result[field_name] = nested_data\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                if value:  # Only if list is not empty\n                                    result[field_name] = [\n                                        self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                        for item in value\n                                    ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                        else:\n                            # For fields without HasField (primitives), check if not default\n                            value = getattr(request, field_name, None)\n                            if self._is_not_default_value(value, field):\n                                if hasattr(value, 'DESCRIPTOR'):\n                                    nested_data = self._extract_request_data(value)\n                                    if nested_data:\n                                        result[field_name] = nested_data\n                                elif field.label == field.LABEL_REPEATED:\n                                    if value:\n                                        result[field_name] = [\n                                            self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                            for item in value\n                                        ]\n                                else:\n                                    result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        # Only log errors for non-empty fields\n                        if str(e) != \"Message does not have field\":\n                            result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}","new_string":"    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract only fields with actual values\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(request, field_name, None)\n                        \n                        # For message fields, use HasField if available\n                        if field.type == field.TYPE_MESSAGE:\n                            try:\n                                if hasattr(request, 'HasField') and request.HasField(field_name):\n                                    if hasattr(value, 'DESCRIPTOR'):\n                                        nested_data = self._extract_request_data(value)\n                                        if nested_data:  # Only add if not empty\n                                            result[field_name] = nested_data\n                                    else:\n                                        result[field_name] = self._serialize_value(value)\n                            except ValueError:\n                                # Field might not support HasField, try to extract anyway\n                                if hasattr(value, 'DESCRIPTOR'):\n                                    nested_data = self._extract_request_data(value)\n                                    if nested_data:\n                                        result[field_name] = nested_data\n                                elif value is not None:\n                                    result[field_name] = self._serialize_value(value)\n                        \n                        # For repeated fields\n                        elif field.label == field.LABEL_REPEATED:\n                            if value:  # Only if list is not empty\n                                result[field_name] = [\n                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                        \n                        # For scalar fields, check if not default value\n                        else:\n                            if self._is_not_default_value(value, field):\n                                result[field_name] = self._serialize_value(value)\n                                \n                    except Exception as e:\n                        # Skip logging errors for presence checks\n                        if \"does not have presence\" not in str(e):\n                            result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":2724,"cache_read_input_tokens":99634,"cache_creation":{"ephemeral_5m_input_tokens":2724,"ephemeral_1h_input_tokens":0},"output_tokens":1543,"service_tier":"standard"}},"requestId":"req_011CSJ9k1aBcqaCu9L8ZfBSH","type":"assistant","uuid":"ffd13a22-549a-4c9d-817d-8c396b2e355a","timestamp":"2025-08-20T02:12:29.563Z"}
{"parentUuid":"ffd13a22-549a-4c9d-817d-8c396b2e355a","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01C4cnqhK1vKv4ShmUgQGgqW","type":"tool_result","content":"The file /Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   345→            self.logger.warning(\n   346→                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n   347→            )\n   348→    \n   349→    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n   350→        \"\"\"Extract relevant data from request for logging.\"\"\"\n   351→        try:\n   352→            if hasattr(request, 'DESCRIPTOR'):\n   353→                # Protobuf message - extract only fields with actual values\n   354→                result = {}\n   355→                for field in request.DESCRIPTOR.fields:\n   356→                    field_name = field.name\n   357→                    try:\n   358→                        value = getattr(request, field_name, None)\n   359→                        \n   360→                        # For message fields, use HasField if available\n   361→                        if field.type == field.TYPE_MESSAGE:\n   362→                            try:\n   363→                                if hasattr(request, 'HasField') and request.HasField(field_name):\n   364→                                    if hasattr(value, 'DESCRIPTOR'):\n   365→                                        nested_data = self._extract_request_data(value)\n   366→                                        if nested_data:  # Only add if not empty\n   367→                                            result[field_name] = nested_data\n   368→                                    else:\n   369→                                        result[field_name] = self._serialize_value(value)\n   370→                            except ValueError:\n   371→                                # Field might not support HasField, try to extract anyway\n   372→                                if hasattr(value, 'DESCRIPTOR'):\n   373→                                    nested_data = self._extract_request_data(value)\n   374→                                    if nested_data:\n   375→                                        result[field_name] = nested_data\n   376→                                elif value is not None:\n   377→                                    result[field_name] = self._serialize_value(value)\n   378→                        \n   379→                        # For repeated fields\n   380→                        elif field.label == field.LABEL_REPEATED:\n   381→                            if value:  # Only if list is not empty\n   382→                                result[field_name] = [\n   383→                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n   384→                                    for item in value\n   385→                                ]\n   386→                        \n   387→                        # For scalar fields, check if not default value\n   388→                        else:\n   389→                            if self._is_not_default_value(value, field):\n   390→                                result[field_name] = self._serialize_value(value)\n   391→                                \n   392→                    except Exception as e:\n   393→                        # Skip logging errors for presence checks\n   394→                        if \"does not have presence\" not in str(e):\n   395→                            result[field_name] = f\"<error: {str(e)}>\"\n   396→                return result\n   397→            elif hasattr(request, '__dict__'):\n   398→                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n   399→            else:\n   400→                return {'value': str(request)}\n   401→        except Exception:\n   402→            return {'error': 'Failed to extract request data'}\n   403→    \n   404→    def _is_not_default_value(self, value: Any, field: Any) -> bool:\n   405→        \"\"\"Check if value is not the default value for the field type.\"\"\"\n   406→        try:"}]},"uuid":"04bd6409-abdf-4833-ba33-19c6d9be8eb8","timestamp":"2025-08-20T02:12:38.767Z","toolUseResult":{"filePath":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py","oldString":"    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract only fields with actual values\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        # Check if field has been set (not default value)\n                        if hasattr(request, 'HasField') and request.HasField(field_name):\n                            value = getattr(request, field_name)\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                nested_data = self._extract_request_data(value)\n                                if nested_data:  # Only add if not empty\n                                    result[field_name] = nested_data\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                if value:  # Only if list is not empty\n                                    result[field_name] = [\n                                        self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                        for item in value\n                                    ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                        else:\n                            # For fields without HasField (primitives), check if not default\n                            value = getattr(request, field_name, None)\n                            if self._is_not_default_value(value, field):\n                                if hasattr(value, 'DESCRIPTOR'):\n                                    nested_data = self._extract_request_data(value)\n                                    if nested_data:\n                                        result[field_name] = nested_data\n                                elif field.label == field.LABEL_REPEATED:\n                                    if value:\n                                        result[field_name] = [\n                                            self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                            for item in value\n                                        ]\n                                else:\n                                    result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        # Only log errors for non-empty fields\n                        if str(e) != \"Message does not have field\":\n                            result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}","newString":"    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract only fields with actual values\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(request, field_name, None)\n                        \n                        # For message fields, use HasField if available\n                        if field.type == field.TYPE_MESSAGE:\n                            try:\n                                if hasattr(request, 'HasField') and request.HasField(field_name):\n                                    if hasattr(value, 'DESCRIPTOR'):\n                                        nested_data = self._extract_request_data(value)\n                                        if nested_data:  # Only add if not empty\n                                            result[field_name] = nested_data\n                                    else:\n                                        result[field_name] = self._serialize_value(value)\n                            except ValueError:\n                                # Field might not support HasField, try to extract anyway\n                                if hasattr(value, 'DESCRIPTOR'):\n                                    nested_data = self._extract_request_data(value)\n                                    if nested_data:\n                                        result[field_name] = nested_data\n                                elif value is not None:\n                                    result[field_name] = self._serialize_value(value)\n                        \n                        # For repeated fields\n                        elif field.label == field.LABEL_REPEATED:\n                            if value:  # Only if list is not empty\n                                result[field_name] = [\n                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                        \n                        # For scalar fields, check if not default value\n                        else:\n                            if self._is_not_default_value(value, field):\n                                result[field_name] = self._serialize_value(value)\n                                \n                    except Exception as e:\n                        # Skip logging errors for presence checks\n                        if \"does not have presence\" not in str(e):\n                            result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}","originalFile":"\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n\nThis interceptor provides:\n- Detailed request/response logging\n- Performance metrics tracking\n- Error logging and analysis\n- Request correlation and tracing\n- Structured logging with context\n\"\"\"\nimport json\nimport logging\nimport time\nfrom typing import Any, Callable, Iterator, Optional, Dict\nfrom uuid import uuid4\n\nimport grpc\nimport grpc.aio\nfrom grpc.aio import ServerInterceptor, ServicerContext\n\n\nclass LoggingInterceptor(grpc.aio.ServerInterceptor):\n    \"\"\"Advanced logging interceptor for gRPC services.\n    \n    Features:\n    - Request/response logging with structured data\n    - Performance metrics and timing\n    - Error tracking and analysis\n    - Request correlation IDs\n    - Configurable log levels and filters\n    - Security-aware logging (excludes sensitive data)\n    \"\"\"\n    \n    def __init__(\n        self,\n        logger: Optional[logging.Logger] = None,\n        log_level: int = logging.INFO,\n        log_requests: bool = True,\n        log_responses: bool = True,\n        log_errors: bool = True,\n        log_performance: bool = True,\n        exclude_methods: Optional[list] = None,\n        sensitive_fields: Optional[list] = None\n    ):\n        self.logger = logger or logging.getLogger(__name__)\n        self.log_level = log_level\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_errors = log_errors\n        self.log_performance = log_performance\n        self.exclude_methods = exclude_methods or []\n        self.sensitive_fields = sensitive_fields or [\n            'password', 'token', 'secret', 'key', 'credential', 'auth'\n        ]\n        \n        # Performance metrics\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n    \n    async def intercept_service(\n        self,\n        continuation: Callable[[Any], Any],\n        handler_call_details: Any,\n    ) -> Any:\n        \"\"\"Intercept service calls for logging.\"\"\"\n        \n        # Skip excluded methods\n        method_name = handler_call_details.method\n        if method_name in self.exclude_methods:\n            return await continuation(handler_call_details)\n        \n        # Get the original handler\n        handler = await continuation(handler_call_details)\n        if handler is None:\n            return None\n        \n        # Wrap the handler based on its type; bypass reflection and health entirely\n        if method_name in {\n            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n            '/grpc.health.v1.Health/Check',\n            '/grpc.health.v1.Health/Watch',\n        }:\n            return handler\n        if handler.unary_unary:\n            return grpc.unary_unary_rpc_method_handler(\n                self._wrap_unary_unary(handler.unary_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.unary_stream:\n            return grpc.unary_stream_rpc_method_handler(\n                self._wrap_unary_stream(handler.unary_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_unary:\n            return grpc.stream_unary_rpc_method_handler(\n                self._wrap_stream_unary(handler.stream_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_stream:\n            return grpc.stream_stream_rpc_method_handler(\n                self._wrap_stream_stream(handler.stream_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        else:\n            return handler\n    \n    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-unary RPC method.\"\"\"\n        \n        async def wrapper(request: Any, context: ServicerContext) -> Any:\n            return await self._handle_request(\n                behavior, request, context, method_name, is_streaming=False\n            )\n        \n        return wrapper\n    \n    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n        async def wrapper(request: Any, context: ServicerContext):\n            # For streaming responses, return the iterator directly (do not await)\n            try:\n                return behavior(request, context)\n            finally:\n                # Minimal logging to avoid interfering with stream lifecycle\n                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-unary RPC method.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n            # Request is streaming, response is unary; await the final response\n            try:\n                return await behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext):\n            # For bi-di streaming, return the response iterator directly\n            try:\n                return behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n        return wrapper\n    \n    async def _handle_request(\n        self, \n        behavior: Callable, \n        request: Any, \n        context: ServicerContext, \n        method_name: str,\n        is_streaming: bool = False\n    ) -> Any:\n        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n        \n        # Generate correlation ID\n        correlation_id = str(uuid4())\n        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            pass\n        \n        start_time = time.time()\n        \n        # Update metrics\n        self._metrics['total_requests'] += 1\n        if method_name not in self._metrics['method_stats']:\n            self._metrics['method_stats'][method_name] = {\n                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n            }\n        self._metrics['method_stats'][method_name]['count'] += 1\n        \n        try:\n            # Log request\n            if self.log_requests:\n                self._log_request(method_name, request, correlation_id, context)\n            \n            # Execute the actual method\n            response = await behavior(request, context)\n            \n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update success metrics\n            self._metrics['successful_requests'] += 1\n            self._metrics['method_stats'][method_name]['success'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log response\n            if self.log_responses:\n                self._log_response(method_name, response, correlation_id, duration, context)\n            \n            # Log performance\n            if self.log_performance:\n                self._log_performance(method_name, duration, correlation_id)\n            \n            return response\n            \n        except Exception as e:\n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update error metrics\n            self._metrics['failed_requests'] += 1\n            self._metrics['method_stats'][method_name]['error'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log error\n            if self.log_errors:\n                self._log_error(method_name, e, correlation_id, duration, context)\n            \n            # Re-raise the exception\n            raise\n    \n    async def _handle_streaming_request(\n        self,\n        behavior: Callable,\n        request_data: Any,\n        context: ServicerContext,\n        method_name: str,\n        request_streaming: bool = False,\n        response_streaming: bool = True\n    ) -> Any:\n        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n        # For Phase 3, we're focusing on request/response patterns\n        # Streaming would be implemented in a future phase\n        correlation_id = str(uuid4())\n        # Store correlation ID in invocation metadata instead of setting attributes\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            # If metadata setting fails, continue without it\n            pass\n        \n        self.logger.info(\n            f\"[{correlation_id}] Streaming method {method_name} called \"\n            f\"(streaming implementation pending)\"\n        )\n        \n        return behavior(request_data, context)\n    \n    def _log_request(\n        self, \n        method_name: str, \n        request: Any, \n        correlation_id: str, \n        context: ServicerContext\n    ):\n        \"\"\"Log incoming request as JSON string.\"\"\"\n        try:\n            # Extract and sanitize request data\n            request_data = self._sanitize_data(self._extract_request_data(request))\n            \n            # Create simple log object\n            log_obj = {\n                'type': 'REQUEST',\n                'method': method_name,\n                'correlation_id': correlation_id,\n                'timestamp': time.time(),\n                'data': request_data\n            }\n            \n            # Log as JSON string\n            self.logger.info(f\"REQUEST: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log request for {method_name}: {e}\")\n    \n    def _log_response(\n        self, \n        method_name: str, \n        response: Any, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log outgoing response as JSON string.\"\"\"\n        try:\n            # Extract and sanitize response data\n            response_data = self._sanitize_data(self._extract_response_data(response))\n            \n            # Create simple log object\n            log_obj = {\n                'type': 'RESPONSE',\n                'method': method_name,\n                'correlation_id': correlation_id,\n                'duration_ms': round(duration * 1000, 2),\n                'timestamp': time.time(),\n                'data': response_data\n            }\n            \n            # Log as JSON string\n            self.logger.info(f\"RESPONSE: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log response for {method_name}: {e}\")\n    \n    def _log_error(\n        self, \n        method_name: str, \n        error: Exception, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log error with comprehensive context.\"\"\"\n        \n        log_data = {\n            'event': 'grpc_error',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'error_type': type(error).__name__,\n            'error_message': str(error),\n            'duration_ms': round(duration * 1000, 2),\n            'timestamp': time.time()\n        }\n        \n        self.logger.error(\n            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n            extra={'structured_data': log_data},\n            exc_info=True\n        )\n    \n    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n        \"\"\"Log performance metrics.\"\"\"\n        \n        if duration > 1.0:  # Log slow requests (>1 second)\n            self.logger.warning(\n                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n            )\n    \n    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract only fields with actual values\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        # Check if field has been set (not default value)\n                        if hasattr(request, 'HasField') and request.HasField(field_name):\n                            value = getattr(request, field_name)\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                nested_data = self._extract_request_data(value)\n                                if nested_data:  # Only add if not empty\n                                    result[field_name] = nested_data\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                if value:  # Only if list is not empty\n                                    result[field_name] = [\n                                        self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                        for item in value\n                                    ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                        else:\n                            # For fields without HasField (primitives), check if not default\n                            value = getattr(request, field_name, None)\n                            if self._is_not_default_value(value, field):\n                                if hasattr(value, 'DESCRIPTOR'):\n                                    nested_data = self._extract_request_data(value)\n                                    if nested_data:\n                                        result[field_name] = nested_data\n                                elif field.label == field.LABEL_REPEATED:\n                                    if value:\n                                        result[field_name] = [\n                                            self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                            for item in value\n                                        ]\n                                else:\n                                    result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        # Only log errors for non-empty fields\n                        if str(e) != \"Message does not have field\":\n                            result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}\n    \n    def _is_not_default_value(self, value: Any, field: Any) -> bool:\n        \"\"\"Check if value is not the default value for the field type.\"\"\"\n        try:\n            if value is None:\n                return False\n            \n            # Check based on field type\n            if field.type == field.TYPE_STRING:\n                return value != \"\"\n            elif field.type in [field.TYPE_INT32, field.TYPE_INT64, field.TYPE_UINT32, field.TYPE_UINT64]:\n                return value != 0\n            elif field.type in [field.TYPE_FLOAT, field.TYPE_DOUBLE]:\n                return value != 0.0\n            elif field.type == field.TYPE_BOOL:\n                return value != False\n            elif field.type == field.TYPE_ENUM:\n                return value != 0\n            else:\n                return True\n        except:\n            return True\n    \n    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a value for logging.\"\"\"\n        try:\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                import base64\n                encoded = base64.b64encode(value).decode()\n                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n            elif hasattr(value, 'isoformat'):\n                return value.isoformat()\n            elif hasattr(value, 'name') and hasattr(value, 'value'):  # Enum value\n                return value.name if value.name else value.value\n            elif hasattr(value, '__class__') and 'enum' in str(type(value)).lower():\n                # Handle enum values that might not have name attribute\n                return str(value)\n            else:\n                str_value = str(value)\n                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n        except Exception:\n            return \"<non-serializable>\"\n    \n    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message - extract only fields with actual values\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        # Check if field has been set (not default value)\n                        if hasattr(response, 'HasField') and response.HasField(field_name):\n                            value = getattr(response, field_name)\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                nested_data = self._extract_response_data(value)\n                                if nested_data:  # Only add if not empty\n                                    result[field_name] = nested_data\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                if value:  # Only if list is not empty\n                                    result[field_name] = [\n                                        self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                        for item in value\n                                    ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                        else:\n                            # For fields without HasField (primitives), check if not default\n                            value = getattr(response, field_name, None)\n                            if self._is_not_default_value(value, field):\n                                if hasattr(value, 'DESCRIPTOR'):\n                                    nested_data = self._extract_response_data(value)\n                                    if nested_data:\n                                        result[field_name] = nested_data\n                                elif field.label == field.LABEL_REPEATED:\n                                    if value:\n                                        result[field_name] = [\n                                            self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                            for item in value\n                                        ]\n                                else:\n                                    result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        # Only log errors for non-empty fields\n                        if str(e) != \"Message does not have field\":\n                            result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}\n    \n    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove sensitive information from logged data.\"\"\"\n        if not isinstance(data, dict):\n            return data\n        \n        sanitized = {}\n        for key, value in data.items():\n            key_lower = key.lower()\n            \n            # Check if field contains sensitive information\n            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n                sanitized[key] = '***REDACTED***'\n            elif isinstance(value, dict):\n                sanitized[key] = self._sanitize_data(value)\n            elif isinstance(value, (list, tuple)):\n                sanitized[key] = [\n                    self._sanitize_data(item) if isinstance(item, dict) else item\n                    for item in value\n                ]\n            else:\n                sanitized[key] = value\n        \n        return sanitized\n    \n    def _update_timing_metrics(self, method_name: str, duration: float):\n        \"\"\"Update timing metrics for performance tracking.\"\"\"\n        method_stats = self._metrics['method_stats'][method_name]\n        current_avg = method_stats['avg_time']\n        count = method_stats['count']\n        \n        # Update method-specific average\n        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n        \n        # Update overall average\n        total_requests = self._metrics['total_requests']\n        current_overall_avg = self._metrics['avg_response_time']\n        self._metrics['avg_response_time'] = (\n            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n        )\n    \n    def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get current performance metrics.\"\"\"\n        return {\n            'interceptor_name': 'LoggingInterceptor',\n            'metrics': self._metrics.copy(),\n            'timestamp': time.time()\n        }\n    \n    def reset_metrics(self):\n        \"\"\"Reset performance metrics.\"\"\"\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n\n\n# Convenience function for creating configured logging interceptor\ndef create_logging_interceptor(\n    logger_name: str = \"grpc.logging\",\n    log_level: int = logging.INFO,\n    **kwargs\n) -> LoggingInterceptor:\n    \"\"\"Create a pre-configured logging interceptor.\n    \n    Args:\n        logger_name: Name for the logger\n        log_level: Logging level\n        **kwargs: Additional configuration options\n        \n    Returns:\n        Configured LoggingInterceptor instance\n    \"\"\"\n    logger = logging.getLogger(logger_name)\n    return LoggingInterceptor(\n        logger=logger,\n        log_level=log_level,\n        **kwargs\n    )","structuredPatch":[{"oldStart":355,"oldLines":42,"newStart":355,"newLines":43,"lines":["                 for field in request.DESCRIPTOR.fields:","                     field_name = field.name","                     try:","-                        # Check if field has been set (not default value)","-                        if hasattr(request, 'HasField') and request.HasField(field_name):","-                            value = getattr(request, field_name)","-                            if hasattr(value, 'DESCRIPTOR'):","-                                # Nested protobuf message","-                                nested_data = self._extract_request_data(value)","-                                if nested_data:  # Only add if not empty","-                                    result[field_name] = nested_data","-                            elif field.label == field.LABEL_REPEATED:","-                                # Repeated field (list)","-                                if value:  # Only if list is not empty","-                                    result[field_name] = [","-                                        self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)","-                                        for item in value","-                                    ]","-                            else:","-                                result[field_name] = self._serialize_value(value)","-                        else:","-                            # For fields without HasField (primitives), check if not default","-                            value = getattr(request, field_name, None)","-                            if self._is_not_default_value(value, field):","+                        value = getattr(request, field_name, None)","+                        ","+                        # For message fields, use HasField if available","+                        if field.type == field.TYPE_MESSAGE:","+                            try:","+                                if hasattr(request, 'HasField') and request.HasField(field_name):","+                                    if hasattr(value, 'DESCRIPTOR'):","+                                        nested_data = self._extract_request_data(value)","+                                        if nested_data:  # Only add if not empty","+                                            result[field_name] = nested_data","+                                    else:","+                                        result[field_name] = self._serialize_value(value)","+                            except ValueError:","+                                # Field might not support HasField, try to extract anyway","                                 if hasattr(value, 'DESCRIPTOR'):","                                     nested_data = self._extract_request_data(value)","                                     if nested_data:","                                         result[field_name] = nested_data","-                                elif field.label == field.LABEL_REPEATED:","-                                    if value:","-                                        result[field_name] = [","-                                            self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)","-                                            for item in value","-                                        ]","-                                else:","+                                elif value is not None:","                                     result[field_name] = self._serialize_value(value)","+                        ","+                        # For repeated fields","+                        elif field.label == field.LABEL_REPEATED:","+                            if value:  # Only if list is not empty","+                                result[field_name] = [","+                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)","+                                    for item in value","+                                ]","+                        ","+                        # For scalar fields, check if not default value","+                        else:","+                            if self._is_not_default_value(value, field):","+                                result[field_name] = self._serialize_value(value)","+                                ","                     except Exception as e:","-                        # Only log errors for non-empty fields","-                        if str(e) != \"Message does not have field\":","+                        # Skip logging errors for presence checks","+                        if \"does not have presence\" not in str(e):","                             result[field_name] = f\"<error: {str(e)}>\"","                 return result","             elif hasattr(request, '__dict__'):"]}],"userModified":false,"replaceAll":false}}
{"parentUuid":"04bd6409-abdf-4833-ba33-19c6d9be8eb8","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01FeZ1CUYr2TQa7i3jCumALS","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Bây giờ tôi cần cập nhật `_extract_response_data` với logic tương tự:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":2545,"cache_read_input_tokens":102358,"cache_creation":{"ephemeral_5m_input_tokens":2545,"ephemeral_1h_input_tokens":0},"output_tokens":7,"service_tier":"standard"}},"requestId":"req_011CSJ9nCKeP5FHFDjYLG9yY","type":"assistant","uuid":"d1146156-7e51-40e6-9167-453707feda54","timestamp":"2025-08-20T02:12:45.383Z"}
{"parentUuid":"d1146156-7e51-40e6-9167-453707feda54","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01FeZ1CUYr2TQa7i3jCumALS","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01EmxEL4cfsejvDozeifLq9R","name":"Edit","input":{"file_path":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py","old_string":"    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message - extract only fields with actual values\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        # Check if field has been set (not default value)\n                        if hasattr(response, 'HasField') and response.HasField(field_name):\n                            value = getattr(response, field_name)\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                nested_data = self._extract_response_data(value)\n                                if nested_data:  # Only add if not empty\n                                    result[field_name] = nested_data\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                if value:  # Only if list is not empty\n                                    result[field_name] = [\n                                        self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                        for item in value\n                                    ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                        else:\n                            # For fields without HasField (primitives), check if not default\n                            value = getattr(response, field_name, None)\n                            if self._is_not_default_value(value, field):\n                                if hasattr(value, 'DESCRIPTOR'):\n                                    nested_data = self._extract_response_data(value)\n                                    if nested_data:\n                                        result[field_name] = nested_data\n                                elif field.label == field.LABEL_REPEATED:\n                                    if value:\n                                        result[field_name] = [\n                                            self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                            for item in value\n                                        ]\n                                else:\n                                    result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        # Only log errors for non-empty fields\n                        if str(e) != \"Message does not have field\":\n                            result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}","new_string":"    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message - extract only fields with actual values\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(response, field_name, None)\n                        \n                        # For message fields, use HasField if available\n                        if field.type == field.TYPE_MESSAGE:\n                            try:\n                                if hasattr(response, 'HasField') and response.HasField(field_name):\n                                    if hasattr(value, 'DESCRIPTOR'):\n                                        nested_data = self._extract_response_data(value)\n                                        if nested_data:  # Only add if not empty\n                                            result[field_name] = nested_data\n                                    else:\n                                        result[field_name] = self._serialize_value(value)\n                            except ValueError:\n                                # Field might not support HasField, try to extract anyway\n                                if hasattr(value, 'DESCRIPTOR'):\n                                    nested_data = self._extract_response_data(value)\n                                    if nested_data:\n                                        result[field_name] = nested_data\n                                elif value is not None:\n                                    result[field_name] = self._serialize_value(value)\n                        \n                        # For repeated fields\n                        elif field.label == field.LABEL_REPEATED:\n                            if value:  # Only if list is not empty\n                                result[field_name] = [\n                                    self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                        \n                        # For scalar fields, check if not default value\n                        else:\n                            if self._is_not_default_value(value, field):\n                                result[field_name] = self._serialize_value(value)\n                                \n                    except Exception as e:\n                        # Skip logging errors for presence checks\n                        if \"does not have presence\" not in str(e):\n                            result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":2545,"cache_read_input_tokens":102358,"cache_creation":{"ephemeral_5m_input_tokens":2545,"ephemeral_1h_input_tokens":0},"output_tokens":1498,"service_tier":"standard"}},"requestId":"req_011CSJ9nCKeP5FHFDjYLG9yY","type":"assistant","uuid":"816a9621-b00a-42cc-a0a9-f89f924c8bc4","timestamp":"2025-08-20T02:12:55.714Z"}
{"parentUuid":"816a9621-b00a-42cc-a0a9-f89f924c8bc4","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01EmxEL4cfsejvDozeifLq9R","type":"tool_result","content":"The file /Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n     1→\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n     2→\n     3→This interceptor provides:\n     4→- Detailed request/response logging\n     5→- Performance metrics tracking\n     6→- Error logging and analysis\n     7→- Request correlation and tracing\n     8→- Structured logging with context\n     9→\"\"\"\n    10→import json\n    11→import logging\n    12→import time\n    13→from typing import Any, Callable, Iterator, Optional, Dict\n    14→from uuid import uuid4\n    15→\n    16→import grpc\n    17→import grpc.aio\n    18→from grpc.aio import ServerInterceptor, ServicerContext\n    19→\n    20→\n    21→class LoggingInterceptor(grpc.aio.ServerInterceptor):\n    22→    \"\"\"Advanced logging interceptor for gRPC services.\n    23→    \n    24→    Features:\n    25→    - Request/response logging with structured data\n    26→    - Performance metrics and timing\n    27→    - Error tracking and analysis\n    28→    - Request correlation IDs\n    29→    - Configurable log levels and filters\n    30→    - Security-aware logging (excludes sensitive data)\n    31→    \"\"\"\n    32→    \n    33→    def __init__(\n    34→        self,\n    35→        logger: Optional[logging.Logger] = None,\n    36→        log_level: int = logging.INFO,\n    37→        log_requests: bool = True,\n    38→        log_responses: bool = True,\n    39→        log_errors: bool = True,\n    40→        log_performance: bool = True,\n    41→        exclude_methods: Optional[list] = None,\n    42→        sensitive_fields: Optional[list] = None\n    43→    ):\n    44→        self.logger = logger or logging.getLogger(__name__)\n    45→        self.log_level = log_level\n    46→        self.log_requests = log_requests\n    47→        self.log_responses = log_responses\n    48→        self.log_errors = log_errors\n    49→        self.log_performance = log_performance\n    50→        self.exclude_methods = exclude_methods or []\n    51→        self.sensitive_fields = sensitive_fields or [\n    52→            'password', 'token', 'secret', 'key', 'credential', 'auth'\n    53→        ]\n    54→        \n    55→        # Performance metrics\n    56→        self._metrics = {\n    57→            'total_requests': 0,\n    58→            'successful_requests': 0,\n    59→            'failed_requests': 0,\n    60→            'avg_response_time': 0.0,\n    61→            'method_stats': {}\n    62→        }\n    63→    \n    64→    async def intercept_service(\n    65→        self,\n    66→        continuation: Callable[[Any], Any],\n    67→        handler_call_details: Any,\n    68→    ) -> Any:\n    69→        \"\"\"Intercept service calls for logging.\"\"\"\n    70→        \n    71→        # Skip excluded methods\n    72→        method_name = handler_call_details.method\n    73→        if method_name in self.exclude_methods:\n    74→            return await continuation(handler_call_details)\n    75→        \n    76→        # Get the original handler\n    77→        handler = await continuation(handler_call_details)\n    78→        if handler is None:\n    79→            return None\n    80→        \n    81→        # Wrap the handler based on its type; bypass reflection and health entirely\n    82→        if method_name in {\n    83→            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n    84→            '/grpc.health.v1.Health/Check',\n    85→            '/grpc.health.v1.Health/Watch',\n    86→        }:\n    87→            return handler\n    88→        if handler.unary_unary:\n    89→            return grpc.unary_unary_rpc_method_handler(\n    90→                self._wrap_unary_unary(handler.unary_unary, method_name),\n    91→                request_deserializer=handler.request_deserializer,\n    92→                response_serializer=handler.response_serializer,\n    93→            )\n    94→        elif handler.unary_stream:\n    95→            return grpc.unary_stream_rpc_method_handler(\n    96→                self._wrap_unary_stream(handler.unary_stream, method_name),\n    97→                request_deserializer=handler.request_deserializer,\n    98→                response_serializer=handler.response_serializer,\n    99→            )\n   100→        elif handler.stream_unary:\n   101→            return grpc.stream_unary_rpc_method_handler(\n   102→                self._wrap_stream_unary(handler.stream_unary, method_name),\n   103→                request_deserializer=handler.request_deserializer,\n   104→                response_serializer=handler.response_serializer,\n   105→            )\n   106→        elif handler.stream_stream:\n   107→            return grpc.stream_stream_rpc_method_handler(\n   108→                self._wrap_stream_stream(handler.stream_stream, method_name),\n   109→                request_deserializer=handler.request_deserializer,\n   110→                response_serializer=handler.response_serializer,\n   111→            )\n   112→        else:\n   113→            return handler\n   114→    \n   115→    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n   116→        \"\"\"Wrap unary-unary RPC method.\"\"\"\n   117→        \n   118→        async def wrapper(request: Any, context: ServicerContext) -> Any:\n   119→            return await self._handle_request(\n   120→                behavior, request, context, method_name, is_streaming=False\n   121→            )\n   122→        \n   123→        return wrapper\n   124→    \n   125→    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n   126→        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n   127→        async def wrapper(request: Any, context: ServicerContext):\n   128→            # For streaming responses, return the iterator directly (do not await)\n   129→            try:\n   130→                return behavior(request, context)\n   131→            finally:\n   132→                # Minimal logging to avoid interfering with stream lifecycle\n   133→                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n   134→        return wrapper\n   135→    \n   136→    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n   137→        \"\"\"Wrap stream-unary RPC method.\"\"\"\n   138→        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n   139→            # Request is streaming, response is unary; await the final response\n   140→            try:\n   141→                return await behavior(request_iterator, context)\n   142→            finally:\n   143→                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n   144→        return wrapper\n   145→    \n   146→    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n   147→        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n   148→        async def wrapper(request_iterator, context: ServicerContext):\n   149→            # For bi-di streaming, return the response iterator directly\n   150→            try:\n   151→                return behavior(request_iterator, context)\n   152→            finally:\n   153→                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n   154→        return wrapper\n   155→    \n   156→    async def _handle_request(\n   157→        self, \n   158→        behavior: Callable, \n   159→        request: Any, \n   160→        context: ServicerContext, \n   161→        method_name: str,\n   162→        is_streaming: bool = False\n   163→    ) -> Any:\n   164→        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n   165→        \n   166→        # Generate correlation ID\n   167→        correlation_id = str(uuid4())\n   168→        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n   169→        try:\n   170→            context.set_trailing_metadata([\n   171→                ('correlation_id', correlation_id),\n   172→                ('request_id', correlation_id)\n   173→            ])\n   174→        except Exception:\n   175→            pass\n   176→        \n   177→        start_time = time.time()\n   178→        \n   179→        # Update metrics\n   180→        self._metrics['total_requests'] += 1\n   181→        if method_name not in self._metrics['method_stats']:\n   182→            self._metrics['method_stats'][method_name] = {\n   183→                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n   184→            }\n   185→        self._metrics['method_stats'][method_name]['count'] += 1\n   186→        \n   187→        try:\n   188→            # Log request\n   189→            if self.log_requests:\n   190→                self._log_request(method_name, request, correlation_id, context)\n   191→            \n   192→            # Execute the actual method\n   193→            response = await behavior(request, context)\n   194→            \n   195→            # Calculate duration\n   196→            duration = time.time() - start_time\n   197→            \n   198→            # Update success metrics\n   199→            self._metrics['successful_requests'] += 1\n   200→            self._metrics['method_stats'][method_name]['success'] += 1\n   201→            self._update_timing_metrics(method_name, duration)\n   202→            \n   203→            # Log response\n   204→            if self.log_responses:\n   205→                self._log_response(method_name, response, correlation_id, duration, context)\n   206→            \n   207→            # Log performance\n   208→            if self.log_performance:\n   209→                self._log_performance(method_name, duration, correlation_id)\n   210→            \n   211→            return response\n   212→            \n   213→        except Exception as e:\n   214→            # Calculate duration\n   215→            duration = time.time() - start_time\n   216→            \n   217→            # Update error metrics\n   218→            self._metrics['failed_requests'] += 1\n   219→            self._metrics['method_stats'][method_name]['error'] += 1\n   220→            self._update_timing_metrics(method_name, duration)\n   221→            \n   222→            # Log error\n   223→            if self.log_errors:\n   224→                self._log_error(method_name, e, correlation_id, duration, context)\n   225→            \n   226→            # Re-raise the exception\n   227→            raise\n   228→    \n   229→    async def _handle_streaming_request(\n   230→        self,\n   231→        behavior: Callable,\n   232→        request_data: Any,\n   233→        context: ServicerContext,\n   234→        method_name: str,\n   235→        request_streaming: bool = False,\n   236→        response_streaming: bool = True\n   237→    ) -> Any:\n   238→        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n   239→        # For Phase 3, we're focusing on request/response patterns\n   240→        # Streaming would be implemented in a future phase\n   241→        correlation_id = str(uuid4())\n   242→        # Store correlation ID in invocation metadata instead of setting attributes\n   243→        try:\n   244→            context.set_trailing_metadata([\n   245→                ('correlation_id', correlation_id),\n   246→                ('request_id', correlation_id)\n   247→            ])\n   248→        except Exception:\n   249→            # If metadata setting fails, continue without it\n   250→            pass\n   251→        \n   252→        self.logger.info(\n   253→            f\"[{correlation_id}] Streaming method {method_name} called \"\n   254→            f\"(streaming implementation pending)\"\n   255→        )\n   256→        \n   257→        return behavior(request_data, context)\n   258→    \n   259→    def _log_request(\n   260→        self, \n   261→        method_name: str, \n   262→        request: Any, \n   263→        correlation_id: str, \n   264→        context: ServicerContext\n   265→    ):\n   266→        \"\"\"Log incoming request as JSON string.\"\"\"\n   267→        try:\n   268→            # Extract and sanitize request data\n   269→            request_data = self._sanitize_data(self._extract_request_data(request))\n   270→            \n   271→            # Create simple log object\n   272→            log_obj = {\n   273→                'type': 'REQUEST',\n   274→                'method': method_name,\n   275→                'correlation_id': correlation_id,\n   276→                'timestamp': time.time(),\n   277→                'data': request_data\n   278→            }\n   279→            \n   280→            # Log as JSON string\n   281→            self.logger.info(f\"REQUEST: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n   282→            \n   283→        except Exception as e:\n   284→            self.logger.error(f\"Failed to log request for {method_name}: {e}\")\n   285→    \n   286→    def _log_response(\n   287→        self, \n   288→        method_name: str, \n   289→        response: Any, \n   290→        correlation_id: str, \n   291→        duration: float,\n   292→        context: ServicerContext\n   293→    ):\n   294→        \"\"\"Log outgoing response as JSON string.\"\"\"\n   295→        try:\n   296→            # Extract and sanitize response data\n   297→            response_data = self._sanitize_data(self._extract_response_data(response))\n   298→            \n   299→            # Create simple log object\n   300→            log_obj = {\n   301→                'type': 'RESPONSE',\n   302→                'method': method_name,\n   303→                'correlation_id': correlation_id,\n   304→                'duration_ms': round(duration * 1000, 2),\n   305→                'timestamp': time.time(),\n   306→                'data': response_data\n   307→            }\n   308→            \n   309→            # Log as JSON string\n   310→            self.logger.info(f\"RESPONSE: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n   311→            \n   312→        except Exception as e:\n   313→            self.logger.error(f\"Failed to log response for {method_name}: {e}\")\n   314→    \n   315→    def _log_error(\n   316→        self, \n   317→        method_name: str, \n   318→        error: Exception, \n   319→        correlation_id: str, \n   320→        duration: float,\n   321→        context: ServicerContext\n   322→    ):\n   323→        \"\"\"Log error with comprehensive context.\"\"\"\n   324→        \n   325→        log_data = {\n   326→            'event': 'grpc_error',\n   327→            'correlation_id': correlation_id,\n   328→            'method': method_name,\n   329→            'error_type': type(error).__name__,\n   330→            'error_message': str(error),\n   331→            'duration_ms': round(duration * 1000, 2),\n   332→            'timestamp': time.time()\n   333→        }\n   334→        \n   335→        self.logger.error(\n   336→            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n   337→            extra={'structured_data': log_data},\n   338→            exc_info=True\n   339→        )\n   340→    \n   341→    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n   342→        \"\"\"Log performance metrics.\"\"\"\n   343→        \n   344→        if duration > 1.0:  # Log slow requests (>1 second)\n   345→            self.logger.warning(\n   346→                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n   347→            )\n   348→    \n   349→    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n   350→        \"\"\"Extract relevant data from request for logging.\"\"\"\n   351→        try:\n   352→            if hasattr(request, 'DESCRIPTOR'):\n   353→                # Protobuf message - extract only fields with actual values\n   354→                result = {}\n   355→                for field in request.DESCRIPTOR.fields:\n   356→                    field_name = field.name\n   357→                    try:\n   358→                        value = getattr(request, field_name, None)\n   359→                        \n   360→                        # For message fields, use HasField if available\n   361→                        if field.type == field.TYPE_MESSAGE:\n   362→                            try:\n   363→                                if hasattr(request, 'HasField') and request.HasField(field_name):\n   364→                                    if hasattr(value, 'DESCRIPTOR'):\n   365→                                        nested_data = self._extract_request_data(value)\n   366→                                        if nested_data:  # Only add if not empty\n   367→                                            result[field_name] = nested_data\n   368→                                    else:\n   369→                                        result[field_name] = self._serialize_value(value)\n   370→                            except ValueError:\n   371→                                # Field might not support HasField, try to extract anyway\n   372→                                if hasattr(value, 'DESCRIPTOR'):\n   373→                                    nested_data = self._extract_request_data(value)\n   374→                                    if nested_data:\n   375→                                        result[field_name] = nested_data\n   376→                                elif value is not None:\n   377→                                    result[field_name] = self._serialize_value(value)\n   378→                        \n   379→                        # For repeated fields\n   380→                        elif field.label == field.LABEL_REPEATED:\n   381→                            if value:  # Only if list is not empty\n   382→                                result[field_name] = [\n   383→                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n   384→                                    for item in value\n   385→                                ]\n   386→                        \n   387→                        # For scalar fields, check if not default value\n   388→                        else:\n   389→                            if self._is_not_default_value(value, field):\n   390→                                result[field_name] = self._serialize_value(value)\n   391→                                \n   392→                    except Exception as e:\n   393→                        # Skip logging errors for presence checks\n   394→                        if \"does not have presence\" not in str(e):\n   395→                            result[field_name] = f\"<error: {str(e)}>\"\n   396→                return result\n   397→            elif hasattr(request, '__dict__'):\n   398→                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n   399→            else:\n   400→                return {'value': str(request)}\n   401→        except Exception:\n   402→            return {'error': 'Failed to extract request data'}\n   403→    \n   404→    def _is_not_default_value(self, value: Any, field: Any) -> bool:\n   405→        \"\"\"Check if value is not the default value for the field type.\"\"\"\n   406→        try:\n   407→            if value is None:\n   408→                return False\n   409→            \n   410→            # Check based on field type\n   411→            if field.type == field.TYPE_STRING:\n   412→                return value != \"\"\n   413→            elif field.type in [field.TYPE_INT32, field.TYPE_INT64, field.TYPE_UINT32, field.TYPE_UINT64]:\n   414→                return value != 0\n   415→            elif field.type in [field.TYPE_FLOAT, field.TYPE_DOUBLE]:\n   416→                return value != 0.0\n   417→            elif field.type == field.TYPE_BOOL:\n   418→                return value != False\n   419→            elif field.type == field.TYPE_ENUM:\n   420→                return value != 0\n   421→            else:\n   422→                return True\n   423→        except:\n   424→            return True\n   425→    \n   426→    def _serialize_value(self, value: Any) -> Any:\n   427→        \"\"\"Serialize a value for logging.\"\"\"\n   428→        try:\n   429→            if isinstance(value, (str, int, float, bool)):\n   430→                return value\n   431→            elif isinstance(value, bytes):\n   432→                import base64\n   433→                encoded = base64.b64encode(value).decode()\n   434→                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n   435→            elif hasattr(value, 'isoformat'):\n   436→                return value.isoformat()\n   437→            elif hasattr(value, 'name') and hasattr(value, 'value'):  # Enum value\n   438→                return value.name if value.name else value.value\n   439→            elif hasattr(value, '__class__') and 'enum' in str(type(value)).lower():\n   440→                # Handle enum values that might not have name attribute\n   441→                return str(value)\n   442→            else:\n   443→                str_value = str(value)\n   444→                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n   445→        except Exception:\n   446→            return \"<non-serializable>\"\n   447→    \n   448→    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n   449→        \"\"\"Extract relevant data from response for logging.\"\"\"\n   450→        try:\n   451→            if hasattr(response, 'DESCRIPTOR'):\n   452→                # Protobuf message - extract only fields with actual values\n   453→                result = {}\n   454→                for field in response.DESCRIPTOR.fields:\n   455→                    field_name = field.name\n   456→                    try:\n   457→                        value = getattr(response, field_name, None)\n   458→                        \n   459→                        # For message fields, use HasField if available\n   460→                        if field.type == field.TYPE_MESSAGE:\n   461→                            try:\n   462→                                if hasattr(response, 'HasField') and response.HasField(field_name):\n   463→                                    if hasattr(value, 'DESCRIPTOR'):\n   464→                                        nested_data = self._extract_response_data(value)\n   465→                                        if nested_data:  # Only add if not empty\n   466→                                            result[field_name] = nested_data\n   467→                                    else:\n   468→                                        result[field_name] = self._serialize_value(value)\n   469→                            except ValueError:\n   470→                                # Field might not support HasField, try to extract anyway\n   471→                                if hasattr(value, 'DESCRIPTOR'):\n   472→                                    nested_data = self._extract_response_data(value)\n   473→                                    if nested_data:\n   474→                                        result[field_name] = nested_data\n   475→                                elif value is not None:\n   476→                                    result[field_name] = self._serialize_value(value)\n   477→                        \n   478→                        # For repeated fields\n   479→                        elif field.label == field.LABEL_REPEATED:\n   480→                            if value:  # Only if list is not empty\n   481→                                result[field_name] = [\n   482→                                    self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n   483→                                    for item in value\n   484→                                ]\n   485→                        \n   486→                        # For scalar fields, check if not default value\n   487→                        else:\n   488→                            if self._is_not_default_value(value, field):\n   489→                                result[field_name] = self._serialize_value(value)\n   490→                                \n   491→                    except Exception as e:\n   492→                        # Skip logging errors for presence checks\n   493→                        if \"does not have presence\" not in str(e):\n   494→                            result[field_name] = f\"<error: {str(e)}>\"\n   495→                return result\n   496→            elif hasattr(response, '__dict__'):\n   497→                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n   498→            else:\n   499→                return {'value': str(response)}\n   500→        except Exception:\n   501→            return {'error': 'Failed to extract response data'}\n   502→    \n   503→    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n   504→        \"\"\"Remove sensitive information from logged data.\"\"\"\n   505→        if not isinstance(data, dict):\n   506→            return data\n   507→        \n   508→        sanitized = {}\n   509→        for key, value in data.items():\n   510→            key_lower = key.lower()\n   511→            \n   512→            # Check if field contains sensitive information\n   513→            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n   514→                sanitized[key] = '***REDACTED***'\n   515→            elif isinstance(value, dict):\n   516→                sanitized[key] = self._sanitize_data(value)\n   517→            elif isinstance(value, (list, tuple)):\n   518→                sanitized[key] = [\n   519→                    self._sanitize_data(item) if isinstance(item, dict) else item\n   520→                    for item in value\n   521→                ]\n   522→            else:\n   523→                sanitized[key] = value\n   524→        \n   525→        return sanitized\n   526→    \n   527→    def _update_timing_metrics(self, method_name: str, duration: float):\n   528→        \"\"\"Update timing metrics for performance tracking.\"\"\"\n   529→        method_stats = self._metrics['method_stats'][method_name]\n   530→        current_avg = method_stats['avg_time']\n   531→        count = method_stats['count']\n   532→        \n   533→        # Update method-specific average\n   534→        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n   535→        \n   536→        # Update overall average\n   537→        total_requests = self._metrics['total_requests']\n   538→        current_overall_avg = self._metrics['avg_response_time']\n   539→        self._metrics['avg_response_time'] = (\n   540→            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n   541→        )\n   542→    \n   543→    def get_metrics(self) -> Dict[str, Any]:\n   544→        \"\"\"Get current performance metrics.\"\"\"\n   545→        return {\n   546→            'interceptor_name': 'LoggingInterceptor',\n   547→            'metrics': self._metrics.copy(),\n   548→            'timestamp': time.time()\n   549→        }\n   550→    \n   551→    def reset_metrics(self):\n   552→        \"\"\"Reset performance metrics.\"\"\"\n   553→        self._metrics = {\n   554→            'total_requests': 0,\n   555→            'successful_requests': 0,\n   556→            'failed_requests': 0,\n   557→            'avg_response_time': 0.0,\n   558→            'method_stats': {}\n   559→        }\n   560→\n   561→\n   562→# Convenience function for creating configured logging interceptor\n   563→def create_logging_interceptor(\n   564→    logger_name: str = \"grpc.logging\",\n   565→    log_level: int = logging.INFO,\n   566→    **kwargs\n   567→) -> LoggingInterceptor:\n   568→    \"\"\"Create a pre-configured logging interceptor.\n   569→    \n   570→    Args:\n   571→        logger_name: Name for the logger\n   572→        log_level: Logging level\n   573→        **kwargs: Additional configuration options\n   574→        \n   575→    Returns:\n   576→        Configured LoggingInterceptor instance\n   577→    \"\"\"\n   578→    logger = logging.getLogger(logger_name)\n   579→    return LoggingInterceptor(\n   580→        logger=logger,\n   581→        log_level=log_level,\n   582→        **kwargs\n   583→    )"}]},"uuid":"beee85a6-a5a9-4503-8892-c4b522255548","timestamp":"2025-08-20T02:13:29.848Z","toolUseResult":{"filePath":"/Users/lee/Documents/src/bep-oms-broker/src/presentation/grpc/interceptors/logging_interceptor.py","oldString":"\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n\nThis interceptor provides:\n- Detailed request/response logging\n- Performance metrics tracking\n- Error logging and analysis\n- Request correlation and tracing\n- Structured logging with context\n\"\"\"\nimport json\nimport logging\nimport time\nfrom typing import Any, Callable, Iterator, Optional, Dict\nfrom uuid import uuid4\n\nimport grpc\nimport grpc.aio\nfrom grpc.aio import ServerInterceptor, ServicerContext\n\n\nclass LoggingInterceptor(grpc.aio.ServerInterceptor):\n    \"\"\"Advanced logging interceptor for gRPC services.\n    \n    Features:\n    - Request/response logging with structured data\n    - Performance metrics and timing\n    - Error tracking and analysis\n    - Request correlation IDs\n    - Configurable log levels and filters\n    - Security-aware logging (excludes sensitive data)\n    \"\"\"\n    \n    def __init__(\n        self,\n        logger: Optional[logging.Logger] = None,\n        log_level: int = logging.INFO,\n        log_requests: bool = True,\n        log_responses: bool = True,\n        log_errors: bool = True,\n        log_performance: bool = True,\n        exclude_methods: Optional[list] = None,\n        sensitive_fields: Optional[list] = None\n    ):\n        self.logger = logger or logging.getLogger(__name__)\n        self.log_level = log_level\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_errors = log_errors\n        self.log_performance = log_performance\n        self.exclude_methods = exclude_methods or []\n        self.sensitive_fields = sensitive_fields or [\n            'password', 'token', 'secret', 'key', 'credential', 'auth'\n        ]\n        \n        # Performance metrics\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n    \n    async def intercept_service(\n        self,\n        continuation: Callable[[Any], Any],\n        handler_call_details: Any,\n    ) -> Any:\n        \"\"\"Intercept service calls for logging.\"\"\"\n        \n        # Skip excluded methods\n        method_name = handler_call_details.method\n        if method_name in self.exclude_methods:\n            return await continuation(handler_call_details)\n        \n        # Get the original handler\n        handler = await continuation(handler_call_details)\n        if handler is None:\n            return None\n        \n        # Wrap the handler based on its type; bypass reflection and health entirely\n        if method_name in {\n            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n            '/grpc.health.v1.Health/Check',\n            '/grpc.health.v1.Health/Watch',\n        }:\n            return handler\n        if handler.unary_unary:\n            return grpc.unary_unary_rpc_method_handler(\n                self._wrap_unary_unary(handler.unary_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.unary_stream:\n            return grpc.unary_stream_rpc_method_handler(\n                self._wrap_unary_stream(handler.unary_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_unary:\n            return grpc.stream_unary_rpc_method_handler(\n                self._wrap_stream_unary(handler.stream_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_stream:\n            return grpc.stream_stream_rpc_method_handler(\n                self._wrap_stream_stream(handler.stream_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        else:\n            return handler\n    \n    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-unary RPC method.\"\"\"\n        \n        async def wrapper(request: Any, context: ServicerContext) -> Any:\n            return await self._handle_request(\n                behavior, request, context, method_name, is_streaming=False\n            )\n        \n        return wrapper\n    \n    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n        async def wrapper(request: Any, context: ServicerContext):\n            # For streaming responses, return the iterator directly (do not await)\n            try:\n                return behavior(request, context)\n            finally:\n                # Minimal logging to avoid interfering with stream lifecycle\n                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-unary RPC method.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n            # Request is streaming, response is unary; await the final response\n            try:\n                return await behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext):\n            # For bi-di streaming, return the response iterator directly\n            try:\n                return behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n        return wrapper\n    \n    async def _handle_request(\n        self, \n        behavior: Callable, \n        request: Any, \n        context: ServicerContext, \n        method_name: str,\n        is_streaming: bool = False\n    ) -> Any:\n        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n        \n        # Generate correlation ID\n        correlation_id = str(uuid4())\n        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            pass\n        \n        start_time = time.time()\n        \n        # Update metrics\n        self._metrics['total_requests'] += 1\n        if method_name not in self._metrics['method_stats']:\n            self._metrics['method_stats'][method_name] = {\n                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n            }\n        self._metrics['method_stats'][method_name]['count'] += 1\n        \n        try:\n            # Log request\n            if self.log_requests:\n                self._log_request(method_name, request, correlation_id, context)\n            \n            # Execute the actual method\n            response = await behavior(request, context)\n            \n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update success metrics\n            self._metrics['successful_requests'] += 1\n            self._metrics['method_stats'][method_name]['success'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log response\n            if self.log_responses:\n                self._log_response(method_name, response, correlation_id, duration, context)\n            \n            # Log performance\n            if self.log_performance:\n                self._log_performance(method_name, duration, correlation_id)\n            \n            return response\n            \n        except Exception as e:\n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update error metrics\n            self._metrics['failed_requests'] += 1\n            self._metrics['method_stats'][method_name]['error'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log error\n            if self.log_errors:\n                self._log_error(method_name, e, correlation_id, duration, context)\n            \n            # Re-raise the exception\n            raise\n    \n    async def _handle_streaming_request(\n        self,\n        behavior: Callable,\n        request_data: Any,\n        context: ServicerContext,\n        method_name: str,\n        request_streaming: bool = False,\n        response_streaming: bool = True\n    ) -> Any:\n        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n        # For Phase 3, we're focusing on request/response patterns\n        # Streaming would be implemented in a future phase\n        correlation_id = str(uuid4())\n        # Store correlation ID in invocation metadata instead of setting attributes\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            # If metadata setting fails, continue without it\n            pass\n        \n        self.logger.info(\n            f\"[{correlation_id}] Streaming method {method_name} called \"\n            f\"(streaming implementation pending)\"\n        )\n        \n        return behavior(request_data, context)\n    \n    def _log_request(\n        self, \n        method_name: str, \n        request: Any, \n        correlation_id: str, \n        context: ServicerContext\n    ):\n        \"\"\"Log incoming request as JSON string.\"\"\"\n        try:\n            # Extract and sanitize request data\n            request_data = self._sanitize_data(self._extract_request_data(request))\n            \n            # Create simple log object\n            log_obj = {\n                'type': 'REQUEST',\n                'method': method_name,\n                'correlation_id': correlation_id,\n                'timestamp': time.time(),\n                'data': request_data\n            }\n            \n            # Log as JSON string\n            self.logger.info(f\"REQUEST: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log request for {method_name}: {e}\")\n    \n    def _log_response(\n        self, \n        method_name: str, \n        response: Any, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log outgoing response as JSON string.\"\"\"\n        try:\n            # Extract and sanitize response data\n            response_data = self._sanitize_data(self._extract_response_data(response))\n            \n            # Create simple log object\n            log_obj = {\n                'type': 'RESPONSE',\n                'method': method_name,\n                'correlation_id': correlation_id,\n                'duration_ms': round(duration * 1000, 2),\n                'timestamp': time.time(),\n                'data': response_data\n            }\n            \n            # Log as JSON string\n            self.logger.info(f\"RESPONSE: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log response for {method_name}: {e}\")\n    \n    def _log_error(\n        self, \n        method_name: str, \n        error: Exception, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log error with comprehensive context.\"\"\"\n        \n        log_data = {\n            'event': 'grpc_error',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'error_type': type(error).__name__,\n            'error_message': str(error),\n            'duration_ms': round(duration * 1000, 2),\n            'timestamp': time.time()\n        }\n        \n        self.logger.error(\n            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n            extra={'structured_data': log_data},\n            exc_info=True\n        )\n    \n    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n        \"\"\"Log performance metrics.\"\"\"\n        \n        if duration > 1.0:  # Log slow requests (>1 second)\n            self.logger.warning(\n                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n            )\n    \n    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract only fields with actual values\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(request, field_name, None)\n                        \n                        # For message fields, use HasField if available\n                        if field.type == field.TYPE_MESSAGE:\n                            try:\n                                if hasattr(request, 'HasField') and request.HasField(field_name):\n                                    if hasattr(value, 'DESCRIPTOR'):\n                                        nested_data = self._extract_request_data(value)\n                                        if nested_data:  # Only add if not empty\n                                            result[field_name] = nested_data\n                                    else:\n                                        result[field_name] = self._serialize_value(value)\n                            except ValueError:\n                                # Field might not support HasField, try to extract anyway\n                                if hasattr(value, 'DESCRIPTOR'):\n                                    nested_data = self._extract_request_data(value)\n                                    if nested_data:\n                                        result[field_name] = nested_data\n                                elif value is not None:\n                                    result[field_name] = self._serialize_value(value)\n                        \n                        # For repeated fields\n                        elif field.label == field.LABEL_REPEATED:\n                            if value:  # Only if list is not empty\n                                result[field_name] = [\n                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                        \n                        # For scalar fields, check if not default value\n                        else:\n                            if self._is_not_default_value(value, field):\n                                result[field_name] = self._serialize_value(value)\n                                \n                    except Exception as e:\n                        # Skip logging errors for presence checks\n                        if \"does not have presence\" not in str(e):\n                            result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}\n    \n    def _is_not_default_value(self, value: Any, field: Any) -> bool:\n        \"\"\"Check if value is not the default value for the field type.\"\"\"\n        try:\n            if value is None:\n                return False\n            \n            # Check based on field type\n            if field.type == field.TYPE_STRING:\n                return value != \"\"\n            elif field.type in [field.TYPE_INT32, field.TYPE_INT64, field.TYPE_UINT32, field.TYPE_UINT64]:\n                return value != 0\n            elif field.type in [field.TYPE_FLOAT, field.TYPE_DOUBLE]:\n                return value != 0.0\n            elif field.type == field.TYPE_BOOL:\n                return value != False\n            elif field.type == field.TYPE_ENUM:\n                return value != 0\n            else:\n                return True\n        except:\n            return True\n    \n    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a value for logging.\"\"\"\n        try:\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                import base64\n                encoded = base64.b64encode(value).decode()\n                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n            elif hasattr(value, 'isoformat'):\n                return value.isoformat()\n            elif hasattr(value, 'name') and hasattr(value, 'value'):  # Enum value\n                return value.name if value.name else value.value\n            elif hasattr(value, '__class__') and 'enum' in str(type(value)).lower():\n                # Handle enum values that might not have name attribute\n                return str(value)\n            else:\n                str_value = str(value)\n                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n        except Exception:\n            return \"<non-serializable>\"\n    \n    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message - extract only fields with actual values\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        # Check if field has been set (not default value)\n                        if hasattr(response, 'HasField') and response.HasField(field_name):\n                            value = getattr(response, field_name)\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                nested_data = self._extract_response_data(value)\n                                if nested_data:  # Only add if not empty\n                                    result[field_name] = nested_data\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                if value:  # Only if list is not empty\n                                    result[field_name] = [\n                                        self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                        for item in value\n                                    ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                        else:\n                            # For fields without HasField (primitives), check if not default\n                            value = getattr(response, field_name, None)\n                            if self._is_not_default_value(value, field):\n                                if hasattr(value, 'DESCRIPTOR'):\n                                    nested_data = self._extract_response_data(value)\n                                    if nested_data:\n                                        result[field_name] = nested_data\n                                elif field.label == field.LABEL_REPEATED:\n                                    if value:\n                                        result[field_name] = [\n                                            self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                            for item in value\n                                        ]\n                                else:\n                                    result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        # Only log errors for non-empty fields\n                        if str(e) != \"Message does not have field\":\n                            result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}\n    \n    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove sensitive information from logged data.\"\"\"\n        if not isinstance(data, dict):\n            return data\n        \n        sanitized = {}\n        for key, value in data.items():\n            key_lower = key.lower()\n            \n            # Check if field contains sensitive information\n            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n                sanitized[key] = '***REDACTED***'\n            elif isinstance(value, dict):\n                sanitized[key] = self._sanitize_data(value)\n            elif isinstance(value, (list, tuple)):\n                sanitized[key] = [\n                    self._sanitize_data(item) if isinstance(item, dict) else item\n                    for item in value\n                ]\n            else:\n                sanitized[key] = value\n        \n        return sanitized\n    \n    def _update_timing_metrics(self, method_name: str, duration: float):\n        \"\"\"Update timing metrics for performance tracking.\"\"\"\n        method_stats = self._metrics['method_stats'][method_name]\n        current_avg = method_stats['avg_time']\n        count = method_stats['count']\n        \n        # Update method-specific average\n        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n        \n        # Update overall average\n        total_requests = self._metrics['total_requests']\n        current_overall_avg = self._metrics['avg_response_time']\n        self._metrics['avg_response_time'] = (\n            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n        )\n    \n    def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get current performance metrics.\"\"\"\n        return {\n            'interceptor_name': 'LoggingInterceptor',\n            'metrics': self._metrics.copy(),\n            'timestamp': time.time()\n        }\n    \n    def reset_metrics(self):\n        \"\"\"Reset performance metrics.\"\"\"\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n\n\n# Convenience function for creating configured logging interceptor\ndef create_logging_interceptor(\n    logger_name: str = \"grpc.logging\",\n    log_level: int = logging.INFO,\n    **kwargs\n) -> LoggingInterceptor:\n    \"\"\"Create a pre-configured logging interceptor.\n    \n    Args:\n        logger_name: Name for the logger\n        log_level: Logging level\n        **kwargs: Additional configuration options\n        \n    Returns:\n        Configured LoggingInterceptor instance\n    \"\"\"\n    logger = logging.getLogger(logger_name)\n    return LoggingInterceptor(\n        logger=logger,\n        log_level=log_level,\n        **kwargs\n    )","newString":"\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n\nThis interceptor provides:\n- Detailed request/response logging\n- Performance metrics tracking\n- Error logging and analysis\n- Request correlation and tracing\n- Structured logging with context\n\"\"\"\nimport json\nimport logging\nimport time\nfrom typing import Any, Callable, Iterator, Optional, Dict\nfrom uuid import uuid4\n\nimport grpc\nimport grpc.aio\nfrom grpc.aio import ServerInterceptor, ServicerContext\n\n\nclass LoggingInterceptor(grpc.aio.ServerInterceptor):\n    \"\"\"Advanced logging interceptor for gRPC services.\n    \n    Features:\n    - Request/response logging with structured data\n    - Performance metrics and timing\n    - Error tracking and analysis\n    - Request correlation IDs\n    - Configurable log levels and filters\n    - Security-aware logging (excludes sensitive data)\n    \"\"\"\n    \n    def __init__(\n        self,\n        logger: Optional[logging.Logger] = None,\n        log_level: int = logging.INFO,\n        log_requests: bool = True,\n        log_responses: bool = True,\n        log_errors: bool = True,\n        log_performance: bool = True,\n        exclude_methods: Optional[list] = None,\n        sensitive_fields: Optional[list] = None\n    ):\n        self.logger = logger or logging.getLogger(__name__)\n        self.log_level = log_level\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_errors = log_errors\n        self.log_performance = log_performance\n        self.exclude_methods = exclude_methods or []\n        self.sensitive_fields = sensitive_fields or [\n            'password', 'token', 'secret', 'key', 'credential', 'auth'\n        ]\n        \n        # Performance metrics\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n    \n    async def intercept_service(\n        self,\n        continuation: Callable[[Any], Any],\n        handler_call_details: Any,\n    ) -> Any:\n        \"\"\"Intercept service calls for logging.\"\"\"\n        \n        # Skip excluded methods\n        method_name = handler_call_details.method\n        if method_name in self.exclude_methods:\n            return await continuation(handler_call_details)\n        \n        # Get the original handler\n        handler = await continuation(handler_call_details)\n        if handler is None:\n            return None\n        \n        # Wrap the handler based on its type; bypass reflection and health entirely\n        if method_name in {\n            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n            '/grpc.health.v1.Health/Check',\n            '/grpc.health.v1.Health/Watch',\n        }:\n            return handler\n        if handler.unary_unary:\n            return grpc.unary_unary_rpc_method_handler(\n                self._wrap_unary_unary(handler.unary_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.unary_stream:\n            return grpc.unary_stream_rpc_method_handler(\n                self._wrap_unary_stream(handler.unary_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_unary:\n            return grpc.stream_unary_rpc_method_handler(\n                self._wrap_stream_unary(handler.stream_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_stream:\n            return grpc.stream_stream_rpc_method_handler(\n                self._wrap_stream_stream(handler.stream_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        else:\n            return handler\n    \n    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-unary RPC method.\"\"\"\n        \n        async def wrapper(request: Any, context: ServicerContext) -> Any:\n            return await self._handle_request(\n                behavior, request, context, method_name, is_streaming=False\n            )\n        \n        return wrapper\n    \n    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n        async def wrapper(request: Any, context: ServicerContext):\n            # For streaming responses, return the iterator directly (do not await)\n            try:\n                return behavior(request, context)\n            finally:\n                # Minimal logging to avoid interfering with stream lifecycle\n                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-unary RPC method.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n            # Request is streaming, response is unary; await the final response\n            try:\n                return await behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext):\n            # For bi-di streaming, return the response iterator directly\n            try:\n                return behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n        return wrapper\n    \n    async def _handle_request(\n        self, \n        behavior: Callable, \n        request: Any, \n        context: ServicerContext, \n        method_name: str,\n        is_streaming: bool = False\n    ) -> Any:\n        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n        \n        # Generate correlation ID\n        correlation_id = str(uuid4())\n        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            pass\n        \n        start_time = time.time()\n        \n        # Update metrics\n        self._metrics['total_requests'] += 1\n        if method_name not in self._metrics['method_stats']:\n            self._metrics['method_stats'][method_name] = {\n                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n            }\n        self._metrics['method_stats'][method_name]['count'] += 1\n        \n        try:\n            # Log request\n            if self.log_requests:\n                self._log_request(method_name, request, correlation_id, context)\n            \n            # Execute the actual method\n            response = await behavior(request, context)\n            \n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update success metrics\n            self._metrics['successful_requests'] += 1\n            self._metrics['method_stats'][method_name]['success'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log response\n            if self.log_responses:\n                self._log_response(method_name, response, correlation_id, duration, context)\n            \n            # Log performance\n            if self.log_performance:\n                self._log_performance(method_name, duration, correlation_id)\n            \n            return response\n            \n        except Exception as e:\n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update error metrics\n            self._metrics['failed_requests'] += 1\n            self._metrics['method_stats'][method_name]['error'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log error\n            if self.log_errors:\n                self._log_error(method_name, e, correlation_id, duration, context)\n            \n            # Re-raise the exception\n            raise\n    \n    async def _handle_streaming_request(\n        self,\n        behavior: Callable,\n        request_data: Any,\n        context: ServicerContext,\n        method_name: str,\n        request_streaming: bool = False,\n        response_streaming: bool = True\n    ) -> Any:\n        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n        # For Phase 3, we're focusing on request/response patterns\n        # Streaming would be implemented in a future phase\n        correlation_id = str(uuid4())\n        # Store correlation ID in invocation metadata instead of setting attributes\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            # If metadata setting fails, continue without it\n            pass\n        \n        self.logger.info(\n            f\"[{correlation_id}] Streaming method {method_name} called \"\n            f\"(streaming implementation pending)\"\n        )\n        \n        return behavior(request_data, context)\n    \n    def _log_request(\n        self, \n        method_name: str, \n        request: Any, \n        correlation_id: str, \n        context: ServicerContext\n    ):\n        \"\"\"Log incoming request as JSON string.\"\"\"\n        try:\n            # Extract and sanitize request data\n            request_data = self._sanitize_data(self._extract_request_data(request))\n            \n            # Create simple log object\n            log_obj = {\n                'type': 'REQUEST',\n                'method': method_name,\n                'correlation_id': correlation_id,\n                'timestamp': time.time(),\n                'data': request_data\n            }\n            \n            # Log as JSON string\n            self.logger.info(f\"REQUEST: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log request for {method_name}: {e}\")\n    \n    def _log_response(\n        self, \n        method_name: str, \n        response: Any, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log outgoing response as JSON string.\"\"\"\n        try:\n            # Extract and sanitize response data\n            response_data = self._sanitize_data(self._extract_response_data(response))\n            \n            # Create simple log object\n            log_obj = {\n                'type': 'RESPONSE',\n                'method': method_name,\n                'correlation_id': correlation_id,\n                'duration_ms': round(duration * 1000, 2),\n                'timestamp': time.time(),\n                'data': response_data\n            }\n            \n            # Log as JSON string\n            self.logger.info(f\"RESPONSE: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log response for {method_name}: {e}\")\n    \n    def _log_error(\n        self, \n        method_name: str, \n        error: Exception, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log error with comprehensive context.\"\"\"\n        \n        log_data = {\n            'event': 'grpc_error',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'error_type': type(error).__name__,\n            'error_message': str(error),\n            'duration_ms': round(duration * 1000, 2),\n            'timestamp': time.time()\n        }\n        \n        self.logger.error(\n            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n            extra={'structured_data': log_data},\n            exc_info=True\n        )\n    \n    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n        \"\"\"Log performance metrics.\"\"\"\n        \n        if duration > 1.0:  # Log slow requests (>1 second)\n            self.logger.warning(\n                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n            )\n    \n    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract only fields with actual values\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(request, field_name, None)\n                        \n                        # For message fields, use HasField if available\n                        if field.type == field.TYPE_MESSAGE:\n                            try:\n                                if hasattr(request, 'HasField') and request.HasField(field_name):\n                                    if hasattr(value, 'DESCRIPTOR'):\n                                        nested_data = self._extract_request_data(value)\n                                        if nested_data:  # Only add if not empty\n                                            result[field_name] = nested_data\n                                    else:\n                                        result[field_name] = self._serialize_value(value)\n                            except ValueError:\n                                # Field might not support HasField, try to extract anyway\n                                if hasattr(value, 'DESCRIPTOR'):\n                                    nested_data = self._extract_request_data(value)\n                                    if nested_data:\n                                        result[field_name] = nested_data\n                                elif value is not None:\n                                    result[field_name] = self._serialize_value(value)\n                        \n                        # For repeated fields\n                        elif field.label == field.LABEL_REPEATED:\n                            if value:  # Only if list is not empty\n                                result[field_name] = [\n                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                        \n                        # For scalar fields, check if not default value\n                        else:\n                            if self._is_not_default_value(value, field):\n                                result[field_name] = self._serialize_value(value)\n                                \n                    except Exception as e:\n                        # Skip logging errors for presence checks\n                        if \"does not have presence\" not in str(e):\n                            result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}\n    \n    def _is_not_default_value(self, value: Any, field: Any) -> bool:\n        \"\"\"Check if value is not the default value for the field type.\"\"\"\n        try:\n            if value is None:\n                return False\n            \n            # Check based on field type\n            if field.type == field.TYPE_STRING:\n                return value != \"\"\n            elif field.type in [field.TYPE_INT32, field.TYPE_INT64, field.TYPE_UINT32, field.TYPE_UINT64]:\n                return value != 0\n            elif field.type in [field.TYPE_FLOAT, field.TYPE_DOUBLE]:\n                return value != 0.0\n            elif field.type == field.TYPE_BOOL:\n                return value != False\n            elif field.type == field.TYPE_ENUM:\n                return value != 0\n            else:\n                return True\n        except:\n            return True\n    \n    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a value for logging.\"\"\"\n        try:\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                import base64\n                encoded = base64.b64encode(value).decode()\n                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n            elif hasattr(value, 'isoformat'):\n                return value.isoformat()\n            elif hasattr(value, 'name') and hasattr(value, 'value'):  # Enum value\n                return value.name if value.name else value.value\n            elif hasattr(value, '__class__') and 'enum' in str(type(value)).lower():\n                # Handle enum values that might not have name attribute\n                return str(value)\n            else:\n                str_value = str(value)\n                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n        except Exception:\n            return \"<non-serializable>\"\n    \n    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message - extract only fields with actual values\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(response, field_name, None)\n                        \n                        # For message fields, use HasField if available\n                        if field.type == field.TYPE_MESSAGE:\n                            try:\n                                if hasattr(response, 'HasField') and response.HasField(field_name):\n                                    if hasattr(value, 'DESCRIPTOR'):\n                                        nested_data = self._extract_response_data(value)\n                                        if nested_data:  # Only add if not empty\n                                            result[field_name] = nested_data\n                                    else:\n                                        result[field_name] = self._serialize_value(value)\n                            except ValueError:\n                                # Field might not support HasField, try to extract anyway\n                                if hasattr(value, 'DESCRIPTOR'):\n                                    nested_data = self._extract_response_data(value)\n                                    if nested_data:\n                                        result[field_name] = nested_data\n                                elif value is not None:\n                                    result[field_name] = self._serialize_value(value)\n                        \n                        # For repeated fields\n                        elif field.label == field.LABEL_REPEATED:\n                            if value:  # Only if list is not empty\n                                result[field_name] = [\n                                    self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                        \n                        # For scalar fields, check if not default value\n                        else:\n                            if self._is_not_default_value(value, field):\n                                result[field_name] = self._serialize_value(value)\n                                \n                    except Exception as e:\n                        # Skip logging errors for presence checks\n                        if \"does not have presence\" not in str(e):\n                            result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}\n    \n    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove sensitive information from logged data.\"\"\"\n        if not isinstance(data, dict):\n            return data\n        \n        sanitized = {}\n        for key, value in data.items():\n            key_lower = key.lower()\n            \n            # Check if field contains sensitive information\n            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n                sanitized[key] = '***REDACTED***'\n            elif isinstance(value, dict):\n                sanitized[key] = self._sanitize_data(value)\n            elif isinstance(value, (list, tuple)):\n                sanitized[key] = [\n                    self._sanitize_data(item) if isinstance(item, dict) else item\n                    for item in value\n                ]\n            else:\n                sanitized[key] = value\n        \n        return sanitized\n    \n    def _update_timing_metrics(self, method_name: str, duration: float):\n        \"\"\"Update timing metrics for performance tracking.\"\"\"\n        method_stats = self._metrics['method_stats'][method_name]\n        current_avg = method_stats['avg_time']\n        count = method_stats['count']\n        \n        # Update method-specific average\n        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n        \n        # Update overall average\n        total_requests = self._metrics['total_requests']\n        current_overall_avg = self._metrics['avg_response_time']\n        self._metrics['avg_response_time'] = (\n            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n        )\n    \n    def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get current performance metrics.\"\"\"\n        return {\n            'interceptor_name': 'LoggingInterceptor',\n            'metrics': self._metrics.copy(),\n            'timestamp': time.time()\n        }\n    \n    def reset_metrics(self):\n        \"\"\"Reset performance metrics.\"\"\"\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n\n\n# Convenience function for creating configured logging interceptor\ndef create_logging_interceptor(\n    logger_name: str = \"grpc.logging\",\n    log_level: int = logging.INFO,\n    **kwargs\n) -> LoggingInterceptor:\n    \"\"\"Create a pre-configured logging interceptor.\n    \n    Args:\n        logger_name: Name for the logger\n        log_level: Logging level\n        **kwargs: Additional configuration options\n        \n    Returns:\n        Configured LoggingInterceptor instance\n    \"\"\"\n    logger = logging.getLogger(logger_name)\n    return LoggingInterceptor(\n        logger=logger,\n        log_level=log_level,\n        **kwargs\n    )","originalFile":"\"\"\"Enhanced gRPC Logging Interceptor with comprehensive request/response logging.\n\nThis interceptor provides:\n- Detailed request/response logging\n- Performance metrics tracking\n- Error logging and analysis\n- Request correlation and tracing\n- Structured logging with context\n\"\"\"\nimport json\nimport logging\nimport time\nfrom typing import Any, Callable, Iterator, Optional, Dict\nfrom uuid import uuid4\n\nimport grpc\nimport grpc.aio\nfrom grpc.aio import ServerInterceptor, ServicerContext\n\n\nclass LoggingInterceptor(grpc.aio.ServerInterceptor):\n    \"\"\"Advanced logging interceptor for gRPC services.\n    \n    Features:\n    - Request/response logging with structured data\n    - Performance metrics and timing\n    - Error tracking and analysis\n    - Request correlation IDs\n    - Configurable log levels and filters\n    - Security-aware logging (excludes sensitive data)\n    \"\"\"\n    \n    def __init__(\n        self,\n        logger: Optional[logging.Logger] = None,\n        log_level: int = logging.INFO,\n        log_requests: bool = True,\n        log_responses: bool = True,\n        log_errors: bool = True,\n        log_performance: bool = True,\n        exclude_methods: Optional[list] = None,\n        sensitive_fields: Optional[list] = None\n    ):\n        self.logger = logger or logging.getLogger(__name__)\n        self.log_level = log_level\n        self.log_requests = log_requests\n        self.log_responses = log_responses\n        self.log_errors = log_errors\n        self.log_performance = log_performance\n        self.exclude_methods = exclude_methods or []\n        self.sensitive_fields = sensitive_fields or [\n            'password', 'token', 'secret', 'key', 'credential', 'auth'\n        ]\n        \n        # Performance metrics\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n    \n    async def intercept_service(\n        self,\n        continuation: Callable[[Any], Any],\n        handler_call_details: Any,\n    ) -> Any:\n        \"\"\"Intercept service calls for logging.\"\"\"\n        \n        # Skip excluded methods\n        method_name = handler_call_details.method\n        if method_name in self.exclude_methods:\n            return await continuation(handler_call_details)\n        \n        # Get the original handler\n        handler = await continuation(handler_call_details)\n        if handler is None:\n            return None\n        \n        # Wrap the handler based on its type; bypass reflection and health entirely\n        if method_name in {\n            '/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo',\n            '/grpc.health.v1.Health/Check',\n            '/grpc.health.v1.Health/Watch',\n        }:\n            return handler\n        if handler.unary_unary:\n            return grpc.unary_unary_rpc_method_handler(\n                self._wrap_unary_unary(handler.unary_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.unary_stream:\n            return grpc.unary_stream_rpc_method_handler(\n                self._wrap_unary_stream(handler.unary_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_unary:\n            return grpc.stream_unary_rpc_method_handler(\n                self._wrap_stream_unary(handler.stream_unary, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        elif handler.stream_stream:\n            return grpc.stream_stream_rpc_method_handler(\n                self._wrap_stream_stream(handler.stream_stream, method_name),\n                request_deserializer=handler.request_deserializer,\n                response_serializer=handler.response_serializer,\n            )\n        else:\n            return handler\n    \n    def _wrap_unary_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-unary RPC method.\"\"\"\n        \n        async def wrapper(request: Any, context: ServicerContext) -> Any:\n            return await self._handle_request(\n                behavior, request, context, method_name, is_streaming=False\n            )\n        \n        return wrapper\n    \n    def _wrap_unary_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap unary-stream RPC method without awaiting the response stream.\"\"\"\n        async def wrapper(request: Any, context: ServicerContext):\n            # For streaming responses, return the iterator directly (do not await)\n            try:\n                return behavior(request, context)\n            finally:\n                # Minimal logging to avoid interfering with stream lifecycle\n                self.logger.info(f\"gRPC unary_stream call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_unary(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-unary RPC method.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext) -> Any:\n            # Request is streaming, response is unary; await the final response\n            try:\n                return await behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_unary call: {method_name}\")\n        return wrapper\n    \n    def _wrap_stream_stream(self, behavior: Callable, method_name: str) -> Callable:\n        \"\"\"Wrap stream-stream RPC method without awaiting either stream.\"\"\"\n        async def wrapper(request_iterator, context: ServicerContext):\n            # For bi-di streaming, return the response iterator directly\n            try:\n                return behavior(request_iterator, context)\n            finally:\n                self.logger.info(f\"gRPC stream_stream call: {method_name}\")\n        return wrapper\n    \n    async def _handle_request(\n        self, \n        behavior: Callable, \n        request: Any, \n        context: ServicerContext, \n        method_name: str,\n        is_streaming: bool = False\n    ) -> Any:\n        \"\"\"Handle a single unary request with comprehensive logging.\"\"\"\n        \n        # Generate correlation ID\n        correlation_id = str(uuid4())\n        # Best-effort: set trailing metadata; guard exceptions to avoid hanging\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            pass\n        \n        start_time = time.time()\n        \n        # Update metrics\n        self._metrics['total_requests'] += 1\n        if method_name not in self._metrics['method_stats']:\n            self._metrics['method_stats'][method_name] = {\n                'count': 0, 'success': 0, 'error': 0, 'avg_time': 0.0\n            }\n        self._metrics['method_stats'][method_name]['count'] += 1\n        \n        try:\n            # Log request\n            if self.log_requests:\n                self._log_request(method_name, request, correlation_id, context)\n            \n            # Execute the actual method\n            response = await behavior(request, context)\n            \n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update success metrics\n            self._metrics['successful_requests'] += 1\n            self._metrics['method_stats'][method_name]['success'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log response\n            if self.log_responses:\n                self._log_response(method_name, response, correlation_id, duration, context)\n            \n            # Log performance\n            if self.log_performance:\n                self._log_performance(method_name, duration, correlation_id)\n            \n            return response\n            \n        except Exception as e:\n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update error metrics\n            self._metrics['failed_requests'] += 1\n            self._metrics['method_stats'][method_name]['error'] += 1\n            self._update_timing_metrics(method_name, duration)\n            \n            # Log error\n            if self.log_errors:\n                self._log_error(method_name, e, correlation_id, duration, context)\n            \n            # Re-raise the exception\n            raise\n    \n    async def _handle_streaming_request(\n        self,\n        behavior: Callable,\n        request_data: Any,\n        context: ServicerContext,\n        method_name: str,\n        request_streaming: bool = False,\n        response_streaming: bool = True\n    ) -> Any:\n        \"\"\"Handle streaming requests (not implemented in this phase).\"\"\"\n        # For Phase 3, we're focusing on request/response patterns\n        # Streaming would be implemented in a future phase\n        correlation_id = str(uuid4())\n        # Store correlation ID in invocation metadata instead of setting attributes\n        try:\n            context.set_trailing_metadata([\n                ('correlation_id', correlation_id),\n                ('request_id', correlation_id)\n            ])\n        except Exception:\n            # If metadata setting fails, continue without it\n            pass\n        \n        self.logger.info(\n            f\"[{correlation_id}] Streaming method {method_name} called \"\n            f\"(streaming implementation pending)\"\n        )\n        \n        return behavior(request_data, context)\n    \n    def _log_request(\n        self, \n        method_name: str, \n        request: Any, \n        correlation_id: str, \n        context: ServicerContext\n    ):\n        \"\"\"Log incoming request as JSON string.\"\"\"\n        try:\n            # Extract and sanitize request data\n            request_data = self._sanitize_data(self._extract_request_data(request))\n            \n            # Create simple log object\n            log_obj = {\n                'type': 'REQUEST',\n                'method': method_name,\n                'correlation_id': correlation_id,\n                'timestamp': time.time(),\n                'data': request_data\n            }\n            \n            # Log as JSON string\n            self.logger.info(f\"REQUEST: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log request for {method_name}: {e}\")\n    \n    def _log_response(\n        self, \n        method_name: str, \n        response: Any, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log outgoing response as JSON string.\"\"\"\n        try:\n            # Extract and sanitize response data\n            response_data = self._sanitize_data(self._extract_response_data(response))\n            \n            # Create simple log object\n            log_obj = {\n                'type': 'RESPONSE',\n                'method': method_name,\n                'correlation_id': correlation_id,\n                'duration_ms': round(duration * 1000, 2),\n                'timestamp': time.time(),\n                'data': response_data\n            }\n            \n            # Log as JSON string\n            self.logger.info(f\"RESPONSE: {json.dumps(log_obj, ensure_ascii=False, default=str)}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log response for {method_name}: {e}\")\n    \n    def _log_error(\n        self, \n        method_name: str, \n        error: Exception, \n        correlation_id: str, \n        duration: float,\n        context: ServicerContext\n    ):\n        \"\"\"Log error with comprehensive context.\"\"\"\n        \n        log_data = {\n            'event': 'grpc_error',\n            'correlation_id': correlation_id,\n            'method': method_name,\n            'error_type': type(error).__name__,\n            'error_message': str(error),\n            'duration_ms': round(duration * 1000, 2),\n            'timestamp': time.time()\n        }\n        \n        self.logger.error(\n            f\"[{correlation_id}] gRPC Error: {method_name} - {type(error).__name__}: {error}\",\n            extra={'structured_data': log_data},\n            exc_info=True\n        )\n    \n    def _log_performance(self, method_name: str, duration: float, correlation_id: str):\n        \"\"\"Log performance metrics.\"\"\"\n        \n        if duration > 1.0:  # Log slow requests (>1 second)\n            self.logger.warning(\n                f\"[{correlation_id}] Slow gRPC call: {method_name} took {duration*1000:.2f}ms\"\n            )\n    \n    def _extract_request_data(self, request: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from request for logging.\"\"\"\n        try:\n            if hasattr(request, 'DESCRIPTOR'):\n                # Protobuf message - extract only fields with actual values\n                result = {}\n                for field in request.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        value = getattr(request, field_name, None)\n                        \n                        # For message fields, use HasField if available\n                        if field.type == field.TYPE_MESSAGE:\n                            try:\n                                if hasattr(request, 'HasField') and request.HasField(field_name):\n                                    if hasattr(value, 'DESCRIPTOR'):\n                                        nested_data = self._extract_request_data(value)\n                                        if nested_data:  # Only add if not empty\n                                            result[field_name] = nested_data\n                                    else:\n                                        result[field_name] = self._serialize_value(value)\n                            except ValueError:\n                                # Field might not support HasField, try to extract anyway\n                                if hasattr(value, 'DESCRIPTOR'):\n                                    nested_data = self._extract_request_data(value)\n                                    if nested_data:\n                                        result[field_name] = nested_data\n                                elif value is not None:\n                                    result[field_name] = self._serialize_value(value)\n                        \n                        # For repeated fields\n                        elif field.label == field.LABEL_REPEATED:\n                            if value:  # Only if list is not empty\n                                result[field_name] = [\n                                    self._extract_request_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                    for item in value\n                                ]\n                        \n                        # For scalar fields, check if not default value\n                        else:\n                            if self._is_not_default_value(value, field):\n                                result[field_name] = self._serialize_value(value)\n                                \n                    except Exception as e:\n                        # Skip logging errors for presence checks\n                        if \"does not have presence\" not in str(e):\n                            result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(request, '__dict__'):\n                return {k: v for k, v in request.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(request)}\n        except Exception:\n            return {'error': 'Failed to extract request data'}\n    \n    def _is_not_default_value(self, value: Any, field: Any) -> bool:\n        \"\"\"Check if value is not the default value for the field type.\"\"\"\n        try:\n            if value is None:\n                return False\n            \n            # Check based on field type\n            if field.type == field.TYPE_STRING:\n                return value != \"\"\n            elif field.type in [field.TYPE_INT32, field.TYPE_INT64, field.TYPE_UINT32, field.TYPE_UINT64]:\n                return value != 0\n            elif field.type in [field.TYPE_FLOAT, field.TYPE_DOUBLE]:\n                return value != 0.0\n            elif field.type == field.TYPE_BOOL:\n                return value != False\n            elif field.type == field.TYPE_ENUM:\n                return value != 0\n            else:\n                return True\n        except:\n            return True\n    \n    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Serialize a value for logging.\"\"\"\n        try:\n            if isinstance(value, (str, int, float, bool)):\n                return value\n            elif isinstance(value, bytes):\n                import base64\n                encoded = base64.b64encode(value).decode()\n                return f\"<bytes:{encoded[:100]}{'...' if len(encoded) > 100 else ''}>\"\n            elif hasattr(value, 'isoformat'):\n                return value.isoformat()\n            elif hasattr(value, 'name') and hasattr(value, 'value'):  # Enum value\n                return value.name if value.name else value.value\n            elif hasattr(value, '__class__') and 'enum' in str(type(value)).lower():\n                # Handle enum values that might not have name attribute\n                return str(value)\n            else:\n                str_value = str(value)\n                return str_value[:1000] + \"...\" if len(str_value) > 1000 else str_value\n        except Exception:\n            return \"<non-serializable>\"\n    \n    def _extract_response_data(self, response: Any) -> Dict[str, Any]:\n        \"\"\"Extract relevant data from response for logging.\"\"\"\n        try:\n            if hasattr(response, 'DESCRIPTOR'):\n                # Protobuf message - extract only fields with actual values\n                result = {}\n                for field in response.DESCRIPTOR.fields:\n                    field_name = field.name\n                    try:\n                        # Check if field has been set (not default value)\n                        if hasattr(response, 'HasField') and response.HasField(field_name):\n                            value = getattr(response, field_name)\n                            if hasattr(value, 'DESCRIPTOR'):\n                                # Nested protobuf message\n                                nested_data = self._extract_response_data(value)\n                                if nested_data:  # Only add if not empty\n                                    result[field_name] = nested_data\n                            elif field.label == field.LABEL_REPEATED:\n                                # Repeated field (list)\n                                if value:  # Only if list is not empty\n                                    result[field_name] = [\n                                        self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                        for item in value\n                                    ]\n                            else:\n                                result[field_name] = self._serialize_value(value)\n                        else:\n                            # For fields without HasField (primitives), check if not default\n                            value = getattr(response, field_name, None)\n                            if self._is_not_default_value(value, field):\n                                if hasattr(value, 'DESCRIPTOR'):\n                                    nested_data = self._extract_response_data(value)\n                                    if nested_data:\n                                        result[field_name] = nested_data\n                                elif field.label == field.LABEL_REPEATED:\n                                    if value:\n                                        result[field_name] = [\n                                            self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)\n                                            for item in value\n                                        ]\n                                else:\n                                    result[field_name] = self._serialize_value(value)\n                    except Exception as e:\n                        # Only log errors for non-empty fields\n                        if str(e) != \"Message does not have field\":\n                            result[field_name] = f\"<error: {str(e)}>\"\n                return result\n            elif hasattr(response, '__dict__'):\n                return {k: v for k, v in response.__dict__.items() if not k.startswith('_')}\n            else:\n                return {'value': str(response)}\n        except Exception:\n            return {'error': 'Failed to extract response data'}\n    \n    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove sensitive information from logged data.\"\"\"\n        if not isinstance(data, dict):\n            return data\n        \n        sanitized = {}\n        for key, value in data.items():\n            key_lower = key.lower()\n            \n            # Check if field contains sensitive information\n            if any(sensitive in key_lower for sensitive in self.sensitive_fields):\n                sanitized[key] = '***REDACTED***'\n            elif isinstance(value, dict):\n                sanitized[key] = self._sanitize_data(value)\n            elif isinstance(value, (list, tuple)):\n                sanitized[key] = [\n                    self._sanitize_data(item) if isinstance(item, dict) else item\n                    for item in value\n                ]\n            else:\n                sanitized[key] = value\n        \n        return sanitized\n    \n    def _update_timing_metrics(self, method_name: str, duration: float):\n        \"\"\"Update timing metrics for performance tracking.\"\"\"\n        method_stats = self._metrics['method_stats'][method_name]\n        current_avg = method_stats['avg_time']\n        count = method_stats['count']\n        \n        # Update method-specific average\n        method_stats['avg_time'] = (current_avg * (count - 1) + duration) / count\n        \n        # Update overall average\n        total_requests = self._metrics['total_requests']\n        current_overall_avg = self._metrics['avg_response_time']\n        self._metrics['avg_response_time'] = (\n            (current_overall_avg * (total_requests - 1) + duration) / total_requests\n        )\n    \n    def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get current performance metrics.\"\"\"\n        return {\n            'interceptor_name': 'LoggingInterceptor',\n            'metrics': self._metrics.copy(),\n            'timestamp': time.time()\n        }\n    \n    def reset_metrics(self):\n        \"\"\"Reset performance metrics.\"\"\"\n        self._metrics = {\n            'total_requests': 0,\n            'successful_requests': 0,\n            'failed_requests': 0,\n            'avg_response_time': 0.0,\n            'method_stats': {}\n        }\n\n\n# Convenience function for creating configured logging interceptor\ndef create_logging_interceptor(\n    logger_name: str = \"grpc.logging\",\n    log_level: int = logging.INFO,\n    **kwargs\n) -> LoggingInterceptor:\n    \"\"\"Create a pre-configured logging interceptor.\n    \n    Args:\n        logger_name: Name for the logger\n        log_level: Logging level\n        **kwargs: Additional configuration options\n        \n    Returns:\n        Configured LoggingInterceptor instance\n    \"\"\"\n    logger = logging.getLogger(logger_name)\n    return LoggingInterceptor(\n        logger=logger,\n        log_level=log_level,\n        **kwargs\n    )","structuredPatch":[{"oldStart":454,"oldLines":42,"newStart":454,"newLines":43,"lines":["                 for field in response.DESCRIPTOR.fields:","                     field_name = field.name","                     try:","-                        # Check if field has been set (not default value)","-                        if hasattr(response, 'HasField') and response.HasField(field_name):","-                            value = getattr(response, field_name)","-                            if hasattr(value, 'DESCRIPTOR'):","-                                # Nested protobuf message","-                                nested_data = self._extract_response_data(value)","-                                if nested_data:  # Only add if not empty","-                                    result[field_name] = nested_data","-                            elif field.label == field.LABEL_REPEATED:","-                                # Repeated field (list)","-                                if value:  # Only if list is not empty","-                                    result[field_name] = [","-                                        self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)","-                                        for item in value","-                                    ]","-                            else:","-                                result[field_name] = self._serialize_value(value)","-                        else:","-                            # For fields without HasField (primitives), check if not default","-                            value = getattr(response, field_name, None)","-                            if self._is_not_default_value(value, field):","+                        value = getattr(response, field_name, None)","+                        ","+                        # For message fields, use HasField if available","+                        if field.type == field.TYPE_MESSAGE:","+                            try:","+                                if hasattr(response, 'HasField') and response.HasField(field_name):","+                                    if hasattr(value, 'DESCRIPTOR'):","+                                        nested_data = self._extract_response_data(value)","+                                        if nested_data:  # Only add if not empty","+                                            result[field_name] = nested_data","+                                    else:","+                                        result[field_name] = self._serialize_value(value)","+                            except ValueError:","+                                # Field might not support HasField, try to extract anyway","                                 if hasattr(value, 'DESCRIPTOR'):","                                     nested_data = self._extract_response_data(value)","                                     if nested_data:","                                         result[field_name] = nested_data","-                                elif field.label == field.LABEL_REPEATED:","-                                    if value:","-                                        result[field_name] = [","-                                            self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)","-                                            for item in value","-                                        ]","-                                else:","+                                elif value is not None:","                                     result[field_name] = self._serialize_value(value)","+                        ","+                        # For repeated fields","+                        elif field.label == field.LABEL_REPEATED:","+                            if value:  # Only if list is not empty","+                                result[field_name] = [","+                                    self._extract_response_data(item) if hasattr(item, 'DESCRIPTOR') else self._serialize_value(item)","+                                    for item in value","+                                ]","+                        ","+                        # For scalar fields, check if not default value","+                        else:","+                            if self._is_not_default_value(value, field):","+                                result[field_name] = self._serialize_value(value)","+                                ","                     except Exception as e:","-                        # Only log errors for non-empty fields","-                        if str(e) != \"Message does not have field\":","+                        # Skip logging errors for presence checks","+                        if \"does not have presence\" not in str(e):","                             result[field_name] = f\"<error: {str(e)}>\"","                 return result","             elif hasattr(response, '__dict__'):"]}],"userModified":false,"replaceAll":false}}
{"parentUuid":"beee85a6-a5a9-4503-8892-c4b522255548","isSidechain":false,"userType":"external","cwd":"/Users/lee/Documents/src/bep-oms-broker","sessionId":"fc2b6231-43bf-46a3-9ad8-bc23257a7fe4","version":"1.0.65","gitBranch":"dev","message":{"id":"msg_01VKgZ7qBn9chioZTqYM36fN","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Bây giờ tôi đã sửa xong. Vấn đề chính là:\n\n1. **Protobuf version 3 không hỗ trợ `HasField()` cho scalar fields** (string, int, float, bool, enum) - chỉ có cho message fields\n2. **Logic xử lý sai** - tôi đã sửa để:\n   - Chỉ dùng `HasField()` cho message fields (`field.type == field.TYPE_MESSAGE`)\n   - Với scalar fields, kiểm tra trực tiếp giá trị có khác default không\n   - Bỏ qua error \"does not have presence\" để tránh spam logs\n\n## Thay đổi chính:\n\n1. **Phân loại field types**: Xử lý riêng message fields và scalar fields\n2. **Safe HasField checking**: Wrap trong try/catch để handle trường hợp không support\n3. **Better error filtering**: Chỉ log error thực sự, không log presence errors\n4. **Improved default value detection**: Kiểm tra chính xác based trên field type\n\nGiờ khi bạn restart server và test lại, logs sẽ hiển thị chính xác dữ liệu thực tế mà không có các error messages về \"does not have presence\"."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":9130,"cache_read_input_tokens":104903,"cache_creation":{"ephemeral_5m_input_tokens":9130,"ephemeral_1h_input_tokens":0},"output_tokens":343,"service_tier":"standard"}},"requestId":"req_011CSJ9rJbHqZCmMGLTJqHBJ","type":"assistant","uuid":"46fa3098-c2e4-4e11-b521-cea9a17f51ee","timestamp":"2025-08-20T02:13:47.747Z"}
